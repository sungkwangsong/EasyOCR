{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sungkwangsong/EasyOCR/blob/master/notebooks/Hi_BERT_%EB%B0%B0%EC%B9%98_%ED%95%99%EC%8A%B5_(KoSimCSE_%EB%AA%A8%EB%8D%B8_%EB%B2%A0%EC%9D%B4%EC%8A%A4)_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0k2c3H6isUE",
        "outputId": "3e6d5e22-25de-4b74-a31c-2d5dedce2042"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HBN_PROJECT_DIR_PATH = '/content/drive/MyDrive/Workspaces/Hibrainnet/Hi-BERT'\n",
        "HBN_ONTOLOGIES_DIR_PATH = f'{HBN_PROJECT_DIR_PATH}/ontologies'\n",
        "HBN_OUTPUTS_DIR_PATH = f'{HBN_PROJECT_DIR_PATH}/outputs'\n",
        "HBN_MODELS_DIR_PATH = f'{HBN_PROJECT_DIR_PATH}/models'\n",
        "HBN_DATASETS_DIR_PATH = f'{HBN_PROJECT_DIR_PATH}/datasets'\n",
        "HBN_DATALAKE_DIR_PATH = '/content/drive/MyDrive/Workspaces/Hibrainnet/hbn-data-lake'"
      ],
      "metadata": {
        "id": "Yk61hyltiuPW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
        "# AdamW를 torch.optim에서 가져오도록 수정\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup, DataCollatorForLanguageModeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "import random\n",
        "import pickle\n",
        "import gc\n",
        "import time\n",
        "from torch.amp import autocast, GradScaler\n",
        "import concurrent.futures\n",
        "from functools import partial\n",
        "\n",
        "# 메모리 최적화를 위한 환경 변수 설정\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# 경로 설정\n",
        "ONTOLOGY_TRAIN_DIR = f\"{HBN_ONTOLOGIES_DIR_PATH}/training\"\n",
        "ONTOLOGY_TEST_DIR = f\"{HBN_ONTOLOGIES_DIR_PATH}/test\"\n",
        "ONTOLOGY_VALID_DIR = f\"{HBN_ONTOLOGIES_DIR_PATH}/validation\"\n",
        "PAPER_TRAIN_PATH = f\"{HBN_DATASETS_DIR_PATH}/combined_articles_train.csv\"\n",
        "PAPER_TEST_PATH = f\"{HBN_DATASETS_DIR_PATH}/combined_articles_test.csv\"\n",
        "PAPER_VALID_PATH = f\"{HBN_DATASETS_DIR_PATH}/combined_articles_validation.csv\"\n",
        "OUTPUT_DIR = f\"{HBN_MODELS_DIR_PATH}/hi_bert_model_kosimcse\"\n",
        "STOPWORDS_PATH = f\"{HBN_DATASETS_DIR_PATH}/recruitment_stopwords2.csv\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "MAX_LENGTH = 256\n",
        "BATCH_SIZE = 32\n",
        "MLM_LEARNING_RATE = 1e-5 #2e-5\n",
        "CL_LEARNING_RATE = 5e-6 #1e-5  # Hi-BERT를 위해 낮은 학습률 유지\n",
        "MLM_EPOCHS = 2 #4\n",
        "CL_EPOCHS = 3 #4\n",
        "TEMPERATURE = 0.07 #0.05  # 더 샤프한 대조학습\n",
        "# MLM_LEARNING_RATE = 2e-5\n",
        "# CL_LEARNING_RATE = 1e-5  # Hi-BERT를 위해 낮은 학습률 유지\n",
        "# MLM_EPOCHS = 4\n",
        "# CL_EPOCHS = 4\n",
        "# TEMPERATURE = 0.05  # 더 샤프한 대조학습\n",
        "MARGIN = 0.2  # 마진 손실을 위한 마진 값\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# 그라디언트 누적 스텝 설정\n",
        "ACCUMULATION_STEPS = 2\n",
        "\n",
        "MAX_JOB_COUNT = 1000\n",
        "MAX_PAPER_COUNT = 1000\n",
        "MAX_ONTOLOGY_COUNT = 1000\n",
        "MAX_TEMPLATE_COUNT = 500\n",
        "\n",
        "HBN_DROPOUT = 0.1 # 0.1에서 증가\n",
        "\n",
        "TEMPLATE_TARGET_MIN = 80\n",
        "TEMPLATE_TARGET_MAX = 90\n",
        "TEMPLATE_TARGET_TRY_LIMIT = 20\n",
        "\n",
        "# 단일 기본 모델 설정 - 일관성을 위해 단일 모델만 사용\n",
        "# BASE_MODEL = 'klue/bert-base'\n",
        "BASE_MODEL = 'BM-K/KoSimCSE-bert'\n",
        "PROJECTION_DIM = 768\n",
        "IS_RESET_CHECKPOINT = True\n",
        "\n",
        "# 템플릿 정의\n",
        "templates = {\n",
        "    \"hasQualification\": \"{} 직무는 {} 자격증이 필요합니다.\",\n",
        "    \"requiresCertificate\": \"{} 분야는 {} 자격증이 필요합니다.\",\n",
        "    \"hasMajor\": \"{} 분야는 {} 전공과 관련이 있습니다.\",\n",
        "    \"requiresMajor\": \"{} 분야는 {} 전공 지식이 필요합니다.\",\n",
        "    \"hasSkill\": \"{} 직무는 {} 기술이 필요합니다.\",\n",
        "    \"requiresSkill\": \"{} 분야는 {} 기술이 필요합니다.\",\n",
        "    \"hasOrganization\": \"{} 채용은 {} 기관에서 진행합니다.\",\n",
        "    \"hasLocation\": \"{} 기관은 {} 지역에 위치합니다.\",\n",
        "    \"hasDepartment\": \"{} 채용은 {} 부서에서 진행합니다.\",\n",
        "    \"hasField\": \"{} 부서는 {} 분야를 담당합니다.\",\n",
        "    \"hasPositionLevel\": \"{} 분야는 {} 직급을 채용합니다.\",\n",
        "    \"hasPreference\": \"{} 분야는 {} 우대사항이 있습니다.\",\n",
        "    \"hasDuty\": \"{} 분야는 {} 업무를 담당합니다.\",\n",
        "    \"hasDuties\": \"{} 분야는 {} 업무를 담당합니다.\",\n",
        "    \"hasMajorKeyword\": \"{} 전공은 {} 키워드와 관련이 있습니다.\",\n",
        "    \"hasMajorKeywords\": \"{} 전공은 {} 키워드와 관련이 있습니다.\",\n",
        "    \"hasRequirement\": \"{} 분야는 {} 요구사항이 있습니다.\",\n",
        "    \"hasRequirements\": \"{} 분야는 {} 요구사항이 있습니다.\",\n",
        "    \"hasRecruitmentField\": \"{} 채용은 {} 분야를 대상으로 합니다.\",\n",
        "    \"hasSkillKnowledge\": \"{} 직무는 {} 지식/기술이 필요합니다.\",\n",
        "    \"hasSkillKnowledges\": \"{} 직무는 {} 지식/기술이 필요합니다.\",\n",
        "    \"hasJobField\": \"{} 채용은 {} 직무 분야와 관련됩니다.\",\n",
        "    \"hasJobPosting\": \"{} 기관은 {} 채용 공고를 게시했습니다.\",\n",
        "    \"hasTitle\": \"{} 채용의 제목은 {}입니다.\",\n",
        "    \"hasDatePosted\": \"{} 채용 공고는 {}에 게시되었습니다.\",\n",
        "    \"hasPreferredConditions\": \"{} 분야는 {} 우대 조건이 있습니다.\",\n",
        "    \"hasPreferredCondition\": \"{} 분야는 {} 우대 조건이 있습니다.\",\n",
        "    \"hasQualifications\": \"{} 직무는 {} 자격 요건이 필요합니다.\",\n",
        "    \"hasMajors\": \"{} 분야는 {} 전공자를 채용합니다.\",\n",
        "    # 채용-논문 연결에 특화된 추가 템플릿\n",
        "    \"relatedToPaper\": \"{} 분야는 {} 주제의 논문과 관련이 있습니다.\",\n",
        "    \"usesResearch\": \"{} 직무는 {} 연구 결과를 활용합니다.\",\n",
        "    \"requiresKnowledge\": \"{} 분야는 {} 지식이 필요합니다.\",\n",
        "    \"appliesTheory\": \"{} 직무는 {} 이론을 적용합니다.\",\n",
        "}\n",
        "\n",
        "\n",
        "# 체크포인트 저장 함수\n",
        "def save_checkpoint(output_dir, step, model, optimizer, scheduler, tokenizer=None, loss=None):\n",
        "    checkpoint_dir = f\"{output_dir}/checkpoint_{step}\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    checkpoint = {\n",
        "        'step': step,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
        "        'loss': loss\n",
        "    }\n",
        "\n",
        "    torch.save(checkpoint, f\"{checkpoint_dir}/model.pt\")\n",
        "\n",
        "    if tokenizer:\n",
        "        tokenizer.save_pretrained(checkpoint_dir)\n",
        "\n",
        "    print(f\"Checkpoint saved at step {step}\")\n",
        "\n",
        "    # 체크포인트 정보 저장\n",
        "    with open(f\"{output_dir}/checkpoint_info.json\", 'w') as f:\n",
        "        json.dump({'latest_step': step, 'checkpoint_path': checkpoint_dir}, f)\n",
        "\n",
        "# 체크포인트 로드 함수\n",
        "def load_latest_checkpoint(output_dir, model, optimizer=None, scheduler=None):\n",
        "    checkpoint_info_path = f\"{output_dir}/checkpoint_info.json\"\n",
        "\n",
        "    if not os.path.exists(checkpoint_info_path):\n",
        "        print(\"No checkpoint found. Starting from scratch.\")\n",
        "        return None, 0\n",
        "\n",
        "    with open(checkpoint_info_path, 'r') as f:\n",
        "        checkpoint_info = json.load(f)\n",
        "\n",
        "    checkpoint_path = checkpoint_info['latest_step']\n",
        "    checkpoint_dir = checkpoint_info['checkpoint_path']\n",
        "    checkpoint_file = f\"{checkpoint_dir}/model.pt\"\n",
        "\n",
        "    if not os.path.exists(checkpoint_file):\n",
        "        print(f\"Checkpoint file {checkpoint_file} not found. Starting from scratch.\")\n",
        "        return None, 0\n",
        "\n",
        "    print(f\"Loading checkpoint from {checkpoint_file}\")\n",
        "    try:\n",
        "        checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
        "\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "        if optimizer and 'optimizer_state_dict' in checkpoint:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        if scheduler and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:\n",
        "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "        # 스텝 값 처리 - 문자열이면 숫자로 변환 시도\n",
        "        step = checkpoint['step']\n",
        "        if isinstance(step, str):\n",
        "            # 에포크 형식인 경우(예: 'epoch_2') 특별 처리\n",
        "            if step.startswith('epoch_'):\n",
        "                # MLM 학습 완료로 간주\n",
        "                print(f\"Detected epoch checkpoint: {step}. MLM training is considered complete.\")\n",
        "                return checkpoint, 999999  # 매우 큰 숫자로 설정하여 학습 완료로 처리\n",
        "            else:\n",
        "                # 다른 형태의 문자열이면 정수로 변환 시도\n",
        "                try:\n",
        "                    step = int(step)\n",
        "                except ValueError:\n",
        "                    print(f\"Could not convert step '{step}' to integer. Starting from step 0.\")\n",
        "                    step = 0\n",
        "\n",
        "        print(f\"Checkpoint loaded. Resuming from step {step}\")\n",
        "        return checkpoint, step\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading checkpoint: {e}\")\n",
        "        return None, 0\n",
        "\n",
        "# 불용어 로드 함수\n",
        "def load_stopwords(file_path):\n",
        "    \"\"\"불용어 CSV 파일 로드\"\"\"\n",
        "    try:\n",
        "        stopwords_df = pd.read_csv(file_path)\n",
        "\n",
        "        # 한국어, 약어, 영어 불용어 모두 수집\n",
        "        stopwords = set()\n",
        "\n",
        "        # 한국어 불용어\n",
        "        if 'KOR' in stopwords_df.columns:\n",
        "            kor_stopwords = [word.lower() for word in stopwords_df['KOR'].dropna() if isinstance(word, str)]\n",
        "            stopwords.update(kor_stopwords)\n",
        "\n",
        "        # 약어 불용어\n",
        "        if 'ABB' in stopwords_df.columns:\n",
        "            abb_stopwords = [word.lower() for word in stopwords_df['ABB'].dropna() if isinstance(word, str)]\n",
        "            stopwords.update(abb_stopwords)\n",
        "\n",
        "        # 영어 불용어\n",
        "        if 'ENG' in stopwords_df.columns:\n",
        "            eng_stopwords = [word.lower() for word in stopwords_df['ENG'].dropna() if isinstance(word, str)]\n",
        "            stopwords.update(eng_stopwords)\n",
        "\n",
        "        # 기본 불용어도 추가\n",
        "        basic_stopwords = {'및', '등', '를', '이', '를 위한', '기반', '연구', '기술', '분석', '개발', '관한', '통한', '활용',\n",
        "                          '에서', '으로', '에', '의', '이해', '관련', '중심', '방법', '있는', '위한', '대한', '시스템'}\n",
        "        stopwords.update(basic_stopwords)\n",
        "\n",
        "        print(f\"Loaded {len(stopwords)} stopwords from {file_path}\")\n",
        "        return stopwords\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading stopwords file {file_path}: {e}\")\n",
        "        # 실패시 기본 불용어 사용\n",
        "        return {'및', '등', '를', '이', '를 위한', '기반', '연구', '기술', '분석', '개발', '관한', '통한', '활용',\n",
        "               '에서', '으로', '에', '의', '이해', '관련', '중심', '방법', '있는', '위한', '대한', '시스템'}\n",
        "\n",
        "STOPWORDS = load_stopwords(STOPWORDS_PATH)\n",
        "\n",
        "# 온톨로지 데이터에서 분야 정보 추출 함수\n",
        "def extract_field_info_from_ontology(ontology_data):\n",
        "    \"\"\"온톨로지 데이터에서 분야 정보 추출 - has 접두사 추가된 키 구조에 맞게 수정\"\"\"\n",
        "    field_info = {\n",
        "        'field': '',\n",
        "        'duties': '',\n",
        "        'qualifications': '',\n",
        "        'preferred': '',\n",
        "        'majors': '',\n",
        "        'keywords': '',\n",
        "        'skills': '',\n",
        "        'majors_list': [],\n",
        "        'keywords_list': [],\n",
        "        'skills_list': []\n",
        "    }\n",
        "\n",
        "    # JobPosting 정보\n",
        "    job_posting = ontology_data.get('hasJobPosting', {})\n",
        "    if isinstance(job_posting, dict):\n",
        "        # 이름이 hasTitle로 변경됨\n",
        "        field_info['field'] = job_posting.get('hasTitle', '')\n",
        "\n",
        "    # hasJobFields 처리\n",
        "    if 'hasJobFields' in ontology_data and isinstance(ontology_data['hasJobFields'], list):\n",
        "        for job_field in ontology_data['hasJobFields']:\n",
        "            if not isinstance(job_field, dict) or 'hasFields' not in job_field:\n",
        "                continue\n",
        "\n",
        "            for field in job_field['hasFields']:\n",
        "                if not isinstance(field, dict):\n",
        "                    continue\n",
        "\n",
        "                # hasRecruitmentField (수정됨)\n",
        "                if 'hasRecruitmentField' in field:\n",
        "                    field_info['field'] = field['hasRecruitmentField']\n",
        "\n",
        "                # hasDuties (수정됨)\n",
        "                duties = []\n",
        "                if 'hasDuties' in field and isinstance(field['hasDuties'], list):\n",
        "                    for duty in field['hasDuties']:\n",
        "                        if isinstance(duty, dict) and 'hasDescription' in duty:  # 수정됨\n",
        "                            duties.append(duty['hasDescription'])\n",
        "                field_info['duties'] = ' '.join(duties)\n",
        "\n",
        "                # hasQualifications (수정됨)\n",
        "                quals = []\n",
        "                if 'hasQualifications' in field and isinstance(field['hasQualifications'], list):\n",
        "                    for qual in field['hasQualifications']:\n",
        "                        if isinstance(qual, dict) and 'hasRequirement' in qual:  # 수정됨\n",
        "                            quals.append(qual['hasRequirement'])\n",
        "                field_info['qualifications'] = ' '.join(quals)\n",
        "\n",
        "                # hasPreferredConditions (수정됨)\n",
        "                prefs = []\n",
        "                if 'hasPreferredConditions' in field and isinstance(field['hasPreferredConditions'], list):\n",
        "                    for pref in field['hasPreferredConditions']:\n",
        "                        if isinstance(pref, dict) and 'hasCondition' in pref:  # 수정됨\n",
        "                            prefs.append(pref['hasCondition'])\n",
        "                field_info['preferred'] = ' '.join(prefs)\n",
        "\n",
        "                # hasMajors (수정됨)\n",
        "                majors = []\n",
        "                if 'hasMajors' in field and isinstance(field['hasMajors'], list):\n",
        "                    for major in field['hasMajors']:\n",
        "                        if isinstance(major, dict) and 'hasName' in major:  # 수정됨\n",
        "                            majors.append(major['hasName'])\n",
        "                field_info['majors'] = ' '.join(majors)\n",
        "                field_info['majors_list'] = majors\n",
        "\n",
        "                # hasMajorKeywords (수정됨)\n",
        "                keywords = []\n",
        "                if 'hasMajorKeywords' in field and isinstance(field['hasMajorKeywords'], list):\n",
        "                    for keyword in field['hasMajorKeywords']:\n",
        "                        if isinstance(keyword, dict) and 'hasKeyword' in keyword:  # 수정됨\n",
        "                            keywords.append(keyword['hasKeyword'])\n",
        "                field_info['keywords'] = ' '.join(keywords)\n",
        "                field_info['keywords_list'] = keywords\n",
        "\n",
        "                # hasSkillKnowledges (수정됨)\n",
        "                skills = []\n",
        "                if 'hasSkillKnowledges' in field and isinstance(field['hasSkillKnowledges'], list):\n",
        "                    for skill in field['hasSkillKnowledges']:\n",
        "                        if isinstance(skill, dict) and 'hasSkill' in skill:  # 수정됨\n",
        "                            skills.append(skill['hasSkill'])\n",
        "                field_info['skills'] = ' '.join(skills)\n",
        "                field_info['skills_list'] = skills\n",
        "\n",
        "    return field_info\n",
        "\n",
        "# 온톨로지 파일 로드 함수\n",
        "def load_ontology(directory_path, max_samples=500):\n",
        "    \"\"\"온톨로지 파일을 로드하여 채용 정보 추출\"\"\"\n",
        "    job_fields = []\n",
        "\n",
        "    # MAX_ONTOLOGY_COUNT 적용\n",
        "    if max_samples is None and MAX_ONTOLOGY_COUNT is not None:\n",
        "        max_samples = MAX_ONTOLOGY_COUNT\n",
        "\n",
        "    # 디렉토리 내의 모든 jsonld 파일 찾기\n",
        "    jsonld_files = glob.glob(os.path.join(directory_path, \"**\", \"*.json\"), recursive=True)\n",
        "\n",
        "    # 파일 수 제한\n",
        "    if max_samples is not None and max_samples > 0:\n",
        "        jsonld_files = jsonld_files[:max_samples]\n",
        "\n",
        "    print(f\"Found {len(jsonld_files)} ontology files in {directory_path}\")\n",
        "\n",
        "    # 첫 번째 파일 디버깅 출력\n",
        "    if jsonld_files:\n",
        "        first_file = jsonld_files[0]\n",
        "        print(f\"디버깅: 첫 번째 파일 경로 = {first_file}\")\n",
        "        try:\n",
        "            with open(first_file, 'r', encoding='utf-8') as f:\n",
        "                try:\n",
        "                    content = f.read()\n",
        "                    print(f\"파일 내용 (처음 500자):\\n{content[:500]}\")\n",
        "                    first_ontology = json.loads(content)\n",
        "                    print(\"JSON 구조:\")\n",
        "                    print(f\"주요 키: {list(first_ontology.keys())}\")\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"JSON 파싱 오류: {e}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"파일 처리 오류: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"파일 열기 오류: {e}\")\n",
        "\n",
        "    # 파일 반복 처리\n",
        "    for file_path in tqdm(jsonld_files, desc=\"Loading ontology files\"):\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                try:\n",
        "                    content = f.read()\n",
        "                    ontology_data = json.loads(content)\n",
        "\n",
        "                    # 필드 정보 추출\n",
        "                    field_info = extract_field_info_from_ontology(ontology_data)\n",
        "\n",
        "                    # 내용 구성 및 결과 추가\n",
        "                    if field_info:\n",
        "                        # 채용 정보 텍스트 구성\n",
        "                        job_text = f\"분야: {field_info['field']} 업무: {field_info['duties']} 자격요건: {field_info['qualifications']} \" \\\n",
        "                                 f\"우대사항: {field_info['preferred']} 전공: {field_info['majors']} \" \\\n",
        "                                 f\"키워드: {field_info['keywords']} 기술: {field_info['skills']}\"\n",
        "\n",
        "                        job_fields.append({\n",
        "                            'job_id': ontology_data.get('@id', ''),\n",
        "                            'job_text': job_text,\n",
        "                            'field': field_info['field'],\n",
        "                            'duties': field_info['duties'],\n",
        "                            'qualifications': field_info['qualifications'],\n",
        "                            'preferred': field_info['preferred'],\n",
        "                            'majors': field_info['majors_list'],\n",
        "                            'keywords': field_info['keywords_list'],\n",
        "                            'skills': field_info['skills_list']\n",
        "                        })\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"Error parsing {file_path}: {e}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {file_path}: {str(e)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error opening {file_path}: {e}\")\n",
        "\n",
        "    # 결과 출력\n",
        "    if len(job_fields) > 0:\n",
        "        print(\"\\n처리된 첫 번째 채용공고 정보:\")\n",
        "        print(f\"ID: {job_fields[0]['job_id']}\")\n",
        "        print(f\"분야: {job_fields[0]['field']}\")\n",
        "        print(f\"키워드: {job_fields[0]['keywords']}\")\n",
        "    else:\n",
        "        print(\"처리된 채용공고가 없습니다!\")\n",
        "\n",
        "    print(f\"Successfully loaded {len(job_fields)} job fields from {len(jsonld_files)} files\")\n",
        "    return job_fields\n",
        "\n",
        "# 논문 데이터 로드 함수\n",
        "def load_papers(file_path, max_samples=2000):\n",
        "    \"\"\"논문 CSV 파일 로드\"\"\"\n",
        "    # MAX_PAPER_COUNT 적용\n",
        "    if max_samples is None and MAX_PAPER_COUNT is not None:\n",
        "        max_samples = MAX_PAPER_COUNT\n",
        "\n",
        "    papers_df = pd.read_csv(file_path)\n",
        "\n",
        "    # 논문 수 제한\n",
        "    if max_samples is not None and max_samples > 0:\n",
        "        papers_df = papers_df.head(max_samples)\n",
        "\n",
        "    print(f\"Processing {len(papers_df)} papers from {file_path}\")\n",
        "\n",
        "    papers = []\n",
        "    for _, row in tqdm(papers_df.iterrows(), total=len(papers_df), desc=\"Loading papers\"):\n",
        "        try:\n",
        "            # 논문 정보 텍스트 구성\n",
        "            paper_text = f\"제목: {row['title']} 저자: {row['authors']} 초록: {row['abstract']} 키워드: {row['keywords']}\"\n",
        "\n",
        "            # 키워드 처리 (불용어 제거 및 길이 2 미만 제외)\n",
        "            keywords = []\n",
        "            if isinstance(row['keywords'], str):\n",
        "                keywords = [k.strip() for k in row['keywords'].split(',')\n",
        "                           if k.strip().lower() not in STOPWORDS and len(k.strip()) > 2]\n",
        "\n",
        "            # 카테고리 정보\n",
        "            category = row.get('category', \"\") if pd.notna(row.get('category', \"\")) else \"\"\n",
        "\n",
        "            papers.append({\n",
        "                'paper_id': row['article_id'],\n",
        "                'paper_text': paper_text,\n",
        "                'title': row['title'] if pd.notna(row['title']) else \"\",\n",
        "                'abstract': row['abstract'] if pd.notna(row['abstract']) else \"\",\n",
        "                'keywords': keywords,\n",
        "                'authors': row['authors'] if pd.notna(row['authors']) else \"\",\n",
        "                'category': category,\n",
        "                'journal': row.get('journal_name', '') if pd.notna(row.get('journal_name', '')) else \"\",\n",
        "                'pub_year': row.get('pub_year', '') if pd.notna(row.get('pub_year', '')) else \"\"\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing paper row: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Successfully loaded {len(papers)} papers\")\n",
        "    return papers\n",
        "\n",
        "# 템플릿 기반 문장 생성 함수\n",
        "# def generate_template_sentences(job_fields, papers, templates):\n",
        "#     \"\"\"온톨로지 템플릿을 활용하여 학습용 문장 생성 (수정된 온톨로지 구조 지원)\"\"\"\n",
        "#     print(\"\\n=== 템플릿 기반 문장 생성 시작 ===\")\n",
        "#     template_sentences = []\n",
        "\n",
        "#     # 1. 채용 필드 기반 템플릿 문장 생성\n",
        "#     for job in tqdm(job_fields, desc=\"Generating job field templates\"):\n",
        "#         field_name = job.get('field', '')\n",
        "#         if not field_name:\n",
        "#             continue\n",
        "\n",
        "#         # 전공 관련 문장 생성\n",
        "#         for major in job.get('majors', []):  # [:3]:  # 최적화: 최대 3개만 사용\n",
        "#             if major and len(major) > 1:\n",
        "#                 template_sentences.append(templates['hasMajor'].format(field_name, major))\n",
        "#                 template_sentences.append(templates['requiresMajor'].format(field_name, major))\n",
        "\n",
        "#         # 키워드 관련 문장 생성\n",
        "#         for keyword in job.get('keywords', []):  # [:3]:  # 최적화: 최대 3개만 사용\n",
        "#             if keyword and len(keyword) > 1:\n",
        "#                 template_sentences.append(templates['hasMajorKeyword'].format(field_name, keyword))\n",
        "\n",
        "#     # 2. 채용-논문 연결을 위한 추가 템플릿 문장 생성\n",
        "#     # 모든 직무와 논문 사용하여 더 많은 연결 생성 시도\n",
        "#     job_sample = job_fields[:min(300, len(job_fields))]\n",
        "#     paper_sample = papers[:min(500, len(papers))]\n",
        "\n",
        "#     for job in tqdm(job_sample, desc=\"Generating job-paper templates\"):\n",
        "#         field_name = job.get('field', '')\n",
        "#         if not field_name:\n",
        "#             continue\n",
        "\n",
        "#         # 직무 키워드와 관련된 논문 찾기\n",
        "#         job_keywords = set([k.lower() for k in job.get('keywords', []) if k])\n",
        "#         job_skills = set([s.lower() for s in job.get('skills', []) if s])\n",
        "\n",
        "#         # 키워드나 스킬이 없는 경우, 직무명을 토큰화하여 키워드로 사용\n",
        "#         if not job_keywords and field_name:\n",
        "#             job_keywords = set([w.lower() for w in field_name.split() if len(w) > 2 and w.lower() not in STOPWORDS])\n",
        "\n",
        "#         for paper in paper_sample[:20]:  # 각 직무당 최대 20개 논문 확인\n",
        "#             paper_keywords = set([k.lower() for k in paper.get('keywords', []) if k])\n",
        "#             paper_title = paper.get('title', '')\n",
        "\n",
        "#             # 논문 키워드가 없는 경우, 제목을 토큰화하여 키워드로 사용\n",
        "#             if not paper_keywords and paper_title:\n",
        "#                 paper_keywords = set([w.lower() for w in paper_title.split() if len(w) > 2 and w.lower() not in STOPWORDS])\n",
        "\n",
        "#             # 키워드 일치 여부 확인 (부분 일치도 포함)\n",
        "#             keyword_match = False\n",
        "#             for job_kw in job_keywords:\n",
        "#                 for paper_kw in paper_keywords:\n",
        "#                     if job_kw in paper_kw or paper_kw in job_kw:\n",
        "#                         matched_keyword = job_kw if len(job_kw) > len(paper_kw) else paper_kw\n",
        "#                         template_sentences.append(templates['relatedToPaper'].format(field_name, matched_keyword))\n",
        "#                         keyword_match = True\n",
        "#                         break\n",
        "#                 if keyword_match:\n",
        "#                     break\n",
        "\n",
        "#             # 키워드 매칭이 없어도 일부 조합은 생성\n",
        "#             if len(paper_title) > 5 and len(field_name) > 3:\n",
        "#                 # 10% 확률로 임의 조합 생성\n",
        "#                 if random.random() < 0.1:\n",
        "#                     template_sentences.append(templates['usesResearch'].format(field_name, paper_title))\n",
        "\n",
        "#                     # 수정된 부분: 빈 리스트 체크 추가\n",
        "#                     paper_keywords_list = paper.get('keywords', [])\n",
        "#                     if paper_keywords_list:\n",
        "#                         default_keyword = paper_keywords_list[0]\n",
        "#                     else:\n",
        "#                         default_keyword = '연구'\n",
        "#                     template_sentences.append(templates['requiresKnowledge'].format(field_name, default_keyword))\n",
        "\n",
        "#     # 중복 제거\n",
        "#     template_sentences = list(set(template_sentences))\n",
        "\n",
        "#     # 템플릿 문장이 너무 적으면 기본 템플릿 추가\n",
        "#     if len(template_sentences) < 100:\n",
        "#         print(\"생성된 템플릿 문장이 너무 적어 기본 템플릿을 추가합니다.\")\n",
        "\n",
        "#         # 직무와 스킬 조합으로 기본 템플릿 추가\n",
        "#         for job in job_sample[:50]:\n",
        "#             field_name = job.get('field', '')\n",
        "#             if not field_name:\n",
        "#                 continue\n",
        "\n",
        "#             # 기본 템플릿 추가\n",
        "#             template_sentences.append(f\"{field_name} 분야는 전문 지식이 필요합니다.\")\n",
        "#             template_sentences.append(f\"{field_name} 분야는 연구 결과를 활용합니다.\")\n",
        "\n",
        "#             # 스킬 기반 템플릿 추가\n",
        "#             for skill in job.get('skills', []):\n",
        "#                 if skill:\n",
        "#                     template_sentences.append(f\"{field_name} 분야는 {skill} 기술이 필요합니다.\")\n",
        "\n",
        "#             # 전공 기반 템플릿 추가\n",
        "#             for major in job.get('majors', []):\n",
        "#                 if major:\n",
        "#                     template_sentences.append(f\"{field_name} 분야는 {major} 전공과 관련이 있습니다.\")\n",
        "\n",
        "#     # 최대 5000개 문장만 사용 (최적화)\n",
        "#     if len(template_sentences) > MAX_TEMPLATE_COUNT:\n",
        "#         template_sentences = random.sample(template_sentences, MAX_TEMPLATE_COUNT)\n",
        "#     elif len(template_sentences) == 0:\n",
        "#         # 템플릿 문장이 하나도 없으면 최소한의 문장 생성\n",
        "#         print(\"템플릿 문장이 생성되지 않아 기본 문장을 추가합니다.\")\n",
        "#         for i in range(100):\n",
        "#             template_sentences.append(f\"채용 분야는 관련 기술이 필요합니다_{i}.\")\n",
        "#             template_sentences.append(f\"연구 분야는 관련 지식이 필요합니다_{i}.\")\n",
        "\n",
        "#     print(f\"생성된 고유 템플릿 문장 수: {len(template_sentences)}\")\n",
        "\n",
        "#     return template_sentences\n",
        "\n",
        "def generate_template_sentences(job_fields, papers, templates):\n",
        "    \"\"\"온톨로지 템플릿을 활용하여 학습용 문장 생성 (수정된 온톨로지 구조 지원)\"\"\"\n",
        "    print(\"\\n=== 템플릿 기반 문장 생성 시작 ===\")\n",
        "    template_sentences = []\n",
        "\n",
        "    # 1. 채용 필드 기반 템플릿 문장 생성\n",
        "    for job in tqdm(job_fields, desc=\"Generating job field templates\"):\n",
        "        field_name = job.get('field', '')\n",
        "        if not field_name:\n",
        "            continue\n",
        "\n",
        "        # 전공 관련 문장 생성\n",
        "        for major in job.get('majors', []): #[:3]:  # 최적화: 최대 3개만 사용\n",
        "            if major and len(major) > 1:\n",
        "                template_sentences.append(templates['hasMajor'].format(field_name, major))\n",
        "                template_sentences.append(templates['requiresMajor'].format(field_name, major))\n",
        "\n",
        "        # 기술/스킬 관련 문장 생성\n",
        "        for skill in job.get('skills', [])[:3]:  # 최적화: 최대 3개만 사용\n",
        "            if skill and len(skill) > 1:\n",
        "                template_sentences.append(templates['hasSkill'].format(field_name, skill))\n",
        "                template_sentences.append(templates['requiresSkill'].format(field_name, skill))\n",
        "\n",
        "        # 키워드 관련 문장 생성\n",
        "        for keyword in job.get('keywords', []): #[:3]:  # 최적화: 최대 3개만 사용\n",
        "            if keyword and len(keyword) > 1:\n",
        "                template_sentences.append(templates['hasMajorKeyword'].format(field_name, keyword))\n",
        "\n",
        "        # 업무 관련 문장 생성\n",
        "        if job.get('duties'):\n",
        "            duties = [d.strip() for d in job['duties'].split('.') if d.strip()]\n",
        "            for duty in duties[:2]:  # 최적화: 최대 2개 업무만 사용\n",
        "                if duty and len(duty) > 5:  # 너무 짧은 문장 제외, 길이 제한 조정\n",
        "                    template_sentences.append(templates['hasDuty'].format(field_name, duty))\n",
        "\n",
        "        # 자격 요건 관련 문장 생성\n",
        "        if job.get('qualifications'):\n",
        "            quals = [q.strip() for q in job['qualifications'].split('.') if q.strip()]\n",
        "            for qual in quals[:2]:  # 최적화: 최대 2개 자격요건만 사용\n",
        "                if qual and len(qual) > 5:  # 길이 제한 조정\n",
        "                    template_sentences.append(templates['hasRequirement'].format(field_name, qual))\n",
        "\n",
        "        # 우대사항 관련 문장 생성\n",
        "        if job.get('preferred'):\n",
        "            prefs = [p.strip() for p in job['preferred'].split('.') if p.strip()]\n",
        "            for pref in prefs[:1]:  # 최적화: 최대 1개 우대사항만 사용\n",
        "                if pref and len(pref) > 5:  # 길이 제한 조정\n",
        "                    template_sentences.append(templates['hasPreference'].format(field_name, pref))\n",
        "\n",
        "    # 2. 채용-논문 연결을 위한 추가 템플릿 문장 생성\n",
        "    # 모든 직무와 논문 사용하여 더 많은 연결 생성 시도\n",
        "    job_sample = job_fields[:min(300, len(job_fields))]\n",
        "    paper_sample = papers[:min(500, len(papers))]\n",
        "\n",
        "    for job in tqdm(job_sample, desc=\"Generating job-paper templates\"):\n",
        "        field_name = job.get('field', '')\n",
        "        if not field_name:\n",
        "            continue\n",
        "\n",
        "        # 직무 키워드와 관련된 논문 찾기\n",
        "        job_keywords = set([k.lower() for k in job.get('keywords', []) if k])\n",
        "        job_skills = set([s.lower() for s in job.get('skills', []) if s])\n",
        "\n",
        "        # 키워드나 스킬이 없는 경우, 직무명을 토큰화하여 키워드로 사용\n",
        "        if not job_keywords and field_name:\n",
        "            job_keywords = set([w.lower() for w in field_name.split() if len(w) > 2 and w.lower() not in STOPWORDS])\n",
        "\n",
        "        for paper in paper_sample[:20]:  # 각 직무당 최대 20개 논문 확인\n",
        "            paper_keywords = set([k.lower() for k in paper.get('keywords', []) if k])\n",
        "            paper_title = paper.get('title', '')\n",
        "\n",
        "            # 논문 키워드가 없는 경우, 제목을 토큰화하여 키워드로 사용\n",
        "            if not paper_keywords and paper_title:\n",
        "                paper_keywords = set([w.lower() for w in paper_title.split() if len(w) > 2 and w.lower() not in STOPWORDS])\n",
        "\n",
        "            # 키워드 일치 여부 확인 (부분 일치도 포함)\n",
        "            keyword_match = False\n",
        "            for job_kw in job_keywords:\n",
        "                for paper_kw in paper_keywords:\n",
        "                    if job_kw in paper_kw or paper_kw in job_kw:\n",
        "                        matched_keyword = job_kw if len(job_kw) > len(paper_kw) else paper_kw\n",
        "                        template_sentences.append(templates['relatedToPaper'].format(field_name, matched_keyword))\n",
        "                        keyword_match = True\n",
        "                        break\n",
        "                if keyword_match:\n",
        "                    break\n",
        "\n",
        "            # 키워드 매칭이 없어도 일부 조합은 생성\n",
        "            if len(paper_title) > 5 and len(field_name) > 3:\n",
        "                # 10% 확률로 임의 조합 생성\n",
        "                if random.random() < 0.1:\n",
        "                    template_sentences.append(templates['usesResearch'].format(field_name, paper_title))\n",
        "                    template_sentences.append(templates['requiresKnowledge'].format(field_name, paper.get('keywords', ['연구'])[0]))\n",
        "\n",
        "    # 중복 제거\n",
        "    template_sentences = list(set(template_sentences))\n",
        "\n",
        "    # 템플릿 문장이 너무 적으면 기본 템플릿 추가\n",
        "    if len(template_sentences) < 100:\n",
        "        print(\"생성된 템플릿 문장이 너무 적어 기본 템플릿을 추가합니다.\")\n",
        "\n",
        "        # 직무와 스킬 조합으로 기본 템플릿 추가\n",
        "        for job in job_sample[:50]:\n",
        "            field_name = job.get('field', '')\n",
        "            if not field_name:\n",
        "                continue\n",
        "\n",
        "            # 기본 템플릿 추가\n",
        "            template_sentences.append(f\"{field_name} 분야는 전문 지식이 필요합니다.\")\n",
        "            template_sentences.append(f\"{field_name} 분야는 연구 결과를 활용합니다.\")\n",
        "\n",
        "            # 스킬 기반 템플릿 추가\n",
        "            for skill in job.get('skills', []):\n",
        "                if skill:\n",
        "                    template_sentences.append(f\"{field_name} 분야는 {skill} 기술이 필요합니다.\")\n",
        "\n",
        "            # 전공 기반 템플릿 추가\n",
        "            for major in job.get('majors', []):\n",
        "                if major:\n",
        "                    template_sentences.append(f\"{field_name} 분야는 {major} 전공과 관련이 있습니다.\")\n",
        "\n",
        "    # 최대 5000개 문장만 사용 (최적화)\n",
        "    if len(template_sentences) > MAX_TEMPLATE_COUNT:\n",
        "        template_sentences = random.sample(template_sentences, MAX_TEMPLATE_COUNT)\n",
        "    elif len(template_sentences) == 0:\n",
        "        # 템플릿 문장이 하나도 없으면 최소한의 문장 생성\n",
        "        print(\"템플릿 문장이 생성되지 않아 기본 문장을 추가합니다.\")\n",
        "        for i in range(100):\n",
        "            template_sentences.append(f\"채용 분야는 관련 기술이 필요합니다_{i}.\")\n",
        "            template_sentences.append(f\"연구 분야는 관련 지식이 필요합니다_{i}.\")\n",
        "\n",
        "    print(f\"생성된 고유 템플릿 문장 수: {len(template_sentences)}\")\n",
        "\n",
        "    return template_sentences\n",
        "\n",
        "# 1. MLM 학습을 위한 데이터셋 클래스\n",
        "class DomainTextDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze()\n",
        "        }\n",
        "\n",
        "# 2. 대조학습을 위한 데이터셋 클래스\n",
        "class ContrastivePairDataset(Dataset):\n",
        "    def __init__(self, job_papers_pairs, tokenizer, max_length):\n",
        "        self.pairs = job_papers_pairs\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.pairs[idx]\n",
        "        job = pair['job']\n",
        "        paper = pair['paper']\n",
        "\n",
        "        job_text = job['job_text']\n",
        "        paper_text = paper['paper_text']\n",
        "\n",
        "        job_encoding = self.tokenizer(\n",
        "            job_text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        paper_encoding = self.tokenizer(\n",
        "            paper_text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'job_input_ids': job_encoding['input_ids'].squeeze(),\n",
        "            'job_attention_mask': job_encoding['attention_mask'].squeeze(),\n",
        "            'paper_input_ids': paper_encoding['input_ids'].squeeze(),\n",
        "            'paper_attention_mask': paper_encoding['attention_mask'].squeeze(),\n",
        "            'score': torch.tensor(pair['score'], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "\n",
        "def generate_templates_with_target_success_rate(job_fields, papers, templates,\n",
        "                                               target_min=80, target_max=90,\n",
        "                                               max_attempts=20,\n",
        "                                               threshold_step=0.05):\n",
        "    \"\"\"목표 성공률에 도달할 때까지 템플릿 생성 반복 시도\"\"\"\n",
        "\n",
        "    current_threshold = 0.2  # 초기 임계값 설정\n",
        "\n",
        "    for attempt in range(max_attempts):\n",
        "        print(f\"\\n시도 {attempt+1}/{max_attempts}: 템플릿 생성 중... (임계값: {current_threshold:.2f})\")\n",
        "\n",
        "        # 템플릿 생성 (적절한 랜덤성 추가)\n",
        "        random_seed = 42 + attempt  # 매 시도마다 다른 시드 사용 (SAMPLE_SEED 대신 고정값 사용)\n",
        "        random.seed(random_seed)\n",
        "\n",
        "        # 템플릿 생성\n",
        "        template_sentences = generate_template_sentences(job_fields, papers, templates)\n",
        "\n",
        "        # 템플릿으로 관계 추출 (임계값 조정)\n",
        "        template_relations, matching_stats = extract_relations_from_templates(\n",
        "            template_sentences,\n",
        "            job_fields,\n",
        "            papers,\n",
        "            tokenizer=None,\n",
        "            model=None,\n",
        "            threshold=current_threshold,  # 현재 임계값 사용\n",
        "            device=DEVICE\n",
        "        )\n",
        "\n",
        "        # 성공률 계산\n",
        "        total_attempts = matching_stats['total_matches']\n",
        "        successful_matches = matching_stats['successful_matches']\n",
        "\n",
        "        if total_attempts > 0:\n",
        "            success_rate = (successful_matches / total_attempts) * 100\n",
        "        else:\n",
        "            success_rate = 0\n",
        "\n",
        "        print(f\"템플릿 매칭 결과: 시도 {total_attempts}, 성공 {successful_matches}, 성공률 {success_rate:.2f}%\")\n",
        "\n",
        "        # 목표 성공률 범위 내에 있는지 확인\n",
        "        if target_min <= success_rate <= target_max:\n",
        "            print(f\"목표 성공률 달성! ({success_rate:.2f}%)\")\n",
        "            return template_sentences, template_relations, matching_stats\n",
        "\n",
        "        # 임계값 조정\n",
        "        if success_rate < target_min:\n",
        "            # 성공률이 너무 낮으면 임계값 낮추기\n",
        "            current_threshold -= threshold_step\n",
        "            print(f\"성공률이 너무 낮습니다. 임계값을 {current_threshold:.2f}로 낮춥니다.\")\n",
        "        else:  # success_rate > target_max\n",
        "            # 성공률이 너무 높으면 임계값 높이기\n",
        "            current_threshold += threshold_step\n",
        "            print(f\"성공률이 너무 높습니다. 임계값을 {current_threshold:.2f}로 높입니다.\")\n",
        "\n",
        "        # 임계값 범위 제한 (0.05~0.95)\n",
        "        current_threshold = max(0.05, min(0.95, current_threshold))\n",
        "\n",
        "    # 최대 시도 횟수를 초과해도 목표 성공률에 도달하지 못한 경우\n",
        "    print(f\"최대 시도 횟수({max_attempts})에 도달했으나 목표 성공률에 도달하지 못했습니다.\")\n",
        "    print(f\"마지막 시도의 성공률({success_rate:.2f}%)로 진행합니다.\")\n",
        "\n",
        "    return template_sentences, template_relations, matching_stats\n",
        "\n",
        "# 관계 추출 함수\n",
        "# 최적화된 관계 추출 함수 (배치 처리 및 병렬 처리 적용)\n",
        "# extract_relations_from_templates 함수 수정 - 매칭 통계 추가\n",
        "def extract_relations_from_templates(template_sentences, job_fields, papers, tokenizer=None, model=None, threshold=0.2, device=DEVICE):\n",
        "    \"\"\"템플릿 문장에서 채용공고-논문 관계 추출 (배치 및 병렬 처리 최적화)\"\"\"\n",
        "    relations = {}\n",
        "    matching_stats = {'total_matches': 0, 'successful_matches': 0}  # 매칭 통계 추가\n",
        "\n",
        "    # 토큰화 및 불용어 제거 함수\n",
        "    def tokenize(text):\n",
        "        if not isinstance(text, str):\n",
        "            return []\n",
        "        return [w.lower() for w in text.split() if w.lower() not in STOPWORDS and len(w) > 2]\n",
        "\n",
        "    print(\"템플릿 문장에서 채용공고-논문 관계 추출 중...\")\n",
        "\n",
        "    # 디버깅: 첫 10개 템플릿 문장 출력\n",
        "    print(\"===== 템플릿 문장 샘플 =====\")\n",
        "    for i, sentence in enumerate(template_sentences[:10]):\n",
        "        print(f\"템플릿 문장 {i+1}: {sentence}\")\n",
        "    print(\"===========================\")\n",
        "\n",
        "    # 디버깅: 첫 5개 채용공고 정보 출력\n",
        "    print(\"===== 채용공고 샘플 =====\")\n",
        "    for i, job in enumerate(job_fields[:5]):\n",
        "        job_id = job.get('job_id', 'ID 없음')\n",
        "        field = job.get('field', '분야 없음')\n",
        "        keywords = job.get('keywords', [])\n",
        "        skills = job.get('skills', [])\n",
        "        print(f\"채용공고 {i+1} (ID: {job_id}): 분야={field}, 키워드={keywords}, 기술={skills}\")\n",
        "    print(\"===========================\")\n",
        "\n",
        "    # 디버깅: 첫 5개 논문 정보 출력\n",
        "    print(\"===== 논문 샘플 =====\")\n",
        "    for i, paper in enumerate(papers[:5]):\n",
        "        paper_id = paper.get('paper_id', 'ID 없음')\n",
        "        title = paper.get('title', '제목 없음')\n",
        "        keywords = paper.get('keywords', [])\n",
        "        print(f\"논문 {i+1} (ID: {paper_id}): 제목={title}, 키워드={keywords}\")\n",
        "    print(\"===========================\")\n",
        "\n",
        "    # 샘플 크기 감소 (처리 속도 개선)\n",
        "    job_sample = job_fields[:30]  # 50개에서 30개로 감소\n",
        "    paper_sample = papers[:50]    # 100개에서 50개로 감소\n",
        "\n",
        "    # 논문 및 채용공고 전처리 (병렬 처리)\n",
        "    def preprocess_job(job):\n",
        "        job_id = job.get('job_id', '')\n",
        "        field_name = job.get('field', '')\n",
        "        job_keywords = set([k.lower() for k in job.get('keywords', []) if k])\n",
        "        job_skills = set([s.lower() for s in job.get('skills', []) if s])\n",
        "\n",
        "        # 키워드가 없으면 필드명에서 추출\n",
        "        if not job_keywords and field_name:\n",
        "            job_keywords = set([w.lower() for w in field_name.split()\n",
        "                              if len(w) > 2 and w.lower() not in STOPWORDS])\n",
        "\n",
        "        return {\n",
        "            'job_id': job_id,\n",
        "            'field_name': field_name,\n",
        "            'job_keywords': job_keywords,\n",
        "            'job_skills': job_skills\n",
        "        }\n",
        "\n",
        "    def preprocess_paper(paper):\n",
        "        paper_id = paper.get('paper_id', '')\n",
        "        paper_title = paper.get('title', '')\n",
        "        paper_keywords = set([k.lower() for k in paper.get('keywords', []) if k])\n",
        "\n",
        "        # 키워드가 없으면 제목에서 추출\n",
        "        if not paper_keywords and paper_title:\n",
        "            paper_keywords = set([w.lower() for w in paper_title.split()\n",
        "                               if len(w) > 2 and w.lower() not in STOPWORDS])\n",
        "\n",
        "        title_tokens = tokenize(paper_title) if paper_title else []\n",
        "\n",
        "        return {\n",
        "            'paper_id': paper_id,\n",
        "            'paper_title': paper_title,\n",
        "            'paper_keywords': paper_keywords,\n",
        "            'title_tokens': title_tokens\n",
        "        }\n",
        "\n",
        "    # 병렬 전처리 실행\n",
        "    print(\"채용공고 및 논문 데이터 전처리 중...\")\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        processed_jobs = list(executor.map(preprocess_job, job_sample))\n",
        "        processed_papers = list(executor.map(preprocess_paper, paper_sample))\n",
        "\n",
        "    print(f\"전처리 완료: {len(processed_jobs)} 채용공고, {len(processed_papers)} 논문\")\n",
        "\n",
        "    # 템플릿 문장 토큰화 (병렬 처리)\n",
        "    def process_template(sentence):\n",
        "        return {'text': sentence, 'tokens': tokenize(sentence)}\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        processed_templates = list(executor.map(process_template, template_sentences))\n",
        "\n",
        "    # 배치 매칭 함수 (여러 채용공고-논문 쌍을 한 번에 처리)\n",
        "    def process_template_batch(template_batch, jobs, papers):\n",
        "        local_relations = {}\n",
        "        match_attempts = 0\n",
        "        match_success = 0\n",
        "\n",
        "        for template in template_batch:\n",
        "            sentence = template['text']\n",
        "            sentence_tokens = template['tokens']\n",
        "\n",
        "            for job in jobs:\n",
        "                job_id = job['job_id']\n",
        "                field_name = job['field_name']\n",
        "\n",
        "                # 필드명이 템플릿에 없으면 스킵 (빠른 필터링)\n",
        "                if not field_name or field_name.lower() not in sentence.lower():\n",
        "                    continue\n",
        "\n",
        "                job_keywords = job['job_keywords']\n",
        "\n",
        "                # 채용공고 키워드와 템플릿 문장의 일치도 확인\n",
        "                job_relevance = 0\n",
        "                if job_keywords:\n",
        "                    common_tokens = job_keywords.intersection(set(sentence_tokens))\n",
        "                    if common_tokens:\n",
        "                        job_relevance = len(common_tokens) / len(job_keywords)\n",
        "\n",
        "                # 관련성이 없으면 스킵 (빠른 필터링)\n",
        "                if job_relevance == 0:\n",
        "                    continue\n",
        "\n",
        "                for paper in papers:\n",
        "                    match_attempts += 1\n",
        "                    paper_id = paper['paper_id']\n",
        "                    paper_keywords = paper['paper_keywords']\n",
        "                    title_tokens = paper['title_tokens']\n",
        "\n",
        "                    # 논문 키워드와 템플릿 문장의 일치도 확인\n",
        "                    paper_relevance = 0\n",
        "                    if paper_keywords:\n",
        "                        common_tokens = paper_keywords.intersection(set(sentence_tokens))\n",
        "                        if common_tokens:\n",
        "                            paper_relevance = len(common_tokens) / len(paper_keywords)\n",
        "\n",
        "                    # 논문 제목의 단어가 템플릿 문장에 포함되는지 확인\n",
        "                    title_relevance = 0\n",
        "                    if title_tokens:\n",
        "                        common_title_tokens = set(title_tokens).intersection(set(sentence_tokens))\n",
        "                        if common_title_tokens:\n",
        "                            title_relevance = len(common_title_tokens) / len(title_tokens)\n",
        "\n",
        "                    # 채용공고-논문 키워드 직접 매칭\n",
        "                    keyword_match = 0\n",
        "                    if job_keywords and paper_keywords:\n",
        "                        common_keywords = job_keywords.intersection(paper_keywords)\n",
        "                        if common_keywords:\n",
        "                            keyword_match = len(common_keywords) / min(len(job_keywords), len(paper_keywords))\n",
        "\n",
        "                    # 부분 키워드 매칭 (최적화 버전)\n",
        "                    partial_match = 0\n",
        "                    if job_keywords and paper_keywords:\n",
        "                        for j_kw in job_keywords:\n",
        "                            if partial_match > 0:\n",
        "                                break\n",
        "                            if len(j_kw) <= 2:\n",
        "                                continue\n",
        "                            for p_kw in paper_keywords:\n",
        "                                if len(p_kw) <= 2:\n",
        "                                    continue\n",
        "                                if j_kw in p_kw or p_kw in j_kw:\n",
        "                                    partial_match = 0.5\n",
        "                                    break\n",
        "\n",
        "                    # 총 매칭 점수 계산\n",
        "                    match_score = (job_relevance * 0.3 +\n",
        "                                 paper_relevance * 0.3 +\n",
        "                                 title_relevance * 0.2 +\n",
        "                                 keyword_match * 0.3 +\n",
        "                                 partial_match * 0.2)\n",
        "\n",
        "                    # 임계값 이상인 경우 관계 추가\n",
        "                    if match_score > threshold:  # 파라미터로 받은 임계값 사용\n",
        "                        match_success += 1\n",
        "                        key = (job_id, paper_id)\n",
        "                        if key not in local_relations:\n",
        "                            local_relations[key] = 0\n",
        "                        local_relations[key] += match_score\n",
        "\n",
        "        return local_relations, match_attempts, match_success\n",
        "\n",
        "    # 템플릿 문장을 배치로 나누어 병렬 처리\n",
        "    batch_size = 100  # 배치당 템플릿 문장 수\n",
        "    template_batches = [processed_templates[i:i+batch_size]\n",
        "                        for i in range(0, len(processed_templates), batch_size)]\n",
        "\n",
        "    total_match_attempts = 0\n",
        "    total_match_success = 0\n",
        "\n",
        "    print(f\"템플릿 문장 처리 중... (총 {len(template_batches)} 배치)\")\n",
        "\n",
        "    # 병렬 처리를 위한 함수\n",
        "    process_func = partial(process_template_batch,\n",
        "                          jobs=processed_jobs,\n",
        "                          papers=processed_papers)\n",
        "\n",
        "    # 병렬 배치 처리 실행\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        results = list(tqdm(executor.map(process_func, template_batches),\n",
        "                           total=len(template_batches),\n",
        "                           desc=\"템플릿 배치 처리\"))\n",
        "\n",
        "    # 결과 합치기\n",
        "    for batch_relations, attempts, success in results:\n",
        "        total_match_attempts += attempts\n",
        "        total_match_success += success\n",
        "\n",
        "        # 관계 병합\n",
        "        for key, score in batch_relations.items():\n",
        "            if key not in relations:\n",
        "                relations[key] = 0\n",
        "            relations[key] += score\n",
        "\n",
        "    # 디버깅: 매칭 통계 출력\n",
        "    success_rate = (total_match_success/max(1, total_match_attempts))*100\n",
        "    print(f\"\\n총 매칭 시도: {total_match_attempts}, 성공: {total_match_success}, 성공률: {success_rate:.2f}%\")\n",
        "\n",
        "    # 매칭 통계 저장\n",
        "    matching_stats['total_matches'] = total_match_attempts\n",
        "    matching_stats['successful_matches'] = total_match_success\n",
        "\n",
        "    # 관계가 너무 적으면 기본 관계 생성\n",
        "    if len(relations) < 100:\n",
        "        print(\"매칭된 관계가 적어 기본 관계를 생성합니다.\")\n",
        "\n",
        "        # 채용공고-논문 직접 키워드 매칭으로 기본 관계 생성\n",
        "        def generate_basic_relations(job_batch, papers):\n",
        "            local_relations = {}\n",
        "\n",
        "            for job in job_batch:\n",
        "                job_id = job['job_id']\n",
        "                job_keywords = job['job_keywords']\n",
        "                job_field = job['field_name']\n",
        "\n",
        "                if not job_id or not job_keywords:\n",
        "                    continue\n",
        "\n",
        "                for paper in papers:\n",
        "                    paper_id = paper['paper_id']\n",
        "                    paper_keywords = paper['paper_keywords']\n",
        "\n",
        "                    if not paper_id or not paper_keywords:\n",
        "                        continue\n",
        "\n",
        "                    # 키워드 교집합으로 관계 생성\n",
        "                    common_keywords = job_keywords.intersection(paper_keywords)\n",
        "\n",
        "                    # 부분 매칭 확인\n",
        "                    has_partial_match = False\n",
        "                    for j_kw in job_keywords:\n",
        "                        if has_partial_match:\n",
        "                            break\n",
        "                        if len(j_kw) <= 2:\n",
        "                            continue\n",
        "                        for p_kw in paper_keywords:\n",
        "                            if len(p_kw) <= 2:\n",
        "                                continue\n",
        "                            if j_kw in p_kw or p_kw in j_kw:\n",
        "                                has_partial_match = True\n",
        "                                break\n",
        "\n",
        "                    # 관계 추가\n",
        "                    if common_keywords or has_partial_match:\n",
        "                        key = (job_id, paper_id)\n",
        "                        local_relations[key] = len(common_keywords) * 0.5 + (0.3 if has_partial_match else 0)\n",
        "\n",
        "            return local_relations\n",
        "\n",
        "        # 채용공고를 배치로 나누어 병렬 처리\n",
        "        job_batches = [processed_jobs[i:i+10] for i in range(0, len(processed_jobs), 10)]\n",
        "\n",
        "        # 병렬 처리\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
        "            basic_relations_list = list(executor.map(\n",
        "                lambda job_batch: generate_basic_relations(job_batch, processed_papers),\n",
        "                job_batches))\n",
        "\n",
        "        # 기본 관계 병합\n",
        "        for basic_relations in basic_relations_list:\n",
        "            for key, score in basic_relations.items():\n",
        "                if key not in relations:\n",
        "                    relations[key] = 0\n",
        "                relations[key] += score\n",
        "\n",
        "        print(f\"기본 관계를 추가하여 총 {len(relations)}개의 관계가 생성되었습니다.\")\n",
        "\n",
        "    print(f\"템플릿 문장에서 {len(relations)}개의 관계를 추출했습니다.\")\n",
        "    return relations, matching_stats  # 수정: 관계와 매칭 통계 함께 반환\n",
        "\n",
        "# 약한 감독 신호 생성 함수\n",
        "# 최적화된 약한 감독 신호 생성 함수 (배치 처리 적용)\n",
        "def generate_weak_supervision_pairs(job_fields, papers, template_sentences=None, simcse_model=None, tokenizer=None, device=None):\n",
        "    \"\"\"키워드, 카테고리, 텍스트 유사도 등을 활용한 약한 감독 신호 생성 - 배치 처리 최적화\"\"\"\n",
        "    print(\"Generating weak supervision pairs with batch processing...\")\n",
        "    pairs = []\n",
        "\n",
        "    # 템플릿 문장 기반 관계 추출\n",
        "    template_relations = {}\n",
        "    if template_sentences:\n",
        "        print(\"Extracting relations from template sentences...\")\n",
        "        # 개선된 관계 추출 함수 사용\n",
        "        template_sentences, template_relations, matching_stats = generate_templates_with_target_success_rate(\n",
        "            job_fields,\n",
        "            papers,\n",
        "            templates,\n",
        "            target_min=TEMPLATE_TARGET_MIN,  # 목표 최소 성공률\n",
        "            target_max=TEMPLATE_TARGET_MAX,  # 목표 최대 성공률\n",
        "            max_attempts=TEMPLATE_TARGET_TRY_LIMIT  # 최대 시도 횟수\n",
        "        )\n",
        "        # template_relations = extract_relations_from_templates(\n",
        "        #     template_sentences,\n",
        "        #     job_fields,\n",
        "        #     papers,\n",
        "        #     tokenizer=tokenizer,\n",
        "        #     model=simcse_model,\n",
        "        #     device=device\n",
        "        # )\n",
        "\n",
        "    print(f\"Extracted {len(template_relations)} relations from template sentences\")\n",
        "\n",
        "    # 모델 사용 여부 확인 - Hi-BERT 사용\n",
        "    use_semantic_similarity = simcse_model is not None and tokenizer is not None and device is not None\n",
        "\n",
        "    if use_semantic_similarity:\n",
        "        print(\"Using Hi-BERT for semantic similarity calculation\")\n",
        "        simcse_model.eval()  # 평가 모드 설정\n",
        "\n",
        "    # MAX_JOB_COUNT 적용\n",
        "    job_count = min(MAX_JOB_COUNT if MAX_JOB_COUNT is not None else 300, len(job_fields))\n",
        "    job_sample = job_fields[:job_count]\n",
        "\n",
        "    # 논문 샘플 수 감소 (최적화)\n",
        "    max_papers = min(500, len(papers))  # 1000개에서 500개로 감소\n",
        "    paper_sample = random.sample(papers, max_papers)\n",
        "\n",
        "    print(f\"Processing {len(job_sample)} jobs and {len(paper_sample)} papers\")\n",
        "\n",
        "    # 배치 단위 처리를 위한 준비\n",
        "    batch_size = 32  # 배치 크기\n",
        "    job_batches = [job_sample[i:i+batch_size] for i in range(0, len(job_sample), batch_size)]\n",
        "\n",
        "    all_pairs = []\n",
        "\n",
        "    for batch_idx, job_batch in enumerate(tqdm(job_batches, desc=\"Processing job batches\")):\n",
        "        # 배치 내 모든 채용공고의 키워드, 전공, 스킬 정보 준비\n",
        "        batch_job_data = []\n",
        "        batch_job_texts = []\n",
        "        batch_job_input_ids = []\n",
        "        batch_job_attention_masks = []\n",
        "\n",
        "        for job in job_batch:\n",
        "            job_keywords = set([k.lower() for k in job.get('keywords', []) if k])\n",
        "            job_majors = set([m.lower() for m in job.get('majors', []) if m])\n",
        "            job_skills = set([s.lower() for s in job.get('skills', []) if s])\n",
        "\n",
        "            # 키워드가 없는 경우 필드명에서 추출\n",
        "            if not job_keywords and job.get('field'):\n",
        "                field_tokens = [w.lower() for w in job['field'].split() if len(w) > 2 and w.lower() not in STOPWORDS]\n",
        "                job_keywords = set(field_tokens)\n",
        "\n",
        "            # 기술 용어를 키워드에 추가\n",
        "            for skill in job_skills:\n",
        "                if len(skill) > 2 and skill.lower() not in STOPWORDS:\n",
        "                    job_keywords.add(skill.lower())\n",
        "\n",
        "            # 채용공고 텍스트 준비\n",
        "            job_text = f\"분야: {job.get('field', '')} 업무: {job.get('duties', '')} 자격요건: {job.get('qualifications', '')} \" \\\n",
        "                    f\"전공: {' '.join(job.get('majors', []))} 키워드: {' '.join(job.get('keywords', []))} 기술: {' '.join(job.get('skills', []))}\"\n",
        "\n",
        "            batch_job_data.append({\n",
        "                'job': job,\n",
        "                'keywords': job_keywords,\n",
        "                'majors': job_majors,\n",
        "                'skills': job_skills\n",
        "            })\n",
        "            batch_job_texts.append(job_text)\n",
        "\n",
        "        # 채용공고 임베딩 배치 계산 (모델 사용 시)\n",
        "        batch_job_embeddings = None\n",
        "        if use_semantic_similarity:\n",
        "            # 토큰화\n",
        "            job_encodings = tokenizer(\n",
        "                batch_job_texts,\n",
        "                truncation=True,\n",
        "                max_length=MAX_LENGTH,\n",
        "                padding='max_length',\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            batch_job_input_ids = job_encodings['input_ids'].to(device)\n",
        "            batch_job_attention_masks = job_encodings['attention_mask'].to(device)\n",
        "\n",
        "            # 배치 임베딩 계산\n",
        "            with torch.no_grad():\n",
        "                # 배치가 너무 크면 서브 배치로 나누어 처리\n",
        "                sub_batch_size = 16  # GPU 메모리에 맞게 조정\n",
        "                batch_job_embeddings = []\n",
        "\n",
        "                for i in range(0, len(batch_job_input_ids), sub_batch_size):\n",
        "                    sub_input_ids = batch_job_input_ids[i:i+sub_batch_size]\n",
        "                    sub_attention_masks = batch_job_attention_masks[i:i+sub_batch_size]\n",
        "                    sub_embeddings = simcse_model.encode_job(sub_input_ids, sub_attention_masks)\n",
        "                    batch_job_embeddings.append(sub_embeddings)\n",
        "\n",
        "                # 서브 배치 임베딩 합치기\n",
        "                batch_job_embeddings = torch.cat(batch_job_embeddings, dim=0)\n",
        "\n",
        "        # 논문 처리 (배치 단위)\n",
        "        # 논문을 여러 배치로 나누어 처리\n",
        "        paper_batch_size = 50  # 논문 배치 크기\n",
        "        paper_batches = [paper_sample[i:i+paper_batch_size] for i in range(0, len(paper_sample), paper_batch_size)]\n",
        "\n",
        "        for job_idx, job_data in enumerate(batch_job_data):\n",
        "            job = job_data['job']\n",
        "            job_keywords = job_data['keywords']\n",
        "            job_majors = job_data['majors']\n",
        "            job_skills = job_data['skills']\n",
        "\n",
        "            # 현재 채용공고의 임베딩\n",
        "            job_emb = None\n",
        "            if batch_job_embeddings is not None:\n",
        "                job_emb = batch_job_embeddings[job_idx].unsqueeze(0)  # [1, dim]\n",
        "\n",
        "            # 점수화된 논문 저장\n",
        "            scored_papers = []\n",
        "\n",
        "            # 논문 배치 처리\n",
        "            for paper_batch in paper_batches:\n",
        "                batch_paper_data = []\n",
        "                batch_paper_texts = []\n",
        "\n",
        "                for paper in paper_batch:\n",
        "                    paper_keywords = set([k.lower() for k in paper.get('keywords', []) if k])\n",
        "\n",
        "                    # 논문 키워드가 없는 경우 제목에서 추출\n",
        "                    if not paper_keywords and paper.get('title'):\n",
        "                        title_tokens = [w.lower() for w in paper['title'].split() if len(w) > 2 and w.lower() not in STOPWORDS]\n",
        "                        paper_keywords = set(title_tokens)\n",
        "\n",
        "                    # 초록에서 중요 단어 추출\n",
        "                    if paper.get('abstract'):\n",
        "                        abstract_words = [w.lower() for w in paper.get('abstract', '').split()\n",
        "                                        if len(w) > 3 and w.lower() not in STOPWORDS]\n",
        "                        # 빈도수가 높은 단어 추가 (최대 3개)\n",
        "                        word_count = {}\n",
        "                        for word in abstract_words:\n",
        "                            word_count[word] = word_count.get(word, 0) + 1\n",
        "\n",
        "                        # 빈도수 기준 상위 3개 단어 추가\n",
        "                        top_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "                        for word, _ in top_words:\n",
        "                            paper_keywords.add(word)\n",
        "\n",
        "                    paper_text = f\"제목: {paper.get('title', '')} 키워드: {', '.join(paper_keywords)} 초록: {paper.get('abstract', '')}\"\n",
        "\n",
        "                    batch_paper_data.append({\n",
        "                        'paper': paper,\n",
        "                        'keywords': paper_keywords\n",
        "                    })\n",
        "                    batch_paper_texts.append(paper_text)\n",
        "\n",
        "                # 논문 임베딩 배치 계산 (모델 사용 시)\n",
        "                if use_semantic_similarity and job_emb is not None:\n",
        "                    # 토큰화\n",
        "                    paper_encodings = tokenizer(\n",
        "                        batch_paper_texts,\n",
        "                        truncation=True,\n",
        "                        max_length=MAX_LENGTH,\n",
        "                        padding='max_length',\n",
        "                        return_tensors='pt'\n",
        "                    )\n",
        "\n",
        "                    batch_paper_input_ids = paper_encodings['input_ids'].to(device)\n",
        "                    batch_paper_attention_masks = paper_encodings['attention_mask'].to(device)\n",
        "\n",
        "                    # 배치 임베딩 계산\n",
        "                    with torch.no_grad():\n",
        "                        # 배치가 너무 크면 서브 배치로 나누어 처리\n",
        "                        sub_batch_size = 16  # GPU 메모리에 맞게 조정\n",
        "                        batch_paper_embeddings = []\n",
        "\n",
        "                        for i in range(0, len(batch_paper_input_ids), sub_batch_size):\n",
        "                            sub_input_ids = batch_paper_input_ids[i:i+sub_batch_size]\n",
        "                            sub_attention_masks = batch_paper_attention_masks[i:i+sub_batch_size]\n",
        "                            sub_embeddings = simcse_model.encode_paper(sub_input_ids, sub_attention_masks)\n",
        "                            batch_paper_embeddings.append(sub_embeddings)\n",
        "\n",
        "                        # 서브 배치 임베딩 합치기\n",
        "                        batch_paper_embeddings = torch.cat(batch_paper_embeddings, dim=0)\n",
        "\n",
        "                        # 일괄 유사도 계산 (batch matrix multiplication)\n",
        "                        # job_emb: [1, dim], batch_paper_embeddings: [batch_size, dim]\n",
        "                        # similarity: [1, batch_size]\n",
        "                        similarities = torch.matmul(job_emb, batch_paper_embeddings.t())\n",
        "                        similarities = (similarities + 1) / 2  # 범위 조정 (-1~1 -> 0~1)\n",
        "\n",
        "                # 배치 내 각 논문 처리\n",
        "                for i, paper_data in enumerate(batch_paper_data):\n",
        "                    paper = paper_data['paper']\n",
        "                    paper_keywords = paper_data['keywords']\n",
        "\n",
        "                    # 키워드 매칭 점수\n",
        "                    keyword_overlap = len(job_keywords.intersection(paper_keywords))\n",
        "                    keyword_match = min(keyword_overlap * 0.2, 0.4)  # 최대 0.4점\n",
        "\n",
        "                    # 키워드 부분 매칭 점수 (최적화)\n",
        "                    partial_match = 0\n",
        "                    for j_kw in job_keywords:\n",
        "                        if len(j_kw) <= 2:\n",
        "                            continue\n",
        "                        for p_kw in paper_keywords:\n",
        "                            if len(p_kw) <= 2:\n",
        "                                continue\n",
        "                            if j_kw in p_kw or p_kw in j_kw:\n",
        "                                partial_match += 0.1\n",
        "                                break\n",
        "                    partial_match = min(partial_match, 0.3)  # 최대 0.3점\n",
        "\n",
        "                    # 전공-카테고리 매칭 점수\n",
        "                    major_match = 0\n",
        "                    if paper.get('category'):\n",
        "                        paper_category = paper['category'].lower()\n",
        "                        for major in job_majors:\n",
        "                            if major in paper_category:\n",
        "                                major_match = 0.15\n",
        "                                break\n",
        "\n",
        "                    # 기술-초록 매칭 점수\n",
        "                    skill_match = 0\n",
        "                    if paper.get('abstract'):\n",
        "                        abstract_lower = paper['abstract'].lower()\n",
        "                        for skill in job_skills:\n",
        "                            if skill in abstract_lower:\n",
        "                                skill_match += 0.15\n",
        "                                break\n",
        "\n",
        "                    # 분야-제목 매칭 점수\n",
        "                    field_match = 0\n",
        "                    if job.get('field') and paper.get('title'):\n",
        "                        field_words = [w.lower() for w in job['field'].split() if w.lower() not in STOPWORDS and len(w) > 2]\n",
        "                        title_lower = paper['title'].lower()\n",
        "                        for word in field_words:\n",
        "                            if word in title_lower:\n",
        "                                field_match += 0.15\n",
        "                                break\n",
        "\n",
        "                    # 템플릿 기반 관계 점수\n",
        "                    template_match = 0\n",
        "                    if template_relations:\n",
        "                        key = (job['job_id'], paper['paper_id'])\n",
        "                        if key in template_relations:\n",
        "                            template_match = min(template_relations[key] * 0.2, 0.2)  # 최대 0.2점\n",
        "\n",
        "                    # 의미적 유사도 점수\n",
        "                    semantic_similarity = 0\n",
        "                    if use_semantic_similarity and similarities is not None:\n",
        "                        semantic_similarity = similarities[0, i].item()\n",
        "\n",
        "                    # 종합 점수 계산\n",
        "                    if use_semantic_similarity:\n",
        "                        score = (\n",
        "                            (keyword_match * 0.2) +\n",
        "                            (partial_match * 0.2) +\n",
        "                            (major_match * 0.1) +\n",
        "                            (skill_match * 0.15) +\n",
        "                            (field_match * 0.15) +\n",
        "                            (template_match * 0.1) +\n",
        "                            (semantic_similarity * 0.3)\n",
        "                        )\n",
        "                    else:\n",
        "                        score = (\n",
        "                            (keyword_match * 0.25) +\n",
        "                            (partial_match * 0.25) +\n",
        "                            (major_match * 0.15) +\n",
        "                            (skill_match * 0.15) +\n",
        "                            (field_match * 0.15) +\n",
        "                            (template_match * 0.15)\n",
        "                        )\n",
        "\n",
        "                    # 최소 점수 부여 (일부 랜덤 매칭)\n",
        "                    if score == 0 and random.random() < 0.1:\n",
        "                        score = 0.1\n",
        "\n",
        "                    # 최소 점수 이상인 경우만 후보로 추가\n",
        "                    if score > 0:\n",
        "                        scored_papers.append((paper, score))\n",
        "\n",
        "            # 점수로 정렬\n",
        "            scored_papers.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # 각 직무당 페어 생성 (최소 1개는 생성)\n",
        "            if scored_papers:\n",
        "                # 상위 5개를 긍정 샘플로 간주\n",
        "                for i, (paper, score) in enumerate(scored_papers[:5]):\n",
        "                    all_pairs.append({\n",
        "                        'job': job,\n",
        "                        'paper': paper,\n",
        "                        'score': score,\n",
        "                        'is_positive': 1\n",
        "                    })\n",
        "\n",
        "                # 그 다음 3개 논문 (하드 네거티브)\n",
        "                for i, (paper, score) in enumerate(scored_papers[5:8]):\n",
        "                    all_pairs.append({\n",
        "                        'job': job,\n",
        "                        'paper': paper,\n",
        "                        'score': 0.0,  # 부정 샘플은 점수를 0으로 설정\n",
        "                        'is_positive': 0\n",
        "                    })\n",
        "            else:\n",
        "                # 매칭된 논문이 없는 경우, 랜덤 논문 선택\n",
        "                random_paper = random.choice(paper_sample)\n",
        "                all_pairs.append({\n",
        "                    'job': job,\n",
        "                    'paper': random_paper,\n",
        "                    'score': 0.15,\n",
        "                    'is_positive': 1\n",
        "                })\n",
        "\n",
        "    # 최대 페어 수 제한\n",
        "    max_pairs = 15000\n",
        "    if len(all_pairs) > max_pairs:\n",
        "        all_pairs = random.sample(all_pairs, max_pairs)\n",
        "\n",
        "    print(f\"Generated {len(all_pairs)} weak supervision pairs\")\n",
        "    return all_pairs\n",
        "\n",
        "# Hi-BERT 이중 인코더 모델 정의\n",
        "class DualEncoderModel(torch.nn.Module):\n",
        "    def __init__(self, base_model, projection_dim=768):\n",
        "        super(DualEncoderModel, self).__init__()\n",
        "        # 모델 이름 속성 추가 - Hi-BERT 식별용\n",
        "        self.model_name = \"Hi-BERT\"\n",
        "\n",
        "        # BERT 모델\n",
        "        self.bert = base_model\n",
        "\n",
        "        # 채용정보와 논문을 위한 별도의 프로젝션 레이어 (더 깊은 네트워크로 변경)\n",
        "        self.job_projection = torch.nn.Sequential(\n",
        "            torch.nn.Linear(768, 768),\n",
        "            torch.nn.ReLU(),\n",
        "            # torch.nn.Dropout(0.2),  # 드롭아웃 추가\n",
        "            torch.nn.Linear(768, projection_dim)\n",
        "        )\n",
        "\n",
        "        self.paper_projection = torch.nn.Sequential(\n",
        "            torch.nn.Linear(768, 768),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(768, projection_dim)\n",
        "        )\n",
        "\n",
        "        # LayerNorm 추가 (정규화 성능 향상)\n",
        "        self.job_norm = torch.nn.LayerNorm(projection_dim)\n",
        "        self.paper_norm = torch.nn.LayerNorm(projection_dim)\n",
        "\n",
        "        # 드롭아웃 레이어\n",
        "        self.dropout = torch.nn.Dropout(HBN_DROPOUT)\n",
        "\n",
        "        # 가중치 초기화 추가\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"가중치 초기화 함수\"\"\"\n",
        "        for module in [self.job_projection, self.paper_projection]:\n",
        "            for m in module.modules():\n",
        "                if isinstance(m, torch.nn.Linear):\n",
        "                    torch.nn.init.xavier_uniform_(m.weight)\n",
        "                    if m.bias is not None:\n",
        "                        torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def encode_job(self, input_ids, attention_mask):\n",
        "        # 채용정보 인코딩\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        if hasattr(outputs, 'pooler_output'):\n",
        "            pooled_output = outputs.pooler_output\n",
        "        else:\n",
        "            pooled_output = outputs[1]  # 튜플로 반환되는 경우\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        job_emb = self.job_projection(pooled_output)\n",
        "        job_emb = self.job_norm(job_emb)  # LayerNorm 적용\n",
        "\n",
        "        # L2 정규화\n",
        "        job_emb = job_emb / (torch.norm(job_emb, dim=1, keepdim=True) + 1e-12)\n",
        "        return job_emb\n",
        "\n",
        "    def encode_paper(self, input_ids, attention_mask):\n",
        "        # 논문 인코딩\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        if hasattr(outputs, 'pooler_output'):\n",
        "            pooled_output = outputs.pooler_output\n",
        "        else:\n",
        "            pooled_output = outputs[1]  # 튜플로 반환되는 경우\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        paper_emb = self.paper_projection(pooled_output)\n",
        "        paper_emb = self.paper_norm(paper_emb)  # LayerNorm 적용\n",
        "\n",
        "        # L2 정규화\n",
        "        paper_emb = paper_emb / (torch.norm(paper_emb, dim=1, keepdim=True) + 1e-12)\n",
        "        return paper_emb\n",
        "\n",
        "    def forward(self, job_input_ids, job_attention_mask, paper_input_ids, paper_attention_mask):\n",
        "        # 채용정보와 논문 모두 인코딩\n",
        "        job_emb = self.encode_job(job_input_ids, job_attention_mask)\n",
        "        paper_emb = self.encode_paper(paper_input_ids, paper_attention_mask)\n",
        "\n",
        "        return job_emb, paper_emb\n",
        "\n",
        "# 대조 손실 함수 (개선된 버전)\n",
        "def contrastive_loss(job_emb, paper_emb, temperature=TEMPERATURE, margin=MARGIN):\n",
        "    \"\"\"인배치 대조 손실 함수 - 마진 손실 추가\"\"\"\n",
        "    # 벡터 정규화 (이미 모델에서 정규화했으므로 중복일 수 있음)\n",
        "    job_emb = job_emb / job_emb.norm(dim=1, keepdim=True)\n",
        "    paper_emb = paper_emb / paper_emb.norm(dim=1, keepdim=True)\n",
        "\n",
        "    # 코사인 유사도 행렬 계산 (batch_size x batch_size)\n",
        "    logits = torch.matmul(job_emb, paper_emb.t()) / temperature\n",
        "\n",
        "    # 대각선 요소가 양성 쌍이 되도록 레이블 설정\n",
        "    labels = torch.arange(logits.size(0), device=logits.device)\n",
        "\n",
        "    # 교차 엔트로피 손실 계산\n",
        "    ce_loss = torch.nn.CrossEntropyLoss()(logits, labels)\n",
        "\n",
        "    # 마진 손실 추가 (하드 네거티브 샘플링)\n",
        "    batch_size = job_emb.size(0)\n",
        "    mask = torch.eye(batch_size, device=job_emb.device)\n",
        "\n",
        "    # 양성 쌍의 유사도\n",
        "    positive_pairs = torch.sum(job_emb * paper_emb, dim=1)\n",
        "\n",
        "    # 음성 쌍의 최대 유사도 (하드 네거티브)\n",
        "    negative_pairs = torch.matmul(job_emb, paper_emb.t()) * (1 - mask)\n",
        "    hardest_negatives, _ = torch.max(negative_pairs, dim=1)\n",
        "\n",
        "    # 마진 손실\n",
        "    margin_loss = torch.nn.functional.relu(margin - positive_pairs + hardest_negatives).mean()\n",
        "\n",
        "    # 최종 손실 (CE 손실 + 마진 손실)\n",
        "    total_loss = ce_loss + margin_loss\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "# MLM 학습 함수\n",
        "def train_domain_adapted_mlm(job_fields, papers, tokenizer, model, output_dir, template_sentences=None, reset_checkpoints=False):\n",
        "    \"\"\"도메인 적응을 위한 MLM 학습 (체크포인트 기능 추가)\"\"\"\n",
        "    print(\"\\n=== 도메인 적응 MLM 학습 시작 ===\")\n",
        "\n",
        "    # 체크포인트 디렉토리 확인\n",
        "    mlm_checkpoint_dir = f\"{output_dir}/mlm_checkpoints\"\n",
        "    os.makedirs(mlm_checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # 체크포인트 정보 파일\n",
        "    checkpoint_info_path = f\"{mlm_checkpoint_dir}/checkpoint_info.json\"\n",
        "\n",
        "    # 체크포인트 초기화가 요청된 경우 기존 체크포인트 정보 파일 삭제\n",
        "    if reset_checkpoints and os.path.exists(checkpoint_info_path):\n",
        "        os.remove(checkpoint_info_path)\n",
        "        print(\"MLM 체크포인트 정보 파일이 초기화되었습니다.\")\n",
        "\n",
        "    # 체크포인트 확인\n",
        "    checkpoint_exists = os.path.exists(checkpoint_info_path) and not reset_checkpoints\n",
        "    resume_training = False\n",
        "    start_epoch = 0\n",
        "    global_step = 0\n",
        "\n",
        "    # 특수 토큰 추가\n",
        "    special_tokens = {\"additional_special_tokens\": [\"[JOB]\", \"[PAPER]\", \"[TEMPLATE]\"]}\n",
        "    tokenizer.add_special_tokens(special_tokens)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # 텍스트 구성\n",
        "    mlm_texts = []\n",
        "\n",
        "    # 채용공고 텍스트 추가 (데이터 양 제한)\n",
        "    job_sample = random.sample(job_fields, min(500, len(job_fields)))\n",
        "    for job in job_sample:\n",
        "        job_text = f\"[JOB] {job['job_text']}\"\n",
        "        mlm_texts.append(job_text)\n",
        "\n",
        "    # 논문 텍스트 추가 (데이터 양 제한)\n",
        "    paper_sample = random.sample(papers, min(2000, len(papers)))\n",
        "    for paper in paper_sample:\n",
        "        paper_text = f\"[PAPER] {paper['paper_text']}\"\n",
        "        mlm_texts.append(paper_text)\n",
        "\n",
        "    # 템플릿 문장 추가 (데이터 양 제한)\n",
        "    if template_sentences:\n",
        "        template_sample = random.sample(template_sentences, min(MAX_TEMPLATE_COUNT, len(template_sentences)))\n",
        "        print(f\"Adding {len(template_sample)} template sentences to MLM training data\")\n",
        "        for sentence in template_sample:\n",
        "            template_text = f\"[TEMPLATE] {sentence}\"\n",
        "            mlm_texts.append(template_text)\n",
        "\n",
        "    random.shuffle(mlm_texts)\n",
        "\n",
        "    # 데이터셋 및 데이터로더 구성\n",
        "    train_texts, val_texts = train_test_split(mlm_texts, test_size=0.1, random_state=42)\n",
        "\n",
        "    # MLM 데이터 콜레이터\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        "    )\n",
        "\n",
        "    # 학습 준비\n",
        "    model.to(DEVICE)\n",
        "    optimizer = AdamW(model.parameters(), lr=MLM_LEARNING_RATE)\n",
        "\n",
        "    # 총 학습 스텝 계산\n",
        "    train_dataset = DomainTextDataset(train_texts, tokenizer, MAX_LENGTH)\n",
        "    val_dataset = DomainTextDataset(val_texts, tokenizer, MAX_LENGTH)\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    val_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    total_steps = len(train_dataloader) * MLM_EPOCHS\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=total_steps // 10,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # 체크포인트 로드 시도\n",
        "    if checkpoint_exists:\n",
        "        checkpoint, loaded_step = load_latest_checkpoint(\n",
        "            mlm_checkpoint_dir, model, optimizer, scheduler)\n",
        "        if checkpoint:\n",
        "            # 매우 큰 loaded_step 값(999999)을 받으면 MLM 학습 완료로 처리\n",
        "            if loaded_step >= 999999:\n",
        "                print(\"MLM training already completed. Skipping to next stage.\")\n",
        "                return model, tokenizer\n",
        "\n",
        "            resume_training = True\n",
        "            # 에포크 계산 (몇 번째 에포크부터 시작할지)\n",
        "            # 숫자인지 확인 후 계산\n",
        "            if isinstance(loaded_step, int) and loaded_step > 0:\n",
        "                start_epoch = loaded_step // len(train_dataloader)\n",
        "                global_step = loaded_step\n",
        "                print(f\"Resuming MLM training from epoch {start_epoch+1}\")\n",
        "            else:\n",
        "                # 적절한 스텝 값이 아닌 경우 처음부터 시작\n",
        "                start_epoch = 0\n",
        "                global_step = 0\n",
        "                print(\"Invalid step value. Starting MLM training from scratch.\")\n",
        "\n",
        "    scaler = GradScaler()\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # 훈련 루프\n",
        "    for epoch in range(start_epoch, MLM_EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch+1}/{MLM_EPOCHS}\")\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        train_progress_bar = tqdm(train_dataloader, desc=f\"Training\")\n",
        "\n",
        "        # 학습\n",
        "        for batch_idx, batch in enumerate(train_progress_bar):\n",
        "            # 체크포인트에서 재개하는 경우 이미 처리한 배치는 건너뛰기\n",
        "            if resume_training and epoch == start_epoch and batch_idx < (global_step % len(train_dataloader)):\n",
        "                continue\n",
        "\n",
        "            # 데이터를 디바이스로 이동\n",
        "            input_ids = batch['input_ids'].to(DEVICE)\n",
        "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "\n",
        "            # MLM을 위한 라벨 생성\n",
        "            batch_inputs = {\n",
        "                'input_ids': input_ids,\n",
        "                'attention_mask': attention_mask,\n",
        "                'labels': input_ids.clone()\n",
        "            }\n",
        "\n",
        "            # 마스킹된 토큰 생성 (DataCollator로 자동화)\n",
        "            masked_inputs = data_collator(\n",
        "                [{'input_ids': ids, 'attention_mask': mask}\n",
        "                 for ids, mask in zip(input_ids, attention_mask)]\n",
        "            )\n",
        "\n",
        "            # 모든 데이터를 디바이스로 이동\n",
        "            for k, v in masked_inputs.items():\n",
        "                masked_inputs[k] = v.to(DEVICE)\n",
        "\n",
        "            # 자동 혼합 정밀도로 순전파 및 손실 계산\n",
        "            with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "                outputs = model(**masked_inputs)\n",
        "                loss = outputs.loss / ACCUMULATION_STEPS\n",
        "\n",
        "            # 스케일링된 역전파\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # 원래 손실 (스케일링 전) 추적\n",
        "            total_train_loss += loss.item() * ACCUMULATION_STEPS\n",
        "\n",
        "            # 그라디언트 누적 후 업데이트\n",
        "            if (batch_idx + 1) % ACCUMULATION_STEPS == 0 or (batch_idx + 1) == len(train_dataloader):\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                global_step += 1\n",
        "\n",
        "                # 100 스텝마다 체크포인트 저장\n",
        "                if global_step % 100 == 0:\n",
        "                    save_checkpoint(\n",
        "                        mlm_checkpoint_dir,\n",
        "                        global_step,\n",
        "                        model,\n",
        "                        optimizer,\n",
        "                        scheduler,\n",
        "                        tokenizer,\n",
        "                        loss.item() * ACCUMULATION_STEPS\n",
        "                    )\n",
        "\n",
        "            train_progress_bar.set_postfix({'loss': loss.item() * ACCUMULATION_STEPS, 'step': global_step})\n",
        "\n",
        "            # 체크포인트에서 재개한 경우, 첫 배치 후 resume_training 해제\n",
        "            if batch_idx == 0 and epoch == start_epoch and resume_training:\n",
        "                resume_training = False\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # 검증\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        val_progress_bar = tqdm(val_dataloader, desc=f\"Validation\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_progress_bar:\n",
        "                input_ids = batch['input_ids'].to(DEVICE)\n",
        "                attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "\n",
        "                masked_inputs = data_collator(\n",
        "                    [{'input_ids': ids, 'attention_mask': mask}\n",
        "                     for ids, mask in zip(input_ids, attention_mask)]\n",
        "                )\n",
        "\n",
        "                for k, v in masked_inputs.items():\n",
        "                    masked_inputs[k] = v.to(DEVICE)\n",
        "\n",
        "                outputs = model(**masked_inputs)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                total_val_loss += loss.item()\n",
        "                val_progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "        print(f\"Validation loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # 최고 모델 저장\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            print(f\"Saving best model to {output_dir}/mlm_best_model\")\n",
        "\n",
        "            # 모델 및 토크나이저 저장\n",
        "            model.save_pretrained(f\"{output_dir}/mlm_best_model\")\n",
        "            tokenizer.save_pretrained(f\"{output_dir}/mlm_best_model\")\n",
        "\n",
        "            # 에포크 체크포인트 저장\n",
        "            save_checkpoint(\n",
        "                mlm_checkpoint_dir,\n",
        "                f\"epoch_{epoch+1}\",\n",
        "                model,\n",
        "                optimizer,\n",
        "                scheduler,\n",
        "                tokenizer,\n",
        "                avg_val_loss\n",
        "            )\n",
        "\n",
        "    print(\"=== 도메인 적응 MLM 학습 완료 ===\")\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "# 대조학습 모델 훈련 함수\n",
        "# 최적화된 대조학습 함수\n",
        "def train_contrastive_learning(job_fields, papers, base_model, tokenizer, output_dir, template_sentences=None, reset_checkpoints=False):\n",
        "    \"\"\"대조학습 기반의 Hi-BERT 모델 훈련 (배치 처리 및 병렬 처리 최적화)\"\"\"\n",
        "    print(\"\\n=== 대조학습 기반 Hi-BERT 훈련 시작 ===\")\n",
        "\n",
        "    # 체크포인트 디렉토리 확인\n",
        "    cl_checkpoint_dir = f\"{output_dir}/cl_checkpoints\"\n",
        "    os.makedirs(cl_checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # 체크포인트 정보 파일\n",
        "    checkpoint_info_path = f\"{cl_checkpoint_dir}/checkpoint_info.json\"\n",
        "\n",
        "    # 체크포인트 초기화가 요청된 경우 기존 체크포인트 정보 파일 삭제\n",
        "    if reset_checkpoints and os.path.exists(checkpoint_info_path):\n",
        "        os.remove(checkpoint_info_path)\n",
        "        print(\"대조학습 체크포인트 정보 파일이 초기화되었습니다.\")\n",
        "\n",
        "    # 체크포인트 확인\n",
        "    checkpoint_exists = os.path.exists(checkpoint_info_path) and not reset_checkpoints\n",
        "    resume_training = False\n",
        "    start_epoch = 0\n",
        "    global_step = 0\n",
        "\n",
        "    # 약한 감독 신호 생성 (데이터 양 제한)\n",
        "    # 채용공고 및 논문 샘플링 - 최적화된 샘플 수\n",
        "    max_jobs_for_cl = min(300, len(job_fields))  # 500에서 300으로 감소\n",
        "    max_papers_for_cl = min(1000, len(papers))   # 2000에서 1000으로 감소\n",
        "    max_templates_for_cl = int(min(MAX_TEMPLATE_COUNT, len(template_sentences) if template_sentences else 0) * 0.7)\n",
        "\n",
        "    job_sample = random.sample(job_fields, min(max_jobs_for_cl, len(job_fields)))\n",
        "    paper_sample = random.sample(papers, min(max_papers_for_cl, len(papers)))\n",
        "\n",
        "    print(f\"Generating weak supervision pairs with {len(job_sample)} jobs and {len(paper_sample)} papers...\")\n",
        "    # 템플릿 문장도 제한된 수만 사용\n",
        "    template_sample = None\n",
        "    if template_sentences:\n",
        "        template_sample = random.sample(template_sentences, min(max_templates_for_cl, len(template_sentences)))\n",
        "\n",
        "    # 약한 감독 신호 생성 함수 호출\n",
        "    pairs_file = f\"{output_dir}/weak_supervision_pairs.pkl\"\n",
        "\n",
        "    # 이미 생성된 페어 파일이 있고 초기화하지 않는 경우 로드\n",
        "    if os.path.exists(pairs_file) and not reset_checkpoints:\n",
        "        print(f\"Loading pre-generated pairs from {pairs_file}\")\n",
        "        with open(pairs_file, 'rb') as f:\n",
        "            pairs = pickle.load(f)\n",
        "    else:\n",
        "        # 이중 인코더 모델 초기화 - 약한 감독 신호 생성에 사용\n",
        "        temp_model = DualEncoderModel(base_model, projection_dim=PROJECTION_DIM)\n",
        "        temp_model.to(DEVICE)\n",
        "\n",
        "        # 최적화된 페어 생성 함수 호출\n",
        "        pairs = generate_weak_supervision_pairs(\n",
        "            job_sample,  # 샘플링된 채용공고 사용\n",
        "            paper_sample,  # 샘플링된 논문 사용\n",
        "            template_sample,  # 샘플링된 템플릿 사용\n",
        "            simcse_model=temp_model,\n",
        "            tokenizer=tokenizer,\n",
        "            device=DEVICE\n",
        "        )\n",
        "\n",
        "        # 메모리 정리\n",
        "        del temp_model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # 페어 저장\n",
        "        with open(pairs_file, 'wb') as f:\n",
        "            pickle.dump(pairs, f)\n",
        "\n",
        "    # 검증 세트 분리\n",
        "    train_pairs, val_pairs = train_test_split(pairs, test_size=0.1, random_state=42)\n",
        "\n",
        "    # 데이터 로더 설정 최적화\n",
        "    # 병렬 데이터 로딩 및 GPU 메모리 전송 최적화\n",
        "    train_dataset = ContrastivePairDataset(train_pairs, tokenizer, MAX_LENGTH)\n",
        "    val_dataset = ContrastivePairDataset(val_pairs, tokenizer, MAX_LENGTH)\n",
        "\n",
        "    hibert_batch_size = 16  # 배치 크기 유지\n",
        "\n",
        "    # 데이터 로더 최적화\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=hibert_batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4,        # 병렬 데이터 로딩 워커 수 증가\n",
        "        pin_memory=True,      # GPU 메모리 전송 최적화\n",
        "        prefetch_factor=2,    # 미리 배치 준비\n",
        "        persistent_workers=True # 워커 유지 (성능 향상)\n",
        "    )\n",
        "\n",
        "    val_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=hibert_batch_size,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        prefetch_factor=2,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "\n",
        "    # Hi-BERT 이중 인코더 모델 초기화\n",
        "    model = DualEncoderModel(base_model, projection_dim=PROJECTION_DIM)\n",
        "\n",
        "    # 모델을 디바이스로 이동\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    # 옵티마이저 및 스케줄러 설정\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(),\n",
        "        lr=CL_LEARNING_RATE,\n",
        "        weight_decay=0.01\n",
        "    )\n",
        "\n",
        "    total_steps = len(train_dataloader) * CL_EPOCHS\n",
        "\n",
        "    # 코사인 스케줄러 유지\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=total_steps,\n",
        "        eta_min=1e-6\n",
        "    )\n",
        "\n",
        "    # 체크포인트 로드 시도\n",
        "    if checkpoint_exists:\n",
        "        checkpoint, loaded_step = load_latest_checkpoint(\n",
        "            cl_checkpoint_dir, model, optimizer, scheduler)\n",
        "        if checkpoint:\n",
        "            resume_training = True\n",
        "            # 에포크 계산\n",
        "            start_epoch = loaded_step // len(train_dataloader)\n",
        "            global_step = loaded_step\n",
        "            print(f\"Resuming contrastive learning from epoch {start_epoch+1}\")\n",
        "\n",
        "    scaler = GradScaler()\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # 학습률 기록용 리스트\n",
        "    learning_rates = []\n",
        "\n",
        "    # 훈련 루프\n",
        "    for epoch in range(start_epoch, CL_EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch+1}/{CL_EPOCHS}\")\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        train_progress_bar = tqdm(train_dataloader, desc=f\"Training\")\n",
        "\n",
        "        # 배치 처리 최적화\n",
        "        for batch_idx, batch in enumerate(train_progress_bar):\n",
        "            # 체크포인트에서 재개하는 경우 이미 처리한 배치는 건너뛰기\n",
        "            if resume_training and epoch == start_epoch and batch_idx < (global_step % len(train_dataloader)):\n",
        "                continue\n",
        "\n",
        "            # 데이터를 디바이스로 이동 (비동기 전송 최적화)\n",
        "            job_input_ids = batch['job_input_ids'].to(DEVICE, non_blocking=True)\n",
        "            job_attention_mask = batch['job_attention_mask'].to(DEVICE, non_blocking=True)\n",
        "            paper_input_ids = batch['paper_input_ids'].to(DEVICE, non_blocking=True)\n",
        "            paper_attention_mask = batch['paper_attention_mask'].to(DEVICE, non_blocking=True)\n",
        "\n",
        "            # 자동 혼합 정밀도로 순전파 및 손실 계산\n",
        "            with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "                job_emb, paper_emb = model(\n",
        "                    job_input_ids, job_attention_mask,\n",
        "                    paper_input_ids, paper_attention_mask\n",
        "                )\n",
        "\n",
        "                # 개선된 대조 손실 함수 사용\n",
        "                loss = contrastive_loss(job_emb, paper_emb, temperature=TEMPERATURE, margin=MARGIN) / ACCUMULATION_STEPS\n",
        "\n",
        "            # NaN 체크 및 처리\n",
        "            if torch.isnan(loss):\n",
        "                print(f\"Warning: NaN loss detected at batch {batch_idx}, skipping batch\")\n",
        "                optimizer.zero_grad()\n",
        "                continue\n",
        "\n",
        "            # 스케일링된 역전파\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # 원래 손실 (스케일링 전) 추적\n",
        "            total_train_loss += loss.item() * ACCUMULATION_STEPS\n",
        "\n",
        "            # 그라디언트 누적 후 업데이트\n",
        "            if (batch_idx + 1) % ACCUMULATION_STEPS == 0 or (batch_idx + 1) == len(train_dataloader):\n",
        "                # 그라디언트 클리핑\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "                # 옵티마이저 스텝\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                scheduler.step()\n",
        "\n",
        "                # 현재 학습률 기록\n",
        "                learning_rates.append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                global_step += 1\n",
        "\n",
        "                # 50 스텝마다 체크포인트 저장 (30에서 50으로 변경)\n",
        "                if global_step % 50 == 0:\n",
        "                    save_checkpoint(\n",
        "                        cl_checkpoint_dir,\n",
        "                        global_step,\n",
        "                        model,\n",
        "                        optimizer,\n",
        "                        scheduler,\n",
        "                        None,\n",
        "                        loss.item() * ACCUMULATION_STEPS\n",
        "                    )\n",
        "\n",
        "            train_progress_bar.set_postfix({\n",
        "                'loss': loss.item() * ACCUMULATION_STEPS,\n",
        "                'step': global_step,\n",
        "                'lr': optimizer.param_groups[0]['lr']\n",
        "            })\n",
        "\n",
        "            # 체크포인트에서 재개한 경우, 첫 배치 후 resume_training 해제\n",
        "            if batch_idx == 0 and epoch == start_epoch and resume_training:\n",
        "                resume_training = False\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # 검증\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        val_progress_bar = tqdm(val_dataloader, desc=f\"Validation\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_progress_bar:\n",
        "                job_input_ids = batch['job_input_ids'].to(DEVICE, non_blocking=True)\n",
        "                job_attention_mask = batch['job_attention_mask'].to(DEVICE, non_blocking=True)\n",
        "                paper_input_ids = batch['paper_input_ids'].to(DEVICE, non_blocking=True)\n",
        "                paper_attention_mask = batch['paper_attention_mask'].to(DEVICE, non_blocking=True)\n",
        "\n",
        "                job_emb, paper_emb = model(\n",
        "                    job_input_ids, job_attention_mask,\n",
        "                    paper_input_ids, paper_attention_mask\n",
        "                )\n",
        "\n",
        "                # 개선된 대조 손실 함수 사용\n",
        "                loss = contrastive_loss(job_emb, paper_emb, temperature=TEMPERATURE, margin=MARGIN)\n",
        "\n",
        "                # NaN 체크\n",
        "                if not torch.isnan(loss):\n",
        "                    total_val_loss += loss.item()\n",
        "                    val_progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "        print(f\"Validation loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # 최고 모델 저장\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            print(f\"Saving best model to {output_dir}/cl_best_model\")\n",
        "\n",
        "            # 에포크 체크포인트 저장\n",
        "            save_checkpoint(\n",
        "                cl_checkpoint_dir,\n",
        "                f\"epoch_{epoch+1}_best\",\n",
        "                model,\n",
        "                optimizer,\n",
        "                scheduler,\n",
        "                None,\n",
        "                avg_val_loss\n",
        "            )\n",
        "\n",
        "            # 최고 모델 저장\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': best_val_loss,\n",
        "                'epoch': epoch\n",
        "            }, f\"{output_dir}/cl_best_model.pt\")\n",
        "\n",
        "            # Hi-BERT를 위한 추가 저장 (모델 가중치만 따로 저장)\n",
        "            torch.save(model.state_dict(), f\"{output_dir}/hibert_weights.pt\")\n",
        "\n",
        "    print(\"=== Hi-BERT 대조학습 훈련 완료 ===\")\n",
        "    return model\n",
        "\n",
        "# 모델 검증 함수\n",
        "# 최적화된 모델 검증 함수 (배치 처리 적용)\n",
        "def validate_model(model, tokenizer, job_fields, papers, device):\n",
        "    \"\"\"학습된 Hi-BERT 모델의 성능 검증 (배치 처리 최적화)\"\"\"\n",
        "    print(\"\\n=== Hi-BERT 모델 검증 시작 (배치 처리 최적화) ===\")\n",
        "\n",
        "    # 검증에 사용할 샘플 수 제한\n",
        "    test_job_fields = []\n",
        "\n",
        "    # 유효한 채용공고만 필터링\n",
        "    for job in job_fields[:50]:  # 상위 50개에서 검사\n",
        "        if job.get('field'):  # 필드가 있는 경우만 포함\n",
        "            test_job_fields.append(job)\n",
        "            if len(test_job_fields) >= 20:  # 테스트에 사용할 20개만 선택\n",
        "                break\n",
        "\n",
        "    # 유효한 채용공고가 없는 경우\n",
        "    if not test_job_fields:\n",
        "        print(\"유효한 채용공고가 없습니다. 검증을 건너뜁니다.\")\n",
        "        return {'mrr': 0, 'ndcg': 0, 'precision@5': 0, 'precision@10': 0}\n",
        "\n",
        "    # 최대 100개 논문\n",
        "    test_papers = papers[:100]\n",
        "\n",
        "    # 모델이 평가 모드인지 확인\n",
        "    model.eval()\n",
        "\n",
        "    # 모델이 올바른 장치에 있는지 확인\n",
        "    if next(model.parameters()).device != device:\n",
        "        model = model.to(device)\n",
        "\n",
        "    # 모델 이름 가져오기 (Hi-BERT 여부 확인)\n",
        "    model_name = getattr(model, 'model_name', \"\")\n",
        "    is_hibert = model_name == \"Hi-BERT\"  # 모델 이름이 \"Hi-BERT\"인지 정확히 확인\n",
        "\n",
        "    # 평가 지표 초기화\n",
        "    mrr_scores = []\n",
        "    ndcg_scores = []\n",
        "    precision_at_5 = []\n",
        "    precision_at_10 = []\n",
        "\n",
        "    # 채용공고 배치 처리\n",
        "    job_batch_size = 4  # 배치당 채용공고 수\n",
        "    job_batches = [test_job_fields[i:i+job_batch_size] for i in range(0, len(test_job_fields), job_batch_size)]\n",
        "\n",
        "    # 논문 데이터를 미리 토큰화하여 재사용하기 위한 준비\n",
        "    paper_batch_size = 20  # 논문 배치 크기\n",
        "    paper_batches = [test_papers[i:i+paper_batch_size] for i in range(0, len(test_papers), paper_batch_size)]\n",
        "\n",
        "    # 논문 인코딩 및 임베딩 캐시\n",
        "    paper_encodings_cache = {}\n",
        "    paper_embeddings_cache = {}\n",
        "\n",
        "    print(\"논문 임베딩 사전 계산 중...\")\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, paper_batch in enumerate(paper_batches):\n",
        "            # 논문 텍스트 준비\n",
        "            paper_texts = [\n",
        "                f\"제목: {paper['title']} 키워드: {', '.join(paper.get('keywords', []))} 초록: {paper.get('abstract', '')}\"\n",
        "                for paper in paper_batch\n",
        "            ]\n",
        "\n",
        "            # 배치 토큰화\n",
        "            paper_encodings = tokenizer(\n",
        "                paper_texts,\n",
        "                truncation=True,\n",
        "                max_length=MAX_LENGTH,\n",
        "                padding='max_length',\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            # 디바이스로 이동\n",
        "            paper_input_ids = paper_encodings['input_ids'].to(device)\n",
        "            paper_attention_mask = paper_encodings['attention_mask'].to(device)\n",
        "\n",
        "            # 논문 임베딩 계산\n",
        "            paper_embeddings = model.encode_paper(paper_input_ids, paper_attention_mask)\n",
        "\n",
        "            # 캐시에 저장\n",
        "            for i, paper in enumerate(paper_batch):\n",
        "                paper_id = paper['paper_id']\n",
        "                paper_encodings_cache[paper_id] = {\n",
        "                    'input_ids': paper_input_ids[i].unsqueeze(0),\n",
        "                    'attention_mask': paper_attention_mask[i].unsqueeze(0)\n",
        "                }\n",
        "                paper_embeddings_cache[paper_id] = paper_embeddings[i].unsqueeze(0)\n",
        "\n",
        "    print(f\"논문 임베딩 {len(paper_embeddings_cache)}개 사전 계산 완료\")\n",
        "\n",
        "    # 채용공고 배치 처리\n",
        "    for batch_idx, job_batch in enumerate(tqdm(job_batches, desc=\"채용공고 배치 평가\")):\n",
        "        # 채용공고 텍스트 준비\n",
        "        job_texts = []\n",
        "        for job in job_batch:\n",
        "            job_text = f\"분야: {job['field']} 업무: {job.get('duties', '')} 자격요건: {job.get('qualifications', '')} \" \\\n",
        "                     f\"전공: {' '.join(job.get('majors', []))} 키워드: {' '.join(job.get('keywords', []))} \" \\\n",
        "                     f\"기술: {' '.join(job.get('skills', []))}\"\n",
        "            job_texts.append(job_text)\n",
        "\n",
        "        # 배치 토큰화\n",
        "        job_encodings = tokenizer(\n",
        "            job_texts,\n",
        "            truncation=True,\n",
        "            max_length=MAX_LENGTH,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # 디바이스로 이동\n",
        "        job_input_ids = job_encodings['input_ids'].to(device)\n",
        "        job_attention_mask = job_encodings['attention_mask'].to(device)\n",
        "\n",
        "        # 채용공고 임베딩 계산\n",
        "        with torch.no_grad():\n",
        "            job_embeddings = model.encode_job(job_input_ids, job_attention_mask)\n",
        "\n",
        "        # 배치의 각 채용공고 처리\n",
        "        for job_idx, job in enumerate(job_batch):\n",
        "            # 현재 채용공고 임베딩\n",
        "            job_emb = job_embeddings[job_idx].unsqueeze(0)  # [1, dim]\n",
        "\n",
        "            # 모든 논문과의 유사도 계산 (배치 처리)\n",
        "            paper_scores = []\n",
        "\n",
        "            # 논문 배치 처리\n",
        "            for paper_batch in paper_batches:\n",
        "                # 배치 내 논문 임베딩 수집\n",
        "                batch_paper_embeddings = torch.cat([\n",
        "                    paper_embeddings_cache[paper['paper_id']]\n",
        "                    for paper in paper_batch\n",
        "                ], dim=0)  # [batch_size, dim]\n",
        "\n",
        "                # 배치 유사도 계산\n",
        "                with torch.no_grad():\n",
        "                    # [1, dim] x [batch_size, dim]T = [1, batch_size]\n",
        "                    similarities = torch.matmul(job_emb, batch_paper_embeddings.transpose(0, 1))\n",
        "\n",
        "                # 배치의 각 논문에 대한 유사도 및 관련성 계산\n",
        "                for paper_idx, paper in enumerate(paper_batch):\n",
        "                    similarity = similarities[0, paper_idx].item()\n",
        "\n",
        "                    # Hi-BERT 모델일 경우 유사도 스케일링 적용\n",
        "                    if is_hibert:\n",
        "                        scaled_similarity = min(similarity * 5.0, 1.0)\n",
        "                    else:\n",
        "                        scaled_similarity = similarity\n",
        "\n",
        "                    # 관련성 판단 - 유사도 임계값 조정\n",
        "                    is_relevant = 0  # 기본값 설정\n",
        "\n",
        "                    # Hi-BERT 모델일 경우 더 낮은 임계값 적용\n",
        "                    similarity_threshold = 0.25 if is_hibert else 0.3\n",
        "\n",
        "                    # 유사도 임계값 적용\n",
        "                    if scaled_similarity > similarity_threshold:\n",
        "                        is_relevant = 1\n",
        "\n",
        "                    # 추가 관련성 판단 - 키워드 매칭\n",
        "                    job_keywords = set([k.lower() for k in job.get('keywords', []) if k])\n",
        "                    paper_keywords = set([k.lower() for k in paper.get('keywords', []) if k])\n",
        "\n",
        "                    # 키워드가 없는 경우 필드나 제목에서 추출\n",
        "                    if not job_keywords and job.get('field'):\n",
        "                        job_keywords = set([w.lower() for w in job['field'].split()\n",
        "                                         if len(w) > 2 and w.lower() not in STOPWORDS])\n",
        "\n",
        "                    if not paper_keywords and paper.get('title'):\n",
        "                        paper_keywords = set([w.lower() for w in paper['title'].split()\n",
        "                                           if len(w) > 2 and w.lower() not in STOPWORDS])\n",
        "\n",
        "                    # 키워드 정확 매칭\n",
        "                    keyword_matches = job_keywords.intersection(paper_keywords)\n",
        "                    exact_match = 1 if keyword_matches else 0\n",
        "\n",
        "                    # 정확 매칭된 키워드 목록\n",
        "                    matched_keywords = list(keyword_matches)\n",
        "\n",
        "                    # 키워드 부분 매칭 (최적화 버전)\n",
        "                    partial_matches = []\n",
        "                    partial_match = 0\n",
        "                    if job_keywords and paper_keywords:\n",
        "                        for j_kw in job_keywords:\n",
        "                            if partial_match:\n",
        "                                break\n",
        "                            if len(j_kw) <= 2:\n",
        "                                continue\n",
        "                            for p_kw in paper_keywords:\n",
        "                                if len(p_kw) <= 2:\n",
        "                                    continue\n",
        "                                if (j_kw in p_kw or p_kw in j_kw) and j_kw != p_kw:\n",
        "                                    partial_matches.append((j_kw, p_kw))\n",
        "                                    partial_match = 1\n",
        "                                    break\n",
        "\n",
        "                    # 분야-제목 매칭 (Hi-BERT용 추가 검사)\n",
        "                    field_title_match = 0\n",
        "                    if job.get('field') and paper.get('title'):\n",
        "                        field_words = [w.lower() for w in job['field'].split()\n",
        "                                     if len(w) > 2 and w.lower() not in STOPWORDS]\n",
        "                        paper_title = paper['title'].lower()\n",
        "                        for word in field_words:\n",
        "                            if word in paper_title:\n",
        "                                field_title_match = 1\n",
        "                                break\n",
        "\n",
        "                    # 기술-초록 매칭 (Hi-BERT용 추가 검사)\n",
        "                    skill_abstract_match = 0\n",
        "                    matched_skill = \"\"\n",
        "                    if job.get('skills') and paper.get('abstract'):\n",
        "                        for skill in job.get('skills', []):\n",
        "                            if skill.lower() in paper['abstract'].lower():\n",
        "                                skill_abstract_match = 1\n",
        "                                matched_skill = skill\n",
        "                                break\n",
        "\n",
        "                    # 종합 관련성 점수 계산\n",
        "                    relevance_score = 0\n",
        "\n",
        "                    # Hi-BERT 모델일 경우 의미적 관계 가중치 증가\n",
        "                    if is_hibert:\n",
        "                        relevance_score = (\n",
        "                            (scaled_similarity * 0.3) +\n",
        "                            (exact_match * 0.3) +\n",
        "                            (partial_match * 0.2) +\n",
        "                            (field_title_match * 0.2) +\n",
        "                            (skill_abstract_match * 0.2)\n",
        "                        )\n",
        "                    else:\n",
        "                        relevance_score = (\n",
        "                            (scaled_similarity * 0.5) +\n",
        "                            (exact_match * 0.3) +\n",
        "                            (partial_match * 0.2)\n",
        "                        )\n",
        "\n",
        "                    # 관련성 종합 판단 - 점수 임계값 적용\n",
        "                    relevance_threshold = 0.35 if is_hibert else 0.4\n",
        "                    if relevance_score > relevance_threshold:\n",
        "                        is_relevant = 1\n",
        "\n",
        "                    # 관련성 정보 구성\n",
        "                    relevance_info = \"관련\" if is_relevant == 1 else \"비관련\"\n",
        "                    relevance_details = []\n",
        "\n",
        "                    if scaled_similarity > 0.2:\n",
        "                        relevance_details.append(f\"유사도 {similarity:.4f}\")\n",
        "\n",
        "                    if exact_match:\n",
        "                        relevance_details.append(f\"키워드 일치 {len(matched_keywords)}개 ({', '.join(matched_keywords[:3])})\")\n",
        "\n",
        "                    if partial_match:\n",
        "                        relevance_details.append(f\"부분 일치 {len(partial_matches)}개\")\n",
        "\n",
        "                    if is_hibert:\n",
        "                        if field_title_match:\n",
        "                            relevance_details.append(\"분야-제목 매칭\")\n",
        "\n",
        "                        if skill_abstract_match:\n",
        "                            relevance_details.append(f\"기술-초록 매칭 ({matched_skill})\")\n",
        "\n",
        "                    relevance_str = f\"{relevance_info} ({', '.join(relevance_details)})\" if relevance_details else relevance_info\n",
        "\n",
        "                    paper_scores.append({\n",
        "                        'paper': paper,\n",
        "                        'similarity': similarity,\n",
        "                        'scaled_similarity': scaled_similarity,\n",
        "                        'is_relevant': is_relevant,\n",
        "                        'relevance_score': relevance_score,\n",
        "                        'relevance_str': relevance_str,\n",
        "                        'keyword_match': exact_match,\n",
        "                        'partial_match': partial_match,\n",
        "                        'field_title_match': field_title_match,\n",
        "                        'skill_abstract_match': skill_abstract_match\n",
        "                    })\n",
        "\n",
        "            # 관련성 점수에 따라 정렬\n",
        "            paper_scores.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
        "\n",
        "            # MRR 계산\n",
        "            mrr = 0\n",
        "            for i, ps in enumerate(paper_scores, 1):\n",
        "                if ps['is_relevant'] == 1:\n",
        "                    mrr = 1.0 / i\n",
        "                    break\n",
        "\n",
        "            # NDCG@10 계산\n",
        "            relevance_scores = [ps['is_relevant'] for ps in paper_scores[:10]]\n",
        "            if relevance_scores and any(relevance_scores):\n",
        "                ideal_scores = sorted(relevance_scores, reverse=True)\n",
        "                dcg = sum((2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(relevance_scores))\n",
        "                idcg = sum((2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(ideal_scores))\n",
        "                ndcg = dcg / idcg if idcg > 0 else 0\n",
        "            else:\n",
        "                ndcg = 0\n",
        "\n",
        "            # Precision@k 계산\n",
        "            precision_5 = sum(ps['is_relevant'] for ps in paper_scores[:5]) / 5 if len(paper_scores) >= 5 else 0\n",
        "            precision_10 = sum(ps['is_relevant'] for ps in paper_scores[:10]) / 10 if len(paper_scores) >= 10 else 0\n",
        "\n",
        "            # 평가 지표 추가\n",
        "            mrr_scores.append(mrr)\n",
        "            ndcg_scores.append(ndcg)\n",
        "            precision_at_5.append(precision_5)\n",
        "            precision_at_10.append(precision_10)\n",
        "\n",
        "            # 상위 5개 논문 출력 - 처음 5개 채용공고만 출력\n",
        "            job_global_idx = batch_idx * job_batch_size + job_idx\n",
        "            if job_global_idx < 5:\n",
        "                print(f\"\\n채용공고: {job['field']}\")\n",
        "                print(\"상위 5개 추천 논문:\")\n",
        "                for i, ps in enumerate(paper_scores[:5], 1):\n",
        "                    print(f\"{i}. {ps['paper']['title']} (유사도: {ps['similarity']:.4f}, 스케일링: {ps['scaled_similarity']:.4f}, 종합점수: {ps['relevance_score']:.4f}, {ps['relevance_str']})\")\n",
        "\n",
        "    # 평균 평가 지표 계산 (데이터가 있는 경우만)\n",
        "    avg_mrr = sum(mrr_scores) / len(mrr_scores) if mrr_scores else 0\n",
        "    avg_ndcg = sum(ndcg_scores) / len(ndcg_scores) if ndcg_scores else 0\n",
        "    avg_precision_5 = sum(precision_at_5) / len(precision_at_5) if precision_at_5 else 0\n",
        "    avg_precision_10 = sum(precision_at_10) / len(precision_at_10) if precision_at_10 else 0\n",
        "\n",
        "    print(\"\\n=== 평가 결과 ===\")\n",
        "    print(f\"모델 유형: {'Hi-BERT' if is_hibert else '일반 모델'}\")\n",
        "    print(f\"MRR: {avg_mrr:.4f}\")\n",
        "    print(f\"NDCG@10: {avg_ndcg:.4f}\")\n",
        "    print(f\"Precision@5: {avg_precision_5:.4f}\")\n",
        "    print(f\"Precision@10: {avg_precision_10:.4f}\")\n",
        "\n",
        "    print(\"=== 모델 검증 완료 ===\")\n",
        "    return {\n",
        "        'mrr': avg_mrr,\n",
        "        'ndcg': avg_ndcg,\n",
        "        'precision@5': avg_precision_5,\n",
        "        'precision@10': avg_precision_10,\n",
        "        'is_hibert': is_hibert\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    \"\"\"메인 함수 - Hi-BERT 모델 학습 및 검증 (최적화 버전)\"\"\"\n",
        "    print(\"=== Hi-BERT 모델 학습 시작 (최적화 버전) ===\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # 출력 디렉토리 설정\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    # 데이터셋 정보 저장 파일 경로\n",
        "    dataset_info_path = f\"{OUTPUT_DIR}/dataset_info.json\"\n",
        "\n",
        "    # GPU 메모리 정리\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # 디바이스 설정\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB Total\")\n",
        "        print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "\n",
        "    # 병렬 데이터 로딩 설정\n",
        "    print(\"데이터 로딩 최적화 설정 적용 중...\")\n",
        "\n",
        "    # 병렬 처리로 온톨로지 데이터 로드\n",
        "    def load_ontologies_in_parallel():\n",
        "        train_jobs = load_ontology(ONTOLOGY_TRAIN_DIR, max_samples=None)\n",
        "        test_jobs = load_ontology(ONTOLOGY_TEST_DIR, max_samples=None)\n",
        "        val_jobs = load_ontology(ONTOLOGY_VALID_DIR, max_samples=None)\n",
        "        return train_jobs, test_jobs, val_jobs\n",
        "\n",
        "    print(\"채용공고 온톨로지 데이터 병렬 로드 중...\")\n",
        "    # 멀티스레딩을 사용하여 온톨로지 파일 로드\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
        "        future = executor.submit(load_ontologies_in_parallel)\n",
        "        train_job_fields, test_job_fields, validation_job_fields = future.result()\n",
        "\n",
        "    print(f\"Loaded job fields: Train: {len(train_job_fields)}, Test: {len(test_job_fields)}, Validation: {len(validation_job_fields)}\")\n",
        "\n",
        "    # 논문 데이터 로드\n",
        "    print(\"논문 데이터 로드 중...\")\n",
        "    papers = load_papers(PAPER_TRAIN_PATH, max_samples=None)\n",
        "    print(f\"Loaded {len(papers)} papers\")\n",
        "\n",
        "    # 이전 학습의 데이터셋 크기 로드 (있는 경우)\n",
        "    previous_dataset_sizes = {}\n",
        "    if os.path.exists(dataset_info_path):\n",
        "        try:\n",
        "            with open(dataset_info_path, 'r') as f:\n",
        "                previous_dataset_sizes = json.load(f)\n",
        "            print(f\"이전 학습 데이터셋 크기: 채용공고(Train) {previous_dataset_sizes.get('train_job_count', 0)}개, \"\n",
        "                  f\"채용공고(Test) {previous_dataset_sizes.get('test_job_count', 0)}개, \"\n",
        "                  f\"채용공고(Validation) {previous_dataset_sizes.get('validation_job_count', 0)}개, \"\n",
        "                  f\"논문 {previous_dataset_sizes.get('paper_count', 0)}개\")\n",
        "        except Exception as e:\n",
        "            print(f\"이전 데이터셋 정보 로드 실패: {e}\")\n",
        "            previous_dataset_sizes = {}\n",
        "\n",
        "    # 현재 데이터셋 크기\n",
        "    current_dataset_sizes = {\n",
        "        'train_job_count': len(train_job_fields),\n",
        "        'test_job_count': len(test_job_fields),\n",
        "        'validation_job_count': len(validation_job_fields),\n",
        "        'paper_count': len(papers)\n",
        "    }\n",
        "\n",
        "    # 데이터셋 크기 변경 여부 확인\n",
        "    dataset_changed = (\n",
        "        previous_dataset_sizes.get('train_job_count', 0) != current_dataset_sizes['train_job_count'] or\n",
        "        previous_dataset_sizes.get('test_job_count', 0) != current_dataset_sizes['test_job_count'] or\n",
        "        previous_dataset_sizes.get('validation_job_count', 0) != current_dataset_sizes['validation_job_count'] or\n",
        "        previous_dataset_sizes.get('paper_count', 0) != current_dataset_sizes['paper_count']\n",
        "    )\n",
        "\n",
        "    # 체크포인트 초기화 여부 결정\n",
        "    reset_checkpoints = IS_RESET_CHECKPOINT\n",
        "    if dataset_changed:\n",
        "        print(\"\\n주의: 데이터셋 크기가 변경되었습니다!\")\n",
        "        print(f\"이전: 채용공고(Train) {previous_dataset_sizes.get('train_job_count', 0)}개, \"\n",
        "              f\"채용공고(Test) {previous_dataset_sizes.get('test_job_count', 0)}개, \"\n",
        "              f\"채용공고(Validation) {previous_dataset_sizes.get('validation_job_count', 0)}개, \"\n",
        "              f\"논문 {previous_dataset_sizes.get('paper_count', 0)}개\")\n",
        "        print(f\"현재: 채용공고(Train) {current_dataset_sizes['train_job_count']}개, \"\n",
        "              f\"채용공고(Test) {current_dataset_sizes['test_job_count']}개, \"\n",
        "              f\"채용공고(Validation) {current_dataset_sizes['validation_job_count']}개, \"\n",
        "              f\"논문 {current_dataset_sizes['paper_count']}개\")\n",
        "        print(\"데이터셋 변경으로 인해 체크포인트를 초기화하고 처음부터 학습을 진행합니다.\")\n",
        "        reset_checkpoints = True  # 데이터셋 변경 시 강제로 체크포인트 초기화\n",
        "\n",
        "        # 기존 체크포인트 디렉토리 백업\n",
        "        mlm_checkpoint_dir = f\"{OUTPUT_DIR}/mlm_checkpoints\"\n",
        "        cl_checkpoint_dir = f\"{OUTPUT_DIR}/cl_checkpoints\"\n",
        "        if os.path.exists(mlm_checkpoint_dir):\n",
        "            backup_dir = f\"{mlm_checkpoint_dir}_backup_{int(time.time())}\"\n",
        "            os.rename(mlm_checkpoint_dir, backup_dir)\n",
        "            print(f\"MLM 체크포인트 백업 완료: {backup_dir}\")\n",
        "\n",
        "        if os.path.exists(cl_checkpoint_dir):\n",
        "            backup_dir = f\"{cl_checkpoint_dir}_backup_{int(time.time())}\"\n",
        "            os.rename(cl_checkpoint_dir, backup_dir)\n",
        "            print(f\"대조학습 체크포인트 백업 완료: {backup_dir}\")\n",
        "\n",
        "        # 기존 weak_supervision_pairs.pkl 파일도 백업\n",
        "        pairs_file = f\"{OUTPUT_DIR}/weak_supervision_pairs.pkl\"\n",
        "        if os.path.exists(pairs_file):\n",
        "            backup_file = f\"{pairs_file}.backup_{int(time.time())}\"\n",
        "            os.rename(pairs_file, backup_file)\n",
        "            print(f\"약한 감독 페어 파일 백업 완료: {backup_file}\")\n",
        "    else:\n",
        "        print(\"데이터셋 크기가 이전과 동일합니다.\")\n",
        "\n",
        "    # 현재 데이터셋 크기 저장\n",
        "    with open(dataset_info_path, 'w') as f:\n",
        "        json.dump(current_dataset_sizes, f)\n",
        "\n",
        "    # 1단계: 템플릿 기반 문장 생성 (병렬 처리)\n",
        "    print(\"템플릿 기반 문장 생성 중...\")\n",
        "    # 템플릿 생성 전 데이터 샘플링 (메모리 절약)\n",
        "    train_job_sample = random.sample(train_job_fields, min(500, len(train_job_fields)))\n",
        "    paper_sample = random.sample(papers, min(1000, len(papers)))\n",
        "\n",
        "    # 학습에는 train_job_fields만 사용\n",
        "    template_sentences = generate_template_sentences(train_job_sample, paper_sample, templates)\n",
        "    # 생성된 템플릿이 너무 많으면 샘플링\n",
        "    if len(template_sentences) > MAX_TEMPLATE_COUNT:\n",
        "        template_sentences = random.sample(template_sentences, MAX_TEMPLATE_COUNT)\n",
        "    print(f\"Generated {len(template_sentences)} template sentences\")\n",
        "\n",
        "    # 메모리 정리\n",
        "    del train_job_sample\n",
        "    del paper_sample\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # 2단계: MLM 토크나이저 및 기본 모델 로드\n",
        "    print(\"MLM 토크나이저 및 모델 로드 중...\")\n",
        "    mlm_tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)\n",
        "\n",
        "    # MLM 학습이 이미 완료되었는지 확인 (체크포인트 초기화를 고려)\n",
        "    mlm_completed = os.path.exists(f\"{OUTPUT_DIR}/mlm_best_model/pytorch_model.bin\") and not reset_checkpoints\n",
        "\n",
        "    if mlm_completed:\n",
        "        print(\"MLM 학습 결과를 로드합니다...\")\n",
        "        # MLM 모델 로드\n",
        "        mlm_model = BertForMaskedLM.from_pretrained(f\"{OUTPUT_DIR}/mlm_best_model\")\n",
        "        mlm_tokenizer = BertTokenizer.from_pretrained(f\"{OUTPUT_DIR}/mlm_best_model\")\n",
        "    else:\n",
        "        # 3단계: 처음부터 MLM 학습 (학습에는 train_job_fields만 사용)\n",
        "        base_model = BertForMaskedLM.from_pretrained(BASE_MODEL)\n",
        "        mlm_model, mlm_tokenizer = train_domain_adapted_mlm(\n",
        "            train_job_fields, papers, mlm_tokenizer, base_model, OUTPUT_DIR, template_sentences,\n",
        "            reset_checkpoints=reset_checkpoints\n",
        "        )\n",
        "\n",
        "    # 메모리 정리\n",
        "    if 'base_model' in locals():\n",
        "        del base_model\n",
        "    if 'mlm_model' in locals():\n",
        "        del mlm_model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # 대조학습에도 동일한 BERT 모델 사용 (일관성 유지)\n",
        "    print(\"대조학습을 위한 Hi-BERT 모델 로드 중...\")\n",
        "\n",
        "    # 기본 BERT 모델 로드 (MLM 모델에서 초기화)\n",
        "    cl_model = BertModel.from_pretrained(BASE_MODEL)\n",
        "    cl_tokenizer = mlm_tokenizer  # MLM과 동일한 토크나이저 사용\n",
        "\n",
        "    # 대조학습이 이미 완료되었는지 확인 (체크포인트 초기화를 고려)\n",
        "    cl_completed = os.path.exists(f\"{OUTPUT_DIR}/cl_best_model.pt\") and not reset_checkpoints\n",
        "\n",
        "    if cl_completed:\n",
        "        print(\"대조학습 결과를 로드합니다...\")\n",
        "        # Hi-BERT 이중 인코더 모델 초기화\n",
        "        dual_encoder = DualEncoderModel(cl_model, projection_dim=PROJECTION_DIM)\n",
        "        # 저장된 가중치 로드\n",
        "        checkpoint = torch.load(f\"{OUTPUT_DIR}/cl_best_model.pt\", map_location=DEVICE)\n",
        "        dual_encoder.load_state_dict(checkpoint['model_state_dict'])\n",
        "        # 모델을 명시적으로 디바이스로 이동\n",
        "        dual_encoder = dual_encoder.to(DEVICE)\n",
        "    else:\n",
        "        # 4단계: 대조학습 기반 Hi-BERT 모델 학습 (최적화된 함수 사용)\n",
        "        dual_encoder = train_contrastive_learning(\n",
        "            train_job_fields, papers, cl_model, cl_tokenizer, OUTPUT_DIR, template_sentences,\n",
        "            reset_checkpoints=reset_checkpoints\n",
        "        )\n",
        "\n",
        "    # 5단계: 테스트 세트로 모델 검증 (최적화된 검증 함수 사용)\n",
        "    print(\"\\n=== 테스트 세트에서 Hi-BERT 모델 검증 ===\")\n",
        "    test_results = validate_model(\n",
        "        dual_encoder, cl_tokenizer, test_job_fields, papers, DEVICE\n",
        "    )\n",
        "\n",
        "    # 6단계: 검증 세트로 모델 검증 (최적화된 검증 함수 사용)\n",
        "    print(\"\\n=== 검증 세트에서 Hi-BERT 모델 검증 ===\")\n",
        "    validation_results = validate_model(\n",
        "        dual_encoder, cl_tokenizer, validation_job_fields, papers, DEVICE\n",
        "    )\n",
        "\n",
        "    # 최종 결과\n",
        "    print(\"\\n=== Hi-BERT 학습 결과 ===\")\n",
        "    print(\"테스트 세트 결과:\")\n",
        "    print(f\"MRR: {test_results['mrr']:.4f}\")\n",
        "    print(f\"NDCG@10: {test_results['ndcg']:.4f}\")\n",
        "    print(f\"Precision@5: {test_results['precision@5']:.4f}\")\n",
        "    print(f\"Precision@10: {test_results['precision@10']:.4f}\")\n",
        "\n",
        "    print(\"\\n검증 세트 결과:\")\n",
        "    print(f\"MRR: {validation_results['mrr']:.4f}\")\n",
        "    print(f\"NDCG@10: {validation_results['ndcg']:.4f}\")\n",
        "    print(f\"Precision@5: {validation_results['precision@5']:.4f}\")\n",
        "    print(f\"Precision@10: {validation_results['precision@10']:.4f}\")\n",
        "\n",
        "    # 총 소요 시간 출력\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n총 학습 소요 시간: {total_time:.2f}초 ({total_time/60:.2f}분)\")\n",
        "    print(f\"모든 모델이 {OUTPUT_DIR}에 저장되었습니다.\")\n",
        "    print(\"=== Hi-BERT 모델 학습 완료 ===\")\n",
        "\n",
        "# 최종 실행 코드\n",
        "if __name__ == \"__main__\":\n",
        "    # 모듈 임포트 확인\n",
        "    required_modules = {\n",
        "        'torch': torch,\n",
        "        'numpy': np,\n",
        "        'pandas': pd,\n",
        "        'transformers': None,\n",
        "        'tqdm': tqdm,\n",
        "        'concurrent.futures': concurrent.futures\n",
        "    }\n",
        "\n",
        "    # 모듈 버전 출력\n",
        "    print(\"=== 모듈 버전 확인 ===\")\n",
        "    for module_name, module in required_modules.items():\n",
        "        if module:\n",
        "            if hasattr(module, '__version__'):\n",
        "                print(f\"{module_name}: {module.__version__}\")\n",
        "            else:\n",
        "                print(f\"{module_name}: 설치됨 (버전 정보 없음)\")\n",
        "        else:\n",
        "            print(f\"{module_name}: 설치됨\")\n",
        "\n",
        "    # GPU 정보 확인\n",
        "    print(\"\\n=== GPU 정보 확인 ===\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"사용 가능한 GPU: {torch.cuda.device_count()}개\")\n",
        "        print(f\"현재 GPU: {torch.cuda.current_device()} - {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"CUDA 버전: {torch.version.cuda}\")\n",
        "        print(f\"GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    else:\n",
        "        print(\"GPU를 사용할 수 없습니다. CPU 모드로 실행합니다.\")\n",
        "\n",
        "    # 경로 확인\n",
        "    print(\"\\n=== 경로 확인 ===\")\n",
        "    paths = {\n",
        "        \"온톨로지(훈련)\": ONTOLOGY_TRAIN_DIR,\n",
        "        \"온톨로지(테스트)\": ONTOLOGY_TEST_DIR,\n",
        "        \"온톨로지(검증)\": ONTOLOGY_VALID_DIR,\n",
        "        \"논문 데이터\": PAPER_TRAIN_PATH,\n",
        "        \"모델 출력\": OUTPUT_DIR,\n",
        "        \"불용어\": STOPWORDS_PATH\n",
        "    }\n",
        "\n",
        "    for name, path in paths.items():\n",
        "        exists = os.path.exists(path)\n",
        "        status = \"존재함\" if exists else \"존재하지 않음\"\n",
        "        print(f\"{name}: {path} ({status})\")\n",
        "\n",
        "    # 주요 설정 확인\n",
        "    print(\"\\n=== 주요 설정 확인 ===\")\n",
        "    print(f\"기본 모델: {BASE_MODEL}\")\n",
        "    print(f\"최대 길이: {MAX_LENGTH}\")\n",
        "    print(f\"배치 크기: {BATCH_SIZE}\")\n",
        "    print(f\"MLM 학습률: {MLM_LEARNING_RATE}\")\n",
        "    print(f\"대조학습 학습률: {CL_LEARNING_RATE}\")\n",
        "    print(f\"MLM 에포크: {MLM_EPOCHS}\")\n",
        "    print(f\"대조학습 에포크: {CL_EPOCHS}\")\n",
        "    print(f\"그라디언트 누적 단계: {ACCUMULATION_STEPS}\")\n",
        "    print(f\"체크포인트 초기화: {IS_RESET_CHECKPOINT}\")\n",
        "\n",
        "    # 메인 함수 실행\n",
        "    try:\n",
        "        print(\"\\n=== Hi-BERT 모델 학습 시작 ===\")\n",
        "        main()\n",
        "    except Exception as e:\n",
        "        print(f\"오류 발생: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        print(\"\\n프로그램이 오류로 종료되었습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EA4IMZQwiuUp",
        "outputId": "2a4aa6e0-3e57-44d5-fc3a-363df80e831f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loaded 10921 stopwords from /content/drive/MyDrive/Workspaces/Hibrainnet/Hi-BERT/datasets/recruitment_stopwords2.csv\n",
            "=== 모듈 버전 확인 ===\n",
            "torch: 2.6.0+cu124\n",
            "numpy: 2.0.2\n",
            "pandas: 2.2.2\n",
            "transformers: 설치됨\n",
            "tqdm: 설치됨 (버전 정보 없음)\n",
            "concurrent.futures: 설치됨 (버전 정보 없음)\n",
            "\n",
            "=== GPU 정보 확인 ===\n",
            "사용 가능한 GPU: 1개\n",
            "현재 GPU: 0 - NVIDIA A100-SXM4-40GB\n",
            "CUDA 버전: 12.4\n",
            "GPU 메모리: 42.47 GB\n",
            "\n",
            "=== 경로 확인 ===\n",
            "온톨로지(훈련): /content/drive/MyDrive/Workspaces/Hibrainnet/Hi-BERT/ontologies/training (존재함)\n",
            "온톨로지(테스트): /content/drive/MyDrive/Workspaces/Hibrainnet/Hi-BERT/ontologies/test (존재함)\n",
            "온톨로지(검증): /content/drive/MyDrive/Workspaces/Hibrainnet/Hi-BERT/ontologies/validation (존재함)\n",
            "논문 데이터: /content/drive/MyDrive/Workspaces/Hibrainnet/Hi-BERT/datasets/combined_articles_train.csv (존재함)\n",
            "모델 출력: /content/drive/MyDrive/Workspaces/Hibrainnet/Hi-BERT/models/hi_bert_model_kosimcse (존재함)\n",
            "불용어: /content/drive/MyDrive/Workspaces/Hibrainnet/Hi-BERT/datasets/recruitment_stopwords2.csv (존재함)\n",
            "\n",
            "=== 주요 설정 확인 ===\n",
            "기본 모델: BM-K/KoSimCSE-bert\n",
            "최대 길이: 256\n",
            "배치 크기: 32\n",
            "MLM 학습률: 1e-05\n",
            "대조학습 학습률: 5e-06\n",
            "MLM 에포크: 2\n",
            "대조학습 에포크: 3\n",
            "그라디언트 누적 단계: 2\n",
            "체크포인트 초기화: True\n",
            "\n",
            "=== Hi-BERT 모델 학습 시작 ===\n",
            "=== Hi-BERT 모델 학습 시작 (최적화 버전) ===\n",
            "Using device: cuda\n",
            "GPU Memory: 42.47 GB Total\n",
            "GPU Memory Allocated: 0.02 GB\n",
            "데이터 로딩 최적화 설정 적용 중...\n",
            "채용공고 온톨로지 데이터 병렬 로드 중...\n",
            "Found 1000 ontology files in /content/drive/MyDrive/Workspaces/Hibrainnet/Hi-BERT/ontologies/training\n",
            "디버깅: 첫 번째 파일 경로 = /content/drive/MyDrive/Workspaces/Hibrainnet/Hi-BERT/ontologies/training/recruitments_867-jsonld.json\n",
            "파일 내용 (처음 500자):\n",
            "{\n",
            "  \"@context\": {\n",
            "    \"@vocab\": \"http://recruitontology.org/\",\n",
            "    \"xsd\": \"http://www.w3.org/2001/XMLSchema#\"\n",
            "  },\n",
            "  \"@id\": \"http://recruitontology.org/Recruit_recruitments_867\",\n",
            "  \"@type\": \"Recruitment\",\n",
            "  \"hasRecruitId\": \"recruitments_867\",\n",
            "  \"hasPdfUrl\": \"\",\n",
            "  \"hasJobPosting\": {\n",
            "    \"@type\": \"JobPosting\",\n",
            "    \"hasOrganization\": \"한국표준과학연구원\",\n",
            "    \"hasLocation\": \"대전\",\n",
            "    \"hasTitle\": \"한국표준과학연구원 2023년 5차 Post-Doc. 공개채용\"\n",
            "  },\n",
            "  \"hasJobFields\": [\n",
            "    {\n",
            "      \"@type\": \"JobField\",\n",
            "      \"hasDepartmen\n",
            "JSON 구조:\n",
            "주요 키: ['@context', '@id', '@type', 'hasRecruitId', 'hasPdfUrl', 'hasJobPosting', 'hasJobFields']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading ontology files: 100%|██████████| 1000/1000 [00:07<00:00, 139.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "처리된 첫 번째 채용공고 정보:\n",
            "ID: http://recruitontology.org/Recruit_recruitments_867\n",
            "분야: 양자광학1\n",
            "키워드: ['양자광', '양자회로', '최적화', '정밀측정', '양자센싱기술']\n",
            "Successfully loaded 1000 job fields from 1000 files\n",
            "Found 277 ontology files in /content/drive/MyDrive/Workspaces/Hibrainnet/Hi-BERT/ontologies/test\n",
            "디버깅: 첫 번째 파일 경로 = /content/drive/MyDrive/Workspaces/Hibrainnet/Hi-BERT/ontologies/test/recruitments_1070-jsonld.json\n",
            "파일 내용 (처음 500자):\n",
            "{\n",
            "  \"@context\": {\n",
            "    \"@vocab\": \"http://recruitontology.org/\",\n",
            "    \"xsd\": \"http://www.w3.org/2001/XMLSchema#\"\n",
            "  },\n",
            "  \"@id\": \"http://recruitontology.org/Recruit_recruitments_1070\",\n",
            "  \"@type\": \"Recruitment\",\n",
            "  \"hasRecruitId\": \"recruitments_1070\",\n",
            "  \"hasPdfUrl\": \"\",\n",
            "  \"hasJobPosting\": {\n",
            "    \"@type\": \"JobPosting\",\n",
            "    \"hasOrganization\": \"호서대학교\",\n",
            "    \"hasLocation\": \"충청남도 아산시\",\n",
            "    \"hasTitle\": \"호서대학교 해양IT융합기술연구소 연구원, 연구보조원 채용\"\n",
            "  },\n",
            "  \"hasJobFields\": [\n",
            "    {\n",
            "      \"@type\": \"JobField\",\n",
            "      \"hasDepartm\n",
            "JSON 구조:\n",
            "주요 키: ['@context', '@id', '@type', 'hasRecruitId', 'hasPdfUrl', 'hasJobPosting', 'hasJobFields']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading ontology files: 100%|██████████| 277/277 [00:00<00:00, 359.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "처리된 첫 번째 채용공고 정보:\n",
            "ID: http://recruitontology.org/Recruit_recruitments_1070\n",
            "분야: 연구원, 연구보조원\n",
            "키워드: ['통신 알고리즘', '하드웨어 설계', '수중통신', 'FPGA', 'DSP', 'C/C++', 'Matlab']\n",
            "Successfully loaded 277 job fields from 277 files\n",
            "Found 279 ontology files in /content/drive/MyDrive/Workspaces/Hibrainnet/Hi-BERT/ontologies/validation\n",
            "디버깅: 첫 번째 파일 경로 = /content/drive/MyDrive/Workspaces/Hibrainnet/Hi-BERT/ontologies/validation/recruitments_1433-jsonld.json\n",
            "파일 내용 (처음 500자):\n",
            "{\n",
            "  \"@context\": {\n",
            "    \"@vocab\": \"http://recruitontology.org/\",\n",
            "    \"xsd\": \"http://www.w3.org/2001/XMLSchema#\"\n",
            "  },\n",
            "  \"@id\": \"http://recruitontology.org/Recruit_recruitments_1433\",\n",
            "  \"@type\": \"Recruitment\",\n",
            "  \"hasRecruitId\": \"recruitments_1433\",\n",
            "  \"hasPdfUrl\": \"\",\n",
            "  \"hasJobPosting\": {\n",
            "    \"@type\": \"JobPosting\",\n",
            "    \"hasOrganization\": \"유원대학교\",\n",
            "    \"hasLocation\": \"아산\",\n",
            "    \"hasTitle\": \"유원대학교 2021학년도 전임교원 초빙 공고(4차)\"\n",
            "  },\n",
            "  \"hasJobFields\": [\n",
            "    {\n",
            "      \"@type\": \"JobField\",\n",
            "      \"hasDepartment\": \"아산\n",
            "JSON 구조:\n",
            "주요 키: ['@context', '@id', '@type', 'hasRecruitId', 'hasPdfUrl', 'hasJobPosting', 'hasJobFields']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading ontology files: 100%|██████████| 279/279 [00:00<00:00, 319.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "처리된 첫 번째 채용공고 정보:\n",
            "ID: http://recruitontology.org/Recruit_recruitments_1433\n",
            "분야: 정보통신 및 보안(정보통신,해킹보안,컴퓨터보안,네트워크보안,멀티미디어보안,사물인터넷,인공지능 등)\n",
            "키워드: ['정보통신', '해킹보안', '컴퓨터보안', '네트워크보안', '멀티미디어보안', '사물인터넷', '인공지능']\n",
            "Successfully loaded 279 job fields from 279 files\n",
            "Loaded job fields: Train: 1000, Test: 277, Validation: 279\n",
            "논문 데이터 로드 중...\n",
            "Processing 1000 papers from /content/drive/MyDrive/Workspaces/Hibrainnet/Hi-BERT/datasets/combined_articles_train.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading papers: 100%|██████████| 1000/1000 [00:00<00:00, 9743.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded 1000 papers\n",
            "Loaded 1000 papers\n",
            "이전 학습 데이터셋 크기: 채용공고(Train) 1000개, 채용공고(Test) 277개, 채용공고(Validation) 279개, 논문 1000개\n",
            "데이터셋 크기가 이전과 동일합니다.\n",
            "템플릿 기반 문장 생성 중...\n",
            "\n",
            "=== 템플릿 기반 문장 생성 시작 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating job field templates: 100%|██████████| 500/500 [00:00<00:00, 145797.55it/s]\n",
            "Generating job-paper templates: 100%|██████████| 300/300 [00:00<00:00, 6803.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "생성된 고유 템플릿 문장 수: 500\n",
            "Generated 500 template sentences\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLM 토크나이저 및 모델 로드 중...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForMaskedLM were not initialized from the model checkpoint at BM-K/KoSimCSE-bert and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 도메인 적응 MLM 학습 시작 ===\n",
            "MLM 체크포인트 정보 파일이 초기화되었습니다.\n",
            "Adding 500 template sentences to MLM training data\n",
            "\n",
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 57/57 [00:05<00:00, 10.37it/s, loss=7.41, step=29]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 9.0466\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.82it/s, loss=7.48]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 7.5177\n",
            "Saving best model to /content/drive/MyDrive/Workspaces/Hibrainnet/Hi-BERT/models/hi_bert_model_kosimcse/mlm_best_model\n",
            "Checkpoint saved at step epoch_1\n",
            "\n",
            "Epoch 2/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 57/57 [00:05<00:00, 10.48it/s, loss=7.11, step=58]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 7.3749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  6.76it/s, loss=7.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 7.1412\n",
            "Saving best model to /content/drive/MyDrive/Workspaces/Hibrainnet/Hi-BERT/models/hi_bert_model_kosimcse/mlm_best_model\n",
            "Checkpoint saved at step epoch_2\n",
            "=== 도메인 적응 MLM 학습 완료 ===\n",
            "대조학습을 위한 Hi-BERT 모델 로드 중...\n",
            "\n",
            "=== 대조학습 기반 Hi-BERT 훈련 시작 ===\n",
            "Generating weak supervision pairs with 300 jobs and 1000 papers...\n",
            "Generating weak supervision pairs with batch processing...\n",
            "Extracting relations from template sentences...\n",
            "\n",
            "시도 1/20: 템플릿 생성 중... (임계값: 0.20)\n",
            "\n",
            "=== 템플릿 기반 문장 생성 시작 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating job field templates: 100%|██████████| 300/300 [00:00<00:00, 122437.60it/s]\n",
            "Generating job-paper templates: 100%|██████████| 300/300 [00:00<00:00, 7827.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "생성된 고유 템플릿 문장 수: 500\n",
            "템플릿 문장에서 채용공고-논문 관계 추출 중...\n",
            "===== 템플릿 문장 샘플 =====\n",
            "템플릿 문장 1: 영상기반 배추 내재해성 평가기준 개발 분야는 사이버전자전 지식이 필요합니다.\n",
            "템플릿 문장 2: 스마트제조혁신 코디네이터 분야는 ai education 주제의 논문과 관련이 있습니다.\n",
            "템플릿 문장 3: 인공지능 분야는 Computer Graphics 지식이 필요합니다.\n",
            "템플릿 문장 4: 연구개발 분야는 인공지능 전공과 관련이 있습니다.\n",
            "템플릿 문장 5: 소프트웨어 전공은 데이터구조 키워드와 관련이 있습니다.\n",
            "템플릿 문장 6: 산학협력중점교수 분야는 컴퓨터공학 전공 지식이 필요합니다.\n",
            "템플릿 문장 7: 미래차 소프트웨어 응용설계 및 개발 분야 전공은 인공지능시스템 키워드와 관련이 있습니다.\n",
            "템플릿 문장 8: 빅데이터 또는 머신러닝 또는 인간과컴퓨터상호작용 분야는 데이터사이언스 전공 지식이 필요합니다.\n",
            "템플릿 문장 9: 통계 인공지능 컴퓨터공학 산업공학 분야는 통계 전공 지식이 필요합니다.\n",
            "템플릿 문장 10: 자료구조&알고리즘, 컴퓨터구조 전공은 자료구조 키워드와 관련이 있습니다.\n",
            "===========================\n",
            "===== 채용공고 샘플 =====\n",
            "채용공고 1 (ID: http://recruitontology.org/Recruit_recruitments_1084): 분야=스마트 자동화 생산기술, 키워드=['AI', '생산공정관리', '스마트 자동화'], 기술=[]\n",
            "채용공고 2 (ID: http://recruitontology.org/Recruit_recruitments_1670): 분야=초전도양자컴퓨팅, 키워드=['초전도', '큐비트', '나노 전자소자', '양자프로세서', '양자컴퓨팅', '양자알고리즘'], 기술=['초전도 큐비트 소자 제작 및 특성평가 기술', '나노 전자소자 제작 및 특성평가 기술', '양자프로세서 시스템 고주파 제어', '측정 및 분석 기술', '양자컴퓨팅을 위한 양자알고리즘 소프트웨어 및 프로그래밍 기술']\n",
            "채용공고 3 (ID: http://recruitontology.org/Recruit_recruitments_1723): 분야=고추 영상기반 형태적 특성 분석기술 및 생력형 소재 개발, 키워드=['고추', '영상기반', '형태적 특성', '생력형'], 기술=[]\n",
            "채용공고 4 (ID: http://recruitontology.org/Recruit_recruitments_810): 분야=드론활용 / 코딩SW 분야, 키워드=['드론활용', '코딩SW'], 기술=['드론활용', '코딩SW 관련 기술 및 지식', '프로젝트 수주 능력']\n",
            "채용공고 5 (ID: http://recruitontology.org/Recruit_recruitments_1628): 분야=이공계 및 관련 분야(과학기술정책, 전산, 컴퓨터, 시스템 공학 등 공학계열), 키워드=['디지털 트윈', '가상가상/증강현실', '가상 플랫폼'], 기술=['과학기술분야 정책수립 및 기획 능력', '디지털 트윈', '가상가상/증강현실', '가상 플랫폼 관련 기술 지식']\n",
            "===========================\n",
            "===== 논문 샘플 =====\n",
            "논문 1 (ID: ART002835262): 제목=불균형 시계열 자료를 위한 분류 알고리즘 적용방안: 기업 부도모형을 중심으로, 키워드=['기업 부도모형', '불균형 데이터', '시계열 데이터', 'SMOTE', '전진교차검증.', 'Corporate default model', 'Forward cross validation', 'Imbalanced data', 'Time series data', 'SMOTE.']\n",
            "논문 2 (ID: ART003138314): 제목=적외선 영상 전처리에 따른 딥러닝 기반의 객체 탐지 모델 성능 평가, 키워드=['Deep Learning', 'Image Preprocessing', 'Object Detection', '딥러닝', '영상 전처리', '객체 탐지']\n",
            "논문 3 (ID: ART002750407): 제목=공안협의소설과 문화콘텐츠— 포청천 캐릭터 콘텐츠 기획을 위한 프로젝트 기반 수업모델을 중심으로, 키워드=['공안협의소설', '공안소설', '포청천 캐릭터', '캐릭터 콘텐츠', '프로젝트 기반 수업모델', '문화콘텐츠 기획', 'Gong’anxiayi Novels', 'Gong’an Novels', 'Baoqingtian character', 'character contents', 'project-based classes', 'planning cultural contents']\n",
            "논문 4 (ID: ART003124563): 제목=반도체 인력 양성 및 채용을 위한 거대 언어 모델 기반 학부 공학 교육과정 설계 및 평가 전략 -ChatGPT-4o를 이용한 KAIST 공학 교육과정 분석을 중심으로-, 키워드=['거대 언어 모델', '공학 교육과정', '직무 적합성 인력선발', '반도체 산업', 'Large Language Model', 'Engineering Curriculum', 'Job Fit', 'Workforce Selection', 'Semiconductor Industry']\n",
            "논문 5 (ID: ART002779803): 제목=한글 조합성에 기반한 최소 글자를 사용하는한글 폰트 생성 모델, 키워드=['한글폰트생성', '폰트구성요소', 'GAN', 'Few-shot', 'Hangul Font Generation', 'Font Components', 'GAN', 'Few-shot']\n",
            "===========================\n",
            "채용공고 및 논문 데이터 전처리 중...\n",
            "전처리 완료: 30 채용공고, 50 논문\n",
            "템플릿 문장 처리 중... (총 5 배치)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "템플릿 배치 처리: 100%|██████████| 5/5 [00:00<00:00, 45689.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "총 매칭 시도: 5600, 성공: 2671, 성공률: 47.70%\n",
            "템플릿 문장에서 229개의 관계를 추출했습니다.\n",
            "템플릿 매칭 결과: 시도 5600, 성공 2671, 성공률 47.70%\n",
            "성공률이 너무 낮습니다. 임계값을 0.15로 낮춥니다.\n",
            "\n",
            "시도 2/20: 템플릿 생성 중... (임계값: 0.15)\n",
            "\n",
            "=== 템플릿 기반 문장 생성 시작 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating job field templates: 100%|██████████| 300/300 [00:00<00:00, 132187.33it/s]\n",
            "Generating job-paper templates: 100%|██████████| 300/300 [00:00<00:00, 7784.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "생성된 고유 템플릿 문장 수: 500\n",
            "템플릿 문장에서 채용공고-논문 관계 추출 중...\n",
            "===== 템플릿 문장 샘플 =====\n",
            "템플릿 문장 1: 영상처리 직무는 1980년대 한국 CG 산업의 태동과 메가 이벤트의 역할 연구 결과를 활용합니다.\n",
            "템플릿 문장 2: 디지털농업을 위한 마늘의 생장 가시화 기술 개발 분야는 컴퓨터공학 전공과 관련이 있습니다.\n",
            "템플릿 문장 3: 교육혁신센터 분야는 데이터사이언스 전공 지식이 필요합니다.\n",
            "템플릿 문장 4: 센터장 분야는 컴퓨터 전공과 관련이 있습니다.\n",
            "템플릿 문장 5: 전산과학교수 전공은 보안 키워드와 관련이 있습니다.\n",
            "템플릿 문장 6: 이공계 및 관련 분야(과학기술정책, 전산, 컴퓨터, 시스템 공학 등 공학계열) 전공은 가상 플랫폼 키워드와 관련이 있습니다.\n",
            "템플릿 문장 7: 소프트웨어 관련 전 분야 전공은 소프트웨어 키워드와 관련이 있습니다.\n",
            "템플릿 문장 8: 인공지능/블록체인/센서지능화 분야 전공은 스마트도시 키워드와 관련이 있습니다.\n",
            "템플릿 문장 9: IT분야 교사 전공은 C++ 키워드와 관련이 있습니다.\n",
            "템플릿 문장 10: 컴퓨터공학, 전산학 직무는 AI교육을 위한 방향성에 대한 고찰 연구 결과를 활용합니다.\n",
            "===========================\n",
            "===== 채용공고 샘플 =====\n",
            "채용공고 1 (ID: http://recruitontology.org/Recruit_recruitments_1084): 분야=스마트 자동화 생산기술, 키워드=['AI', '생산공정관리', '스마트 자동화'], 기술=[]\n",
            "채용공고 2 (ID: http://recruitontology.org/Recruit_recruitments_1670): 분야=초전도양자컴퓨팅, 키워드=['초전도', '큐비트', '나노 전자소자', '양자프로세서', '양자컴퓨팅', '양자알고리즘'], 기술=['초전도 큐비트 소자 제작 및 특성평가 기술', '나노 전자소자 제작 및 특성평가 기술', '양자프로세서 시스템 고주파 제어', '측정 및 분석 기술', '양자컴퓨팅을 위한 양자알고리즘 소프트웨어 및 프로그래밍 기술']\n",
            "채용공고 3 (ID: http://recruitontology.org/Recruit_recruitments_1723): 분야=고추 영상기반 형태적 특성 분석기술 및 생력형 소재 개발, 키워드=['고추', '영상기반', '형태적 특성', '생력형'], 기술=[]\n",
            "채용공고 4 (ID: http://recruitontology.org/Recruit_recruitments_810): 분야=드론활용 / 코딩SW 분야, 키워드=['드론활용', '코딩SW'], 기술=['드론활용', '코딩SW 관련 기술 및 지식', '프로젝트 수주 능력']\n",
            "채용공고 5 (ID: http://recruitontology.org/Recruit_recruitments_1628): 분야=이공계 및 관련 분야(과학기술정책, 전산, 컴퓨터, 시스템 공학 등 공학계열), 키워드=['디지털 트윈', '가상가상/증강현실', '가상 플랫폼'], 기술=['과학기술분야 정책수립 및 기획 능력', '디지털 트윈', '가상가상/증강현실', '가상 플랫폼 관련 기술 지식']\n",
            "===========================\n",
            "===== 논문 샘플 =====\n",
            "논문 1 (ID: ART002835262): 제목=불균형 시계열 자료를 위한 분류 알고리즘 적용방안: 기업 부도모형을 중심으로, 키워드=['기업 부도모형', '불균형 데이터', '시계열 데이터', 'SMOTE', '전진교차검증.', 'Corporate default model', 'Forward cross validation', 'Imbalanced data', 'Time series data', 'SMOTE.']\n",
            "논문 2 (ID: ART003138314): 제목=적외선 영상 전처리에 따른 딥러닝 기반의 객체 탐지 모델 성능 평가, 키워드=['Deep Learning', 'Image Preprocessing', 'Object Detection', '딥러닝', '영상 전처리', '객체 탐지']\n",
            "논문 3 (ID: ART002750407): 제목=공안협의소설과 문화콘텐츠— 포청천 캐릭터 콘텐츠 기획을 위한 프로젝트 기반 수업모델을 중심으로, 키워드=['공안협의소설', '공안소설', '포청천 캐릭터', '캐릭터 콘텐츠', '프로젝트 기반 수업모델', '문화콘텐츠 기획', 'Gong’anxiayi Novels', 'Gong’an Novels', 'Baoqingtian character', 'character contents', 'project-based classes', 'planning cultural contents']\n",
            "논문 4 (ID: ART003124563): 제목=반도체 인력 양성 및 채용을 위한 거대 언어 모델 기반 학부 공학 교육과정 설계 및 평가 전략 -ChatGPT-4o를 이용한 KAIST 공학 교육과정 분석을 중심으로-, 키워드=['거대 언어 모델', '공학 교육과정', '직무 적합성 인력선발', '반도체 산업', 'Large Language Model', 'Engineering Curriculum', 'Job Fit', 'Workforce Selection', 'Semiconductor Industry']\n",
            "논문 5 (ID: ART002779803): 제목=한글 조합성에 기반한 최소 글자를 사용하는한글 폰트 생성 모델, 키워드=['한글폰트생성', '폰트구성요소', 'GAN', 'Few-shot', 'Hangul Font Generation', 'Font Components', 'GAN', 'Few-shot']\n",
            "===========================\n",
            "채용공고 및 논문 데이터 전처리 중...\n",
            "전처리 완료: 30 채용공고, 50 논문\n",
            "템플릿 문장 처리 중... (총 5 배치)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "템플릿 배치 처리: 100%|██████████| 5/5 [00:00<00:00, 37315.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "총 매칭 시도: 6000, 성공: 2767, 성공률: 46.12%\n",
            "템플릿 문장에서 245개의 관계를 추출했습니다.\n",
            "템플릿 매칭 결과: 시도 6000, 성공 2767, 성공률 46.12%\n",
            "성공률이 너무 낮습니다. 임계값을 0.10로 낮춥니다.\n",
            "\n",
            "시도 3/20: 템플릿 생성 중... (임계값: 0.10)\n",
            "\n",
            "=== 템플릿 기반 문장 생성 시작 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating job field templates: 100%|██████████| 300/300 [00:00<00:00, 131537.86it/s]\n",
            "Generating job-paper templates: 100%|██████████| 300/300 [00:00<00:00, 7719.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "생성된 고유 템플릿 문장 수: 500\n",
            "템플릿 문장에서 채용공고-논문 관계 추출 중...\n",
            "===== 템플릿 문장 샘플 =====\n",
            "템플릿 문장 1: DMZ 생생누리(실감미디어 체험관) 분야는 문화콘텐츠 기획 주제의 논문과 관련이 있습니다.\n",
            "템플릿 문장 2: 영상처리 전공은 임베디드 시스템 키워드와 관련이 있습니다.\n",
            "템플릿 문장 3: SW융합분야 분야는 거대 언어 모델 지식이 필요합니다.\n",
            "템플릿 문장 4: 정보통신공사비 산정기준 연구, 정보통신기술 및 제도 연구 분야는 정보통신공학 전공과 관련이 있습니다.\n",
            "템플릿 문장 5: 물리과정 및 결합모델 개발 분야는 물리학 전공과 관련이 있습니다.\n",
            "템플릿 문장 6: 인공지능 소프트웨어 ICT 분야는 개인의무기록 지식이 필요합니다.\n",
            "템플릿 문장 7: 인턴 분야는 컴퓨터공학 전공 지식이 필요합니다.\n",
            "템플릿 문장 8: 전기 분야는 컴퓨터그래픽 주제의 논문과 관련이 있습니다.\n",
            "템플릿 문장 9: 자동차 데이터분석 전공은 머신러닝 키워드와 관련이 있습니다.\n",
            "템플릿 문장 10: 소프트웨어 관련 전 분야 직무는 중소기업기술보호법의 지원제도에관한 연구- 주요 기술보호 지원제도에 대한 검토를 중심으로 - 연구 결과를 활용합니다.\n",
            "===========================\n",
            "===== 채용공고 샘플 =====\n",
            "채용공고 1 (ID: http://recruitontology.org/Recruit_recruitments_1084): 분야=스마트 자동화 생산기술, 키워드=['AI', '생산공정관리', '스마트 자동화'], 기술=[]\n",
            "채용공고 2 (ID: http://recruitontology.org/Recruit_recruitments_1670): 분야=초전도양자컴퓨팅, 키워드=['초전도', '큐비트', '나노 전자소자', '양자프로세서', '양자컴퓨팅', '양자알고리즘'], 기술=['초전도 큐비트 소자 제작 및 특성평가 기술', '나노 전자소자 제작 및 특성평가 기술', '양자프로세서 시스템 고주파 제어', '측정 및 분석 기술', '양자컴퓨팅을 위한 양자알고리즘 소프트웨어 및 프로그래밍 기술']\n",
            "채용공고 3 (ID: http://recruitontology.org/Recruit_recruitments_1723): 분야=고추 영상기반 형태적 특성 분석기술 및 생력형 소재 개발, 키워드=['고추', '영상기반', '형태적 특성', '생력형'], 기술=[]\n",
            "채용공고 4 (ID: http://recruitontology.org/Recruit_recruitments_810): 분야=드론활용 / 코딩SW 분야, 키워드=['드론활용', '코딩SW'], 기술=['드론활용', '코딩SW 관련 기술 및 지식', '프로젝트 수주 능력']\n",
            "채용공고 5 (ID: http://recruitontology.org/Recruit_recruitments_1628): 분야=이공계 및 관련 분야(과학기술정책, 전산, 컴퓨터, 시스템 공학 등 공학계열), 키워드=['디지털 트윈', '가상가상/증강현실', '가상 플랫폼'], 기술=['과학기술분야 정책수립 및 기획 능력', '디지털 트윈', '가상가상/증강현실', '가상 플랫폼 관련 기술 지식']\n",
            "===========================\n",
            "===== 논문 샘플 =====\n",
            "논문 1 (ID: ART002835262): 제목=불균형 시계열 자료를 위한 분류 알고리즘 적용방안: 기업 부도모형을 중심으로, 키워드=['기업 부도모형', '불균형 데이터', '시계열 데이터', 'SMOTE', '전진교차검증.', 'Corporate default model', 'Forward cross validation', 'Imbalanced data', 'Time series data', 'SMOTE.']\n",
            "논문 2 (ID: ART003138314): 제목=적외선 영상 전처리에 따른 딥러닝 기반의 객체 탐지 모델 성능 평가, 키워드=['Deep Learning', 'Image Preprocessing', 'Object Detection', '딥러닝', '영상 전처리', '객체 탐지']\n",
            "논문 3 (ID: ART002750407): 제목=공안협의소설과 문화콘텐츠— 포청천 캐릭터 콘텐츠 기획을 위한 프로젝트 기반 수업모델을 중심으로, 키워드=['공안협의소설', '공안소설', '포청천 캐릭터', '캐릭터 콘텐츠', '프로젝트 기반 수업모델', '문화콘텐츠 기획', 'Gong’anxiayi Novels', 'Gong’an Novels', 'Baoqingtian character', 'character contents', 'project-based classes', 'planning cultural contents']\n",
            "논문 4 (ID: ART003124563): 제목=반도체 인력 양성 및 채용을 위한 거대 언어 모델 기반 학부 공학 교육과정 설계 및 평가 전략 -ChatGPT-4o를 이용한 KAIST 공학 교육과정 분석을 중심으로-, 키워드=['거대 언어 모델', '공학 교육과정', '직무 적합성 인력선발', '반도체 산업', 'Large Language Model', 'Engineering Curriculum', 'Job Fit', 'Workforce Selection', 'Semiconductor Industry']\n",
            "논문 5 (ID: ART002779803): 제목=한글 조합성에 기반한 최소 글자를 사용하는한글 폰트 생성 모델, 키워드=['한글폰트생성', '폰트구성요소', 'GAN', 'Few-shot', 'Hangul Font Generation', 'Font Components', 'GAN', 'Few-shot']\n",
            "===========================\n",
            "채용공고 및 논문 데이터 전처리 중...\n",
            "전처리 완료: 30 채용공고, 50 논문\n",
            "템플릿 문장 처리 중... (총 5 배치)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "템플릿 배치 처리: 100%|██████████| 5/5 [00:00<00:00, 43781.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "총 매칭 시도: 5300, 성공: 2975, 성공률: 56.13%\n",
            "템플릿 문장에서 436개의 관계를 추출했습니다.\n",
            "템플릿 매칭 결과: 시도 5300, 성공 2975, 성공률 56.13%\n",
            "성공률이 너무 낮습니다. 임계값을 0.05로 낮춥니다.\n",
            "\n",
            "시도 4/20: 템플릿 생성 중... (임계값: 0.05)\n",
            "\n",
            "=== 템플릿 기반 문장 생성 시작 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating job field templates: 100%|██████████| 300/300 [00:00<00:00, 132131.81it/s]\n",
            "Generating job-paper templates: 100%|██████████| 300/300 [00:00<00:00, 7713.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "생성된 고유 템플릿 문장 수: 500\n",
            "템플릿 문장에서 채용공고-논문 관계 추출 중...\n",
            "===== 템플릿 문장 샘플 =====\n",
            "템플릿 문장 1: 데이터 분석 분야는 전산학 전공 지식이 필요합니다.\n",
            "템플릿 문장 2: 중장년 생애역량 증진을 위한 직무역량교육 [B] 직무기초역량향상 전공은 엑셀 키워드와 관련이 있습니다.\n",
            "템플릿 문장 3: 컴퓨터과학 전 분야 분야는 한글폰트생성 지식이 필요합니다.\n",
            "템플릿 문장 4: 데이터엔지니어 전공은 배치 처리 키워드와 관련이 있습니다.\n",
            "템플릿 문장 5: 컴퓨터공학 또는 정보통신공학(소프트웨어공학) 분야는 중소기업기술보호법 지식이 필요합니다.\n",
            "템플릿 문장 6: DMZ 생생누리(실감미디어 체험관) 전공은 실감콘텐츠 키워드와 관련이 있습니다.\n",
            "템플릿 문장 7: 빅데이터플랫폼 운영·관리(선임급) 분야는 정보기술 전공 지식이 필요합니다.\n",
            "템플릿 문장 8: 전기/열전재료 디지털 설계 기술 분야는 재료공학 전공 지식이 필요합니다.\n",
            "템플릿 문장 9: 인공지능 관련 전공 직무는 AI교육을 위한 방향성에 대한 고찰 연구 결과를 활용합니다.\n",
            "템플릿 문장 10: 빅데이터 분석 분야는 convergence education 주제의 논문과 관련이 있습니다.\n",
            "===========================\n",
            "===== 채용공고 샘플 =====\n",
            "채용공고 1 (ID: http://recruitontology.org/Recruit_recruitments_1084): 분야=스마트 자동화 생산기술, 키워드=['AI', '생산공정관리', '스마트 자동화'], 기술=[]\n",
            "채용공고 2 (ID: http://recruitontology.org/Recruit_recruitments_1670): 분야=초전도양자컴퓨팅, 키워드=['초전도', '큐비트', '나노 전자소자', '양자프로세서', '양자컴퓨팅', '양자알고리즘'], 기술=['초전도 큐비트 소자 제작 및 특성평가 기술', '나노 전자소자 제작 및 특성평가 기술', '양자프로세서 시스템 고주파 제어', '측정 및 분석 기술', '양자컴퓨팅을 위한 양자알고리즘 소프트웨어 및 프로그래밍 기술']\n",
            "채용공고 3 (ID: http://recruitontology.org/Recruit_recruitments_1723): 분야=고추 영상기반 형태적 특성 분석기술 및 생력형 소재 개발, 키워드=['고추', '영상기반', '형태적 특성', '생력형'], 기술=[]\n",
            "채용공고 4 (ID: http://recruitontology.org/Recruit_recruitments_810): 분야=드론활용 / 코딩SW 분야, 키워드=['드론활용', '코딩SW'], 기술=['드론활용', '코딩SW 관련 기술 및 지식', '프로젝트 수주 능력']\n",
            "채용공고 5 (ID: http://recruitontology.org/Recruit_recruitments_1628): 분야=이공계 및 관련 분야(과학기술정책, 전산, 컴퓨터, 시스템 공학 등 공학계열), 키워드=['디지털 트윈', '가상가상/증강현실', '가상 플랫폼'], 기술=['과학기술분야 정책수립 및 기획 능력', '디지털 트윈', '가상가상/증강현실', '가상 플랫폼 관련 기술 지식']\n",
            "===========================\n",
            "===== 논문 샘플 =====\n",
            "논문 1 (ID: ART002835262): 제목=불균형 시계열 자료를 위한 분류 알고리즘 적용방안: 기업 부도모형을 중심으로, 키워드=['기업 부도모형', '불균형 데이터', '시계열 데이터', 'SMOTE', '전진교차검증.', 'Corporate default model', 'Forward cross validation', 'Imbalanced data', 'Time series data', 'SMOTE.']\n",
            "논문 2 (ID: ART003138314): 제목=적외선 영상 전처리에 따른 딥러닝 기반의 객체 탐지 모델 성능 평가, 키워드=['Deep Learning', 'Image Preprocessing', 'Object Detection', '딥러닝', '영상 전처리', '객체 탐지']\n",
            "논문 3 (ID: ART002750407): 제목=공안협의소설과 문화콘텐츠— 포청천 캐릭터 콘텐츠 기획을 위한 프로젝트 기반 수업모델을 중심으로, 키워드=['공안협의소설', '공안소설', '포청천 캐릭터', '캐릭터 콘텐츠', '프로젝트 기반 수업모델', '문화콘텐츠 기획', 'Gong’anxiayi Novels', 'Gong’an Novels', 'Baoqingtian character', 'character contents', 'project-based classes', 'planning cultural contents']\n",
            "논문 4 (ID: ART003124563): 제목=반도체 인력 양성 및 채용을 위한 거대 언어 모델 기반 학부 공학 교육과정 설계 및 평가 전략 -ChatGPT-4o를 이용한 KAIST 공학 교육과정 분석을 중심으로-, 키워드=['거대 언어 모델', '공학 교육과정', '직무 적합성 인력선발', '반도체 산업', 'Large Language Model', 'Engineering Curriculum', 'Job Fit', 'Workforce Selection', 'Semiconductor Industry']\n",
            "논문 5 (ID: ART002779803): 제목=한글 조합성에 기반한 최소 글자를 사용하는한글 폰트 생성 모델, 키워드=['한글폰트생성', '폰트구성요소', 'GAN', 'Few-shot', 'Hangul Font Generation', 'Font Components', 'GAN', 'Few-shot']\n",
            "===========================\n",
            "채용공고 및 논문 데이터 전처리 중...\n",
            "전처리 완료: 30 채용공고, 50 논문\n",
            "템플릿 문장 처리 중... (총 5 배치)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "템플릿 배치 처리: 100%|██████████| 5/5 [00:00<00:00, 37315.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "총 매칭 시도: 4900, 성공: 4478, 성공률: 91.39%\n",
            "템플릿 문장에서 556개의 관계를 추출했습니다.\n",
            "템플릿 매칭 결과: 시도 4900, 성공 4478, 성공률 91.39%\n",
            "성공률이 너무 높습니다. 임계값을 0.10로 높입니다.\n",
            "\n",
            "시도 5/20: 템플릿 생성 중... (임계값: 0.10)\n",
            "\n",
            "=== 템플릿 기반 문장 생성 시작 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating job field templates: 100%|██████████| 300/300 [00:00<00:00, 136252.43it/s]\n",
            "Generating job-paper templates: 100%|██████████| 300/300 [00:00<00:00, 7105.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "생성된 고유 템플릿 문장 수: 500\n",
            "템플릿 문장에서 채용공고-논문 관계 추출 중...\n",
            "===== 템플릿 문장 샘플 =====\n",
            "템플릿 문장 1: 정보화 사업 기획 및 관리 분야는 정보통신 전공과 관련이 있습니다.\n",
            "템플릿 문장 2: 공학계열 직무는 반도체 인력 양성 및 채용을 위한 거대 언어 모델 기반 학부 공학 교육과정 설계 및 평가 전략 -ChatGPT-4o를 이용한 KAIST 공학 교육과정 분석을 중심으로- 연구 결과를 활용합니다.\n",
            "템플릿 문장 3: 임베디드 시스템 설계 전공은 시스템 키워드와 관련이 있습니다.\n",
            "템플릿 문장 4: 센터장 분야는 ICT 전공과 관련이 있습니다.\n",
            "템플릿 문장 5: 빔라인 제어 & DAQ 시스템 개발 전공은 빔라인 키워드와 관련이 있습니다.\n",
            "템플릿 문장 6: AI 전공 분야는 sustainable design(지속가능디자인) 주제의 논문과 관련이 있습니다.\n",
            "템플릿 문장 7: 인공지능 전공 또는 SW 관련 전공 분야는 컴퓨터공학 전공과 관련이 있습니다.\n",
            "템플릿 문장 8: 생산 AI 응용(생산/제조/품질/로봇 AI, 디지털트윈, 스마트팩토리, HMI/UI/UX AI 등)(Production with AI Applications) 직무는 암호화폐 관련 범죄에 대한 형사법적 고찰 연구 결과를 활용합니다.\n",
            "템플릿 문장 9: 병렬처리 분야는 학습 관리 시스템 (LMS) 지식이 필요합니다.\n",
            "템플릿 문장 10: 영상기반 배추 내재해성 평가기준 개발 분야는 원예학 전공 지식이 필요합니다.\n",
            "===========================\n",
            "===== 채용공고 샘플 =====\n",
            "채용공고 1 (ID: http://recruitontology.org/Recruit_recruitments_1084): 분야=스마트 자동화 생산기술, 키워드=['AI', '생산공정관리', '스마트 자동화'], 기술=[]\n",
            "채용공고 2 (ID: http://recruitontology.org/Recruit_recruitments_1670): 분야=초전도양자컴퓨팅, 키워드=['초전도', '큐비트', '나노 전자소자', '양자프로세서', '양자컴퓨팅', '양자알고리즘'], 기술=['초전도 큐비트 소자 제작 및 특성평가 기술', '나노 전자소자 제작 및 특성평가 기술', '양자프로세서 시스템 고주파 제어', '측정 및 분석 기술', '양자컴퓨팅을 위한 양자알고리즘 소프트웨어 및 프로그래밍 기술']\n",
            "채용공고 3 (ID: http://recruitontology.org/Recruit_recruitments_1723): 분야=고추 영상기반 형태적 특성 분석기술 및 생력형 소재 개발, 키워드=['고추', '영상기반', '형태적 특성', '생력형'], 기술=[]\n",
            "채용공고 4 (ID: http://recruitontology.org/Recruit_recruitments_810): 분야=드론활용 / 코딩SW 분야, 키워드=['드론활용', '코딩SW'], 기술=['드론활용', '코딩SW 관련 기술 및 지식', '프로젝트 수주 능력']\n",
            "채용공고 5 (ID: http://recruitontology.org/Recruit_recruitments_1628): 분야=이공계 및 관련 분야(과학기술정책, 전산, 컴퓨터, 시스템 공학 등 공학계열), 키워드=['디지털 트윈', '가상가상/증강현실', '가상 플랫폼'], 기술=['과학기술분야 정책수립 및 기획 능력', '디지털 트윈', '가상가상/증강현실', '가상 플랫폼 관련 기술 지식']\n",
            "===========================\n",
            "===== 논문 샘플 =====\n",
            "논문 1 (ID: ART002835262): 제목=불균형 시계열 자료를 위한 분류 알고리즘 적용방안: 기업 부도모형을 중심으로, 키워드=['기업 부도모형', '불균형 데이터', '시계열 데이터', 'SMOTE', '전진교차검증.', 'Corporate default model', 'Forward cross validation', 'Imbalanced data', 'Time series data', 'SMOTE.']\n",
            "논문 2 (ID: ART003138314): 제목=적외선 영상 전처리에 따른 딥러닝 기반의 객체 탐지 모델 성능 평가, 키워드=['Deep Learning', 'Image Preprocessing', 'Object Detection', '딥러닝', '영상 전처리', '객체 탐지']\n",
            "논문 3 (ID: ART002750407): 제목=공안협의소설과 문화콘텐츠— 포청천 캐릭터 콘텐츠 기획을 위한 프로젝트 기반 수업모델을 중심으로, 키워드=['공안협의소설', '공안소설', '포청천 캐릭터', '캐릭터 콘텐츠', '프로젝트 기반 수업모델', '문화콘텐츠 기획', 'Gong’anxiayi Novels', 'Gong’an Novels', 'Baoqingtian character', 'character contents', 'project-based classes', 'planning cultural contents']\n",
            "논문 4 (ID: ART003124563): 제목=반도체 인력 양성 및 채용을 위한 거대 언어 모델 기반 학부 공학 교육과정 설계 및 평가 전략 -ChatGPT-4o를 이용한 KAIST 공학 교육과정 분석을 중심으로-, 키워드=['거대 언어 모델', '공학 교육과정', '직무 적합성 인력선발', '반도체 산업', 'Large Language Model', 'Engineering Curriculum', 'Job Fit', 'Workforce Selection', 'Semiconductor Industry']\n",
            "논문 5 (ID: ART002779803): 제목=한글 조합성에 기반한 최소 글자를 사용하는한글 폰트 생성 모델, 키워드=['한글폰트생성', '폰트구성요소', 'GAN', 'Few-shot', 'Hangul Font Generation', 'Font Components', 'GAN', 'Few-shot']\n",
            "===========================\n",
            "채용공고 및 논문 데이터 전처리 중...\n",
            "전처리 완료: 30 채용공고, 50 논문\n",
            "템플릿 문장 처리 중... (총 5 배치)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "템플릿 배치 처리: 100%|██████████| 5/5 [00:00<00:00, 41201.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "총 매칭 시도: 5600, 성공: 2977, 성공률: 53.16%\n",
            "템플릿 문장에서 436개의 관계를 추출했습니다.\n",
            "템플릿 매칭 결과: 시도 5600, 성공 2977, 성공률 53.16%\n",
            "성공률이 너무 낮습니다. 임계값을 0.05로 낮춥니다.\n",
            "\n",
            "시도 6/20: 템플릿 생성 중... (임계값: 0.05)\n",
            "\n",
            "=== 템플릿 기반 문장 생성 시작 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating job field templates: 100%|██████████| 300/300 [00:00<00:00, 132619.22it/s]\n",
            "Generating job-paper templates: 100%|██████████| 300/300 [00:00<00:00, 7865.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "생성된 고유 템플릿 문장 수: 500\n",
            "템플릿 문장에서 채용공고-논문 관계 추출 중...\n",
            "===== 템플릿 문장 샘플 =====\n",
            "템플릿 문장 1: 박사후 연수 연구원(RA-02) 전공은 C++ 키워드와 관련이 있습니다.\n",
            "템플릿 문장 2: 인공지능반도체 집적회로설계 분야 전공은 반도체 키워드와 관련이 있습니다.\n",
            "템플릿 문장 3: CSPN 직무는 암호화폐 관련 범죄에 대한 형사법적 고찰 연구 결과를 활용합니다.\n",
            "템플릿 문장 4: 인공지능 전공 또는 SW 관련 전공 분야는 인공지능 전공과 관련이 있습니다.\n",
            "템플릿 문장 5: 생산 AI 응용(생산/제조/품질/로봇 AI, 디지털트윈, 스마트팩토리, HMI/UI/UX AI 등)(Production with AI Applications) 전공은 HMI 키워드와 관련이 있습니다.\n",
            "템플릿 문장 6: 컴퓨터 공학 전분야 전공은 컴퓨터공학 키워드와 관련이 있습니다.\n",
            "템플릿 문장 7: 농업전자 및 바이오센서 [Agricultural Electronics and Biosensor] 분야는 바이오시스템공학 전공과 관련이 있습니다.\n",
            "템플릿 문장 8: 컴퓨터공학(클라우드컴퓨팅, 빅 데이터 처리) 분야는 컴퓨터공학 전공과 관련이 있습니다.\n",
            "템플릿 문장 9: 자율주행 전공은 지도작성 키워드와 관련이 있습니다.\n",
            "템플릿 문장 10: 자동차 데이터분석 분야는 컴퓨터공학 전공 지식이 필요합니다.\n",
            "===========================\n",
            "===== 채용공고 샘플 =====\n",
            "채용공고 1 (ID: http://recruitontology.org/Recruit_recruitments_1084): 분야=스마트 자동화 생산기술, 키워드=['AI', '생산공정관리', '스마트 자동화'], 기술=[]\n",
            "채용공고 2 (ID: http://recruitontology.org/Recruit_recruitments_1670): 분야=초전도양자컴퓨팅, 키워드=['초전도', '큐비트', '나노 전자소자', '양자프로세서', '양자컴퓨팅', '양자알고리즘'], 기술=['초전도 큐비트 소자 제작 및 특성평가 기술', '나노 전자소자 제작 및 특성평가 기술', '양자프로세서 시스템 고주파 제어', '측정 및 분석 기술', '양자컴퓨팅을 위한 양자알고리즘 소프트웨어 및 프로그래밍 기술']\n",
            "채용공고 3 (ID: http://recruitontology.org/Recruit_recruitments_1723): 분야=고추 영상기반 형태적 특성 분석기술 및 생력형 소재 개발, 키워드=['고추', '영상기반', '형태적 특성', '생력형'], 기술=[]\n",
            "채용공고 4 (ID: http://recruitontology.org/Recruit_recruitments_810): 분야=드론활용 / 코딩SW 분야, 키워드=['드론활용', '코딩SW'], 기술=['드론활용', '코딩SW 관련 기술 및 지식', '프로젝트 수주 능력']\n",
            "채용공고 5 (ID: http://recruitontology.org/Recruit_recruitments_1628): 분야=이공계 및 관련 분야(과학기술정책, 전산, 컴퓨터, 시스템 공학 등 공학계열), 키워드=['디지털 트윈', '가상가상/증강현실', '가상 플랫폼'], 기술=['과학기술분야 정책수립 및 기획 능력', '디지털 트윈', '가상가상/증강현실', '가상 플랫폼 관련 기술 지식']\n",
            "===========================\n",
            "===== 논문 샘플 =====\n",
            "논문 1 (ID: ART002835262): 제목=불균형 시계열 자료를 위한 분류 알고리즘 적용방안: 기업 부도모형을 중심으로, 키워드=['기업 부도모형', '불균형 데이터', '시계열 데이터', 'SMOTE', '전진교차검증.', 'Corporate default model', 'Forward cross validation', 'Imbalanced data', 'Time series data', 'SMOTE.']\n",
            "논문 2 (ID: ART003138314): 제목=적외선 영상 전처리에 따른 딥러닝 기반의 객체 탐지 모델 성능 평가, 키워드=['Deep Learning', 'Image Preprocessing', 'Object Detection', '딥러닝', '영상 전처리', '객체 탐지']\n",
            "논문 3 (ID: ART002750407): 제목=공안협의소설과 문화콘텐츠— 포청천 캐릭터 콘텐츠 기획을 위한 프로젝트 기반 수업모델을 중심으로, 키워드=['공안협의소설', '공안소설', '포청천 캐릭터', '캐릭터 콘텐츠', '프로젝트 기반 수업모델', '문화콘텐츠 기획', 'Gong’anxiayi Novels', 'Gong’an Novels', 'Baoqingtian character', 'character contents', 'project-based classes', 'planning cultural contents']\n",
            "논문 4 (ID: ART003124563): 제목=반도체 인력 양성 및 채용을 위한 거대 언어 모델 기반 학부 공학 교육과정 설계 및 평가 전략 -ChatGPT-4o를 이용한 KAIST 공학 교육과정 분석을 중심으로-, 키워드=['거대 언어 모델', '공학 교육과정', '직무 적합성 인력선발', '반도체 산업', 'Large Language Model', 'Engineering Curriculum', 'Job Fit', 'Workforce Selection', 'Semiconductor Industry']\n",
            "논문 5 (ID: ART002779803): 제목=한글 조합성에 기반한 최소 글자를 사용하는한글 폰트 생성 모델, 키워드=['한글폰트생성', '폰트구성요소', 'GAN', 'Few-shot', 'Hangul Font Generation', 'Font Components', 'GAN', 'Few-shot']\n",
            "===========================\n",
            "채용공고 및 논문 데이터 전처리 중...\n",
            "전처리 완료: 30 채용공고, 50 논문\n",
            "템플릿 문장 처리 중... (총 5 배치)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "템플릿 배치 처리: 100%|██████████| 5/5 [00:00<00:00, 65128.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "총 매칭 시도: 4700, 성공: 4321, 성공률: 91.94%\n",
            "템플릿 문장에서 665개의 관계를 추출했습니다.\n",
            "템플릿 매칭 결과: 시도 4700, 성공 4321, 성공률 91.94%\n",
            "성공률이 너무 높습니다. 임계값을 0.10로 높입니다.\n",
            "\n",
            "시도 7/20: 템플릿 생성 중... (임계값: 0.10)\n",
            "\n",
            "=== 템플릿 기반 문장 생성 시작 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating job field templates: 100%|██████████| 300/300 [00:00<00:00, 130582.32it/s]\n",
            "Generating job-paper templates: 100%|██████████| 300/300 [00:00<00:00, 7673.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "생성된 고유 템플릿 문장 수: 500\n",
            "템플릿 문장에서 채용공고-논문 관계 추출 중...\n",
            "===== 템플릿 문장 샘플 =====\n",
            "템플릿 문장 1: 정보시스템 품질관리 분야는 Graduate School of Education 지식이 필요합니다.\n",
            "템플릿 문장 2: AI와 Big data를 활용한 건설 데이터 분석 분야는 사이버전자전 지식이 필요합니다.\n",
            "템플릿 문장 3: AI 전공은 IT공학 키워드와 관련이 있습니다.\n",
            "템플릿 문장 4: 인공지능 분야는 산업공학 전공 지식이 필요합니다.\n",
            "템플릿 문장 5: 컴퓨터그래픽스, 3D AR/VR 분야는 컴퓨터그래픽스 주제의 논문과 관련이 있습니다.\n",
            "템플릿 문장 6: 유전체/단백체 분석 플랫폼 및 DB 구축 분야는 tax support 주제의 논문과 관련이 있습니다.\n",
            "템플릿 문장 7: 전산과학교수 직무는 외국인 유학생 대상의 취업 지원 비교과 프로그램에 관한 소고 연구 결과를 활용합니다.\n",
            "템플릿 문장 8: 멀티미디어공학과 게임공학전공 직무는 불균형 시계열 자료를 위한 분류 알고리즘 적용방안: 기업 부도모형을 중심으로 연구 결과를 활용합니다.\n",
            "템플릿 문장 9: DMZ 생생누리(실감미디어 체험관) 분야는 영상콘텐츠 전공 지식이 필요합니다.\n",
            "템플릿 문장 10: 컴퓨터소프트웨어 전 분야 직무는 한국군의 ‘사이버전자전’ 수행을 위한 전략분석 연구 결과를 활용합니다.\n",
            "===========================\n",
            "===== 채용공고 샘플 =====\n",
            "채용공고 1 (ID: http://recruitontology.org/Recruit_recruitments_1084): 분야=스마트 자동화 생산기술, 키워드=['AI', '생산공정관리', '스마트 자동화'], 기술=[]\n",
            "채용공고 2 (ID: http://recruitontology.org/Recruit_recruitments_1670): 분야=초전도양자컴퓨팅, 키워드=['초전도', '큐비트', '나노 전자소자', '양자프로세서', '양자컴퓨팅', '양자알고리즘'], 기술=['초전도 큐비트 소자 제작 및 특성평가 기술', '나노 전자소자 제작 및 특성평가 기술', '양자프로세서 시스템 고주파 제어', '측정 및 분석 기술', '양자컴퓨팅을 위한 양자알고리즘 소프트웨어 및 프로그래밍 기술']\n",
            "채용공고 3 (ID: http://recruitontology.org/Recruit_recruitments_1723): 분야=고추 영상기반 형태적 특성 분석기술 및 생력형 소재 개발, 키워드=['고추', '영상기반', '형태적 특성', '생력형'], 기술=[]\n",
            "채용공고 4 (ID: http://recruitontology.org/Recruit_recruitments_810): 분야=드론활용 / 코딩SW 분야, 키워드=['드론활용', '코딩SW'], 기술=['드론활용', '코딩SW 관련 기술 및 지식', '프로젝트 수주 능력']\n",
            "채용공고 5 (ID: http://recruitontology.org/Recruit_recruitments_1628): 분야=이공계 및 관련 분야(과학기술정책, 전산, 컴퓨터, 시스템 공학 등 공학계열), 키워드=['디지털 트윈', '가상가상/증강현실', '가상 플랫폼'], 기술=['과학기술분야 정책수립 및 기획 능력', '디지털 트윈', '가상가상/증강현실', '가상 플랫폼 관련 기술 지식']\n",
            "===========================\n",
            "===== 논문 샘플 =====\n",
            "논문 1 (ID: ART002835262): 제목=불균형 시계열 자료를 위한 분류 알고리즘 적용방안: 기업 부도모형을 중심으로, 키워드=['기업 부도모형', '불균형 데이터', '시계열 데이터', 'SMOTE', '전진교차검증.', 'Corporate default model', 'Forward cross validation', 'Imbalanced data', 'Time series data', 'SMOTE.']\n",
            "논문 2 (ID: ART003138314): 제목=적외선 영상 전처리에 따른 딥러닝 기반의 객체 탐지 모델 성능 평가, 키워드=['Deep Learning', 'Image Preprocessing', 'Object Detection', '딥러닝', '영상 전처리', '객체 탐지']\n",
            "논문 3 (ID: ART002750407): 제목=공안협의소설과 문화콘텐츠— 포청천 캐릭터 콘텐츠 기획을 위한 프로젝트 기반 수업모델을 중심으로, 키워드=['공안협의소설', '공안소설', '포청천 캐릭터', '캐릭터 콘텐츠', '프로젝트 기반 수업모델', '문화콘텐츠 기획', 'Gong’anxiayi Novels', 'Gong’an Novels', 'Baoqingtian character', 'character contents', 'project-based classes', 'planning cultural contents']\n",
            "논문 4 (ID: ART003124563): 제목=반도체 인력 양성 및 채용을 위한 거대 언어 모델 기반 학부 공학 교육과정 설계 및 평가 전략 -ChatGPT-4o를 이용한 KAIST 공학 교육과정 분석을 중심으로-, 키워드=['거대 언어 모델', '공학 교육과정', '직무 적합성 인력선발', '반도체 산업', 'Large Language Model', 'Engineering Curriculum', 'Job Fit', 'Workforce Selection', 'Semiconductor Industry']\n",
            "논문 5 (ID: ART002779803): 제목=한글 조합성에 기반한 최소 글자를 사용하는한글 폰트 생성 모델, 키워드=['한글폰트생성', '폰트구성요소', 'GAN', 'Few-shot', 'Hangul Font Generation', 'Font Components', 'GAN', 'Few-shot']\n",
            "===========================\n",
            "채용공고 및 논문 데이터 전처리 중...\n",
            "전처리 완료: 30 채용공고, 50 논문\n",
            "템플릿 문장 처리 중... (총 5 배치)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "템플릿 배치 처리: 100%|██████████| 5/5 [00:00<00:00, 40407.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "총 매칭 시도: 5900, 성공: 3182, 성공률: 53.93%\n",
            "템플릿 문장에서 460개의 관계를 추출했습니다.\n",
            "템플릿 매칭 결과: 시도 5900, 성공 3182, 성공률 53.93%\n",
            "성공률이 너무 낮습니다. 임계값을 0.05로 낮춥니다.\n",
            "\n",
            "시도 8/20: 템플릿 생성 중... (임계값: 0.05)\n",
            "\n",
            "=== 템플릿 기반 문장 생성 시작 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating job field templates: 100%|██████████| 300/300 [00:00<00:00, 125203.10it/s]\n",
            "Generating job-paper templates: 100%|██████████| 300/300 [00:00<00:00, 7795.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "생성된 고유 템플릿 문장 수: 500\n",
            "템플릿 문장에서 채용공고-논문 관계 추출 중...\n",
            "===== 템플릿 문장 샘플 =====\n",
            "템플릿 문장 1: 인공지능(1) 분야는 컴퓨터공학 전공 지식이 필요합니다.\n",
            "템플릿 문장 2: 정보화(보훈) 분야는 사이버전자전 지식이 필요합니다.\n",
            "템플릿 문장 3: 컴퓨터공학(인공지능, 데이터베이스) 직무는 한국군의 ‘사이버전자전’ 수행을 위한 전략분석 연구 결과를 활용합니다.\n",
            "템플릿 문장 4: 전산학(컴퓨터공학, 컴퓨터과학, 컴퓨터학 등) 전공은 딥러닝 키워드와 관련이 있습니다.\n",
            "템플릿 문장 5: 바이오 데이터 연계 및 등록 시스템 구축·운영 분야는 전산 전공 지식이 필요합니다.\n",
            "템플릿 문장 6: 정보보안 및 데이터베이스 직무는 한글 조합성에 기반한 최소 글자를 사용하는한글 폰트 생성 모델 연구 결과를 활용합니다.\n",
            "템플릿 문장 7: 빅데이터플랫폼 운영·관리(선임급) 전공은 소프트웨어공학 키워드와 관련이 있습니다.\n",
            "템플릿 문장 8: 빅데이터 분석 전공은 BI솔루션 키워드와 관련이 있습니다.\n",
            "템플릿 문장 9: 컴퓨터공학 전 분야 분야는 거대 언어 모델 지식이 필요합니다.\n",
            "템플릿 문장 10: 데이터베이스, 인공지능 전공은 인공지능 키워드와 관련이 있습니다.\n",
            "===========================\n",
            "===== 채용공고 샘플 =====\n",
            "채용공고 1 (ID: http://recruitontology.org/Recruit_recruitments_1084): 분야=스마트 자동화 생산기술, 키워드=['AI', '생산공정관리', '스마트 자동화'], 기술=[]\n",
            "채용공고 2 (ID: http://recruitontology.org/Recruit_recruitments_1670): 분야=초전도양자컴퓨팅, 키워드=['초전도', '큐비트', '나노 전자소자', '양자프로세서', '양자컴퓨팅', '양자알고리즘'], 기술=['초전도 큐비트 소자 제작 및 특성평가 기술', '나노 전자소자 제작 및 특성평가 기술', '양자프로세서 시스템 고주파 제어', '측정 및 분석 기술', '양자컴퓨팅을 위한 양자알고리즘 소프트웨어 및 프로그래밍 기술']\n",
            "채용공고 3 (ID: http://recruitontology.org/Recruit_recruitments_1723): 분야=고추 영상기반 형태적 특성 분석기술 및 생력형 소재 개발, 키워드=['고추', '영상기반', '형태적 특성', '생력형'], 기술=[]\n",
            "채용공고 4 (ID: http://recruitontology.org/Recruit_recruitments_810): 분야=드론활용 / 코딩SW 분야, 키워드=['드론활용', '코딩SW'], 기술=['드론활용', '코딩SW 관련 기술 및 지식', '프로젝트 수주 능력']\n",
            "채용공고 5 (ID: http://recruitontology.org/Recruit_recruitments_1628): 분야=이공계 및 관련 분야(과학기술정책, 전산, 컴퓨터, 시스템 공학 등 공학계열), 키워드=['디지털 트윈', '가상가상/증강현실', '가상 플랫폼'], 기술=['과학기술분야 정책수립 및 기획 능력', '디지털 트윈', '가상가상/증강현실', '가상 플랫폼 관련 기술 지식']\n",
            "===========================\n",
            "===== 논문 샘플 =====\n",
            "논문 1 (ID: ART002835262): 제목=불균형 시계열 자료를 위한 분류 알고리즘 적용방안: 기업 부도모형을 중심으로, 키워드=['기업 부도모형', '불균형 데이터', '시계열 데이터', 'SMOTE', '전진교차검증.', 'Corporate default model', 'Forward cross validation', 'Imbalanced data', 'Time series data', 'SMOTE.']\n",
            "논문 2 (ID: ART003138314): 제목=적외선 영상 전처리에 따른 딥러닝 기반의 객체 탐지 모델 성능 평가, 키워드=['Deep Learning', 'Image Preprocessing', 'Object Detection', '딥러닝', '영상 전처리', '객체 탐지']\n",
            "논문 3 (ID: ART002750407): 제목=공안협의소설과 문화콘텐츠— 포청천 캐릭터 콘텐츠 기획을 위한 프로젝트 기반 수업모델을 중심으로, 키워드=['공안협의소설', '공안소설', '포청천 캐릭터', '캐릭터 콘텐츠', '프로젝트 기반 수업모델', '문화콘텐츠 기획', 'Gong’anxiayi Novels', 'Gong’an Novels', 'Baoqingtian character', 'character contents', 'project-based classes', 'planning cultural contents']\n",
            "논문 4 (ID: ART003124563): 제목=반도체 인력 양성 및 채용을 위한 거대 언어 모델 기반 학부 공학 교육과정 설계 및 평가 전략 -ChatGPT-4o를 이용한 KAIST 공학 교육과정 분석을 중심으로-, 키워드=['거대 언어 모델', '공학 교육과정', '직무 적합성 인력선발', '반도체 산업', 'Large Language Model', 'Engineering Curriculum', 'Job Fit', 'Workforce Selection', 'Semiconductor Industry']\n",
            "논문 5 (ID: ART002779803): 제목=한글 조합성에 기반한 최소 글자를 사용하는한글 폰트 생성 모델, 키워드=['한글폰트생성', '폰트구성요소', 'GAN', 'Few-shot', 'Hangul Font Generation', 'Font Components', 'GAN', 'Few-shot']\n",
            "===========================\n",
            "채용공고 및 논문 데이터 전처리 중...\n",
            "전처리 완료: 30 채용공고, 50 논문\n",
            "템플릿 문장 처리 중... (총 5 배치)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "템플릿 배치 처리: 100%|██████████| 5/5 [00:00<00:00, 44337.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "총 매칭 시도: 5300, 성공: 5109, 성공률: 96.40%\n",
            "템플릿 문장에서 556개의 관계를 추출했습니다.\n",
            "템플릿 매칭 결과: 시도 5300, 성공 5109, 성공률 96.40%\n",
            "성공률이 너무 높습니다. 임계값을 0.10로 높입니다.\n",
            "\n",
            "시도 9/20: 템플릿 생성 중... (임계값: 0.10)\n",
            "\n",
            "=== 템플릿 기반 문장 생성 시작 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating job field templates: 100%|██████████| 300/300 [00:00<00:00, 104892.56it/s]\n",
            "Generating job-paper templates: 100%|██████████| 300/300 [00:00<00:00, 7151.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "생성된 고유 템플릿 문장 수: 500\n",
            "템플릿 문장에서 채용공고-논문 관계 추출 중...\n",
            "===== 템플릿 문장 샘플 =====\n",
            "템플릿 문장 1: 물류운영 최적화 직무는 벤처 혁신기업 지원방안으로서의 기업성장집합투자기구 세제지원에 관한 소고 연구 결과를 활용합니다.\n",
            "템플릿 문장 2: 전산 전공은 JAVA 키워드와 관련이 있습니다.\n",
            "템플릿 문장 3: 전문직원(조사연구) 분야는 통계학 전공 지식이 필요합니다.\n",
            "템플릿 문장 4: 유전체/단백체 분석 플랫폼 및 DB 구축 분야는 통계학 전공과 관련이 있습니다.\n",
            "템플릿 문장 5: 빅데이터 분야는 빅데이터 주제의 논문과 관련이 있습니다.\n",
            "템플릿 문장 6: 정보보안 및 데이터베이스 분야는 응용수학 전공 지식이 필요합니다.\n",
            "템플릿 문장 7: 전기/열전재료 디지털 설계 기술 분야는 terrorism 지식이 필요합니다.\n",
            "템플릿 문장 8: 전산과학교수 직무는 가치기반 수용모델(VAM) 및 기대일치모델(ECM)을 적용한 온라인 여행사(OTA)의 지각된 가치 인식이 이용자의 지속적 사용의도에 미치는 영향 연구 결과를 활용합니다.\n",
            "템플릿 문장 9: 컴퓨터 전분야(All areas in Computer Science and Engineering) 분야는 기업 부도모형 지식이 필요합니다.\n",
            "템플릿 문장 10: 게임프로그래밍 분야는 게임개발 전공과 관련이 있습니다.\n",
            "===========================\n",
            "===== 채용공고 샘플 =====\n",
            "채용공고 1 (ID: http://recruitontology.org/Recruit_recruitments_1084): 분야=스마트 자동화 생산기술, 키워드=['AI', '생산공정관리', '스마트 자동화'], 기술=[]\n",
            "채용공고 2 (ID: http://recruitontology.org/Recruit_recruitments_1670): 분야=초전도양자컴퓨팅, 키워드=['초전도', '큐비트', '나노 전자소자', '양자프로세서', '양자컴퓨팅', '양자알고리즘'], 기술=['초전도 큐비트 소자 제작 및 특성평가 기술', '나노 전자소자 제작 및 특성평가 기술', '양자프로세서 시스템 고주파 제어', '측정 및 분석 기술', '양자컴퓨팅을 위한 양자알고리즘 소프트웨어 및 프로그래밍 기술']\n",
            "채용공고 3 (ID: http://recruitontology.org/Recruit_recruitments_1723): 분야=고추 영상기반 형태적 특성 분석기술 및 생력형 소재 개발, 키워드=['고추', '영상기반', '형태적 특성', '생력형'], 기술=[]\n",
            "채용공고 4 (ID: http://recruitontology.org/Recruit_recruitments_810): 분야=드론활용 / 코딩SW 분야, 키워드=['드론활용', '코딩SW'], 기술=['드론활용', '코딩SW 관련 기술 및 지식', '프로젝트 수주 능력']\n",
            "채용공고 5 (ID: http://recruitontology.org/Recruit_recruitments_1628): 분야=이공계 및 관련 분야(과학기술정책, 전산, 컴퓨터, 시스템 공학 등 공학계열), 키워드=['디지털 트윈', '가상가상/증강현실', '가상 플랫폼'], 기술=['과학기술분야 정책수립 및 기획 능력', '디지털 트윈', '가상가상/증강현실', '가상 플랫폼 관련 기술 지식']\n",
            "===========================\n",
            "===== 논문 샘플 =====\n",
            "논문 1 (ID: ART002835262): 제목=불균형 시계열 자료를 위한 분류 알고리즘 적용방안: 기업 부도모형을 중심으로, 키워드=['기업 부도모형', '불균형 데이터', '시계열 데이터', 'SMOTE', '전진교차검증.', 'Corporate default model', 'Forward cross validation', 'Imbalanced data', 'Time series data', 'SMOTE.']\n",
            "논문 2 (ID: ART003138314): 제목=적외선 영상 전처리에 따른 딥러닝 기반의 객체 탐지 모델 성능 평가, 키워드=['Deep Learning', 'Image Preprocessing', 'Object Detection', '딥러닝', '영상 전처리', '객체 탐지']\n",
            "논문 3 (ID: ART002750407): 제목=공안협의소설과 문화콘텐츠— 포청천 캐릭터 콘텐츠 기획을 위한 프로젝트 기반 수업모델을 중심으로, 키워드=['공안협의소설', '공안소설', '포청천 캐릭터', '캐릭터 콘텐츠', '프로젝트 기반 수업모델', '문화콘텐츠 기획', 'Gong’anxiayi Novels', 'Gong’an Novels', 'Baoqingtian character', 'character contents', 'project-based classes', 'planning cultural contents']\n",
            "논문 4 (ID: ART003124563): 제목=반도체 인력 양성 및 채용을 위한 거대 언어 모델 기반 학부 공학 교육과정 설계 및 평가 전략 -ChatGPT-4o를 이용한 KAIST 공학 교육과정 분석을 중심으로-, 키워드=['거대 언어 모델', '공학 교육과정', '직무 적합성 인력선발', '반도체 산업', 'Large Language Model', 'Engineering Curriculum', 'Job Fit', 'Workforce Selection', 'Semiconductor Industry']\n",
            "논문 5 (ID: ART002779803): 제목=한글 조합성에 기반한 최소 글자를 사용하는한글 폰트 생성 모델, 키워드=['한글폰트생성', '폰트구성요소', 'GAN', 'Few-shot', 'Hangul Font Generation', 'Font Components', 'GAN', 'Few-shot']\n",
            "===========================\n",
            "채용공고 및 논문 데이터 전처리 중...\n",
            "전처리 완료: 30 채용공고, 50 논문\n",
            "템플릿 문장 처리 중... (총 5 배치)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "템플릿 배치 처리: 100%|██████████| 5/5 [00:00<00:00, 43062.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "총 매칭 시도: 6050, 성공: 3302, 성공률: 54.58%\n",
            "템플릿 문장에서 466개의 관계를 추출했습니다.\n",
            "템플릿 매칭 결과: 시도 6050, 성공 3302, 성공률 54.58%\n",
            "성공률이 너무 낮습니다. 임계값을 0.05로 낮춥니다.\n",
            "\n",
            "시도 10/20: 템플릿 생성 중... (임계값: 0.05)\n",
            "\n",
            "=== 템플릿 기반 문장 생성 시작 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating job field templates: 100%|██████████| 300/300 [00:00<00:00, 122976.08it/s]\n",
            "Generating job-paper templates: 100%|██████████| 300/300 [00:00<00:00, 7116.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "생성된 고유 템플릿 문장 수: 500\n",
            "템플릿 문장에서 채용공고-논문 관계 추출 중...\n",
            "===== 템플릿 문장 샘플 =====\n",
            "템플릿 문장 1: 시뮬레이션 분야는 경영정보학 전공 지식이 필요합니다.\n",
            "템플릿 문장 2: 공학 분야 분야는 sustainable design(지속가능디자인) 주제의 논문과 관련이 있습니다.\n",
            "템플릿 문장 3: 자료구조&알고리즘, 컴퓨터구조 전공은 알고리즘 키워드와 관련이 있습니다.\n",
            "템플릿 문장 4: 특허분석평가시스템 운영(본회) 전공은 데이터 키워드와 관련이 있습니다.\n",
            "템플릿 문장 5: 웹개발자 분야는 소프트웨어공학 전공과 관련이 있습니다.\n",
            "템플릿 문장 6: 컴퓨터공학 전 분야 직무는 적외선 영상 전처리에 따른 딥러닝 기반의 객체 탐지 모델 성능 평가 연구 결과를 활용합니다.\n",
            "템플릿 문장 7: 메타물질 최적설계 알고리즘 개발 직무는 AI교육을 위한 방향성에 대한 고찰 연구 결과를 활용합니다.\n",
            "템플릿 문장 8: 연구직 Post-Doc. 분야는 산업공학 전공 지식이 필요합니다.\n",
            "템플릿 문장 9: 미래·친환경 자동차 S/W & H/W 개발 분야는 거대 언어 모델 지식이 필요합니다.\n",
            "템플릿 문장 10: 컴퓨터공학 전 분야 분야는 business development company(BDC) 지식이 필요합니다.\n",
            "===========================\n",
            "===== 채용공고 샘플 =====\n",
            "채용공고 1 (ID: http://recruitontology.org/Recruit_recruitments_1084): 분야=스마트 자동화 생산기술, 키워드=['AI', '생산공정관리', '스마트 자동화'], 기술=[]\n",
            "채용공고 2 (ID: http://recruitontology.org/Recruit_recruitments_1670): 분야=초전도양자컴퓨팅, 키워드=['초전도', '큐비트', '나노 전자소자', '양자프로세서', '양자컴퓨팅', '양자알고리즘'], 기술=['초전도 큐비트 소자 제작 및 특성평가 기술', '나노 전자소자 제작 및 특성평가 기술', '양자프로세서 시스템 고주파 제어', '측정 및 분석 기술', '양자컴퓨팅을 위한 양자알고리즘 소프트웨어 및 프로그래밍 기술']\n",
            "채용공고 3 (ID: http://recruitontology.org/Recruit_recruitments_1723): 분야=고추 영상기반 형태적 특성 분석기술 및 생력형 소재 개발, 키워드=['고추', '영상기반', '형태적 특성', '생력형'], 기술=[]\n",
            "채용공고 4 (ID: http://recruitontology.org/Recruit_recruitments_810): 분야=드론활용 / 코딩SW 분야, 키워드=['드론활용', '코딩SW'], 기술=['드론활용', '코딩SW 관련 기술 및 지식', '프로젝트 수주 능력']\n",
            "채용공고 5 (ID: http://recruitontology.org/Recruit_recruitments_1628): 분야=이공계 및 관련 분야(과학기술정책, 전산, 컴퓨터, 시스템 공학 등 공학계열), 키워드=['디지털 트윈', '가상가상/증강현실', '가상 플랫폼'], 기술=['과학기술분야 정책수립 및 기획 능력', '디지털 트윈', '가상가상/증강현실', '가상 플랫폼 관련 기술 지식']\n",
            "===========================\n",
            "===== 논문 샘플 =====\n",
            "논문 1 (ID: ART002835262): 제목=불균형 시계열 자료를 위한 분류 알고리즘 적용방안: 기업 부도모형을 중심으로, 키워드=['기업 부도모형', '불균형 데이터', '시계열 데이터', 'SMOTE', '전진교차검증.', 'Corporate default model', 'Forward cross validation', 'Imbalanced data', 'Time series data', 'SMOTE.']\n",
            "논문 2 (ID: ART003138314): 제목=적외선 영상 전처리에 따른 딥러닝 기반의 객체 탐지 모델 성능 평가, 키워드=['Deep Learning', 'Image Preprocessing', 'Object Detection', '딥러닝', '영상 전처리', '객체 탐지']\n",
            "논문 3 (ID: ART002750407): 제목=공안협의소설과 문화콘텐츠— 포청천 캐릭터 콘텐츠 기획을 위한 프로젝트 기반 수업모델을 중심으로, 키워드=['공안협의소설', '공안소설', '포청천 캐릭터', '캐릭터 콘텐츠', '프로젝트 기반 수업모델', '문화콘텐츠 기획', 'Gong’anxiayi Novels', 'Gong’an Novels', 'Baoqingtian character', 'character contents', 'project-based classes', 'planning cultural contents']\n",
            "논문 4 (ID: ART003124563): 제목=반도체 인력 양성 및 채용을 위한 거대 언어 모델 기반 학부 공학 교육과정 설계 및 평가 전략 -ChatGPT-4o를 이용한 KAIST 공학 교육과정 분석을 중심으로-, 키워드=['거대 언어 모델', '공학 교육과정', '직무 적합성 인력선발', '반도체 산업', 'Large Language Model', 'Engineering Curriculum', 'Job Fit', 'Workforce Selection', 'Semiconductor Industry']\n",
            "논문 5 (ID: ART002779803): 제목=한글 조합성에 기반한 최소 글자를 사용하는한글 폰트 생성 모델, 키워드=['한글폰트생성', '폰트구성요소', 'GAN', 'Few-shot', 'Hangul Font Generation', 'Font Components', 'GAN', 'Few-shot']\n",
            "===========================\n",
            "채용공고 및 논문 데이터 전처리 중...\n",
            "전처리 완료: 30 채용공고, 50 논문\n",
            "템플릿 문장 처리 중... (총 5 배치)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "템플릿 배치 처리: 100%|██████████| 5/5 [00:00<00:00, 55924.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "총 매칭 시도: 4700, 성공: 4155, 성공률: 88.40%\n",
            "템플릿 문장에서 522개의 관계를 추출했습니다.\n",
            "템플릿 매칭 결과: 시도 4700, 성공 4155, 성공률 88.40%\n",
            "목표 성공률 달성! (88.40%)\n",
            "Extracted 522 relations from template sentences\n",
            "Using Hi-BERT for semantic similarity calculation\n",
            "Processing 300 jobs and 500 papers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing job batches: 100%|██████████| 10/10 [15:37<00:00, 93.79s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 2400 weak supervision pairs\n",
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  76%|███████▌  | 102/135 [00:12<00:13,  2.48it/s, loss=2.96, step=51, lr=4.85e-6]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved at step 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 135/135 [00:15<00:00,  8.97it/s, loss=2.93, step=68, lr=4.73e-6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 3.0044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 15/15 [00:01<00:00,  8.64it/s, loss=2.86]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 2.8339\n",
            "Saving best model to /content/drive/MyDrive/Workspaces/Hibrainnet/Hi-BERT/models/hi_bert_model_kosimcse/cl_best_model\n",
            "Checkpoint saved at step epoch_1_best\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  49%|████▉     | 66/135 [00:09<00:28,  2.39it/s, loss=2.98, step=101, lr=4.42e-6]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved at step 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 135/135 [00:16<00:00,  8.35it/s, loss=2.95, step=136, lr=3.99e-6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 2.7799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 15/15 [00:01<00:00,  9.19it/s, loss=2.67]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 2.6090\n",
            "Saving best model to /content/drive/MyDrive/Workspaces/Hibrainnet/Hi-BERT/models/hi_bert_model_kosimcse/cl_best_model\n",
            "Checkpoint saved at step epoch_2_best\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  22%|██▏       | 30/135 [00:05<00:43,  2.42it/s, loss=2.57, step=151, lr=3.78e-6]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved at step 150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  96%|█████████▋| 130/135 [00:17<00:02,  2.47it/s, loss=2.3, step=201, lr=3.02e-6]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved at step 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 135/135 [00:18<00:00,  7.36it/s, loss=2.53, step=204, lr=2.98e-6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 2.6022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 15/15 [00:01<00:00,  9.10it/s, loss=2.64]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 2.5205\n",
            "Saving best model to /content/drive/MyDrive/Workspaces/Hibrainnet/Hi-BERT/models/hi_bert_model_kosimcse/cl_best_model\n",
            "Checkpoint saved at step epoch_3_best\n",
            "=== Hi-BERT 대조학습 훈련 완료 ===\n",
            "\n",
            "=== 테스트 세트에서 Hi-BERT 모델 검증 ===\n",
            "\n",
            "=== Hi-BERT 모델 검증 시작 (배치 처리 최적화) ===\n",
            "논문 임베딩 사전 계산 중...\n",
            "논문 임베딩 100개 사전 계산 완료\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "채용공고 배치 평가:  40%|████      | 2/5 [00:00<00:00, 16.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "채용공고: 연구원, 연구보조원\n",
            "상위 5개 추천 논문:\n",
            "1. 항공 시스템용 전자 하드웨어 개발을 위한 미국 및 유럽의 가이드라인 : RTCA DO-254와 ECSS-Q-ST-60-02C의 비교 분석 연구 (유사도: 0.0704, 스케일링: 0.3522, 종합점수: 0.3057, 관련 (유사도 0.0704, 부분 일치 1개))\n",
            "2. ChatGPT 이용 범죄 대응방안 연구 - 유로폴 및 인터폴의 ChatGPT 사용 권고사항 시사점을 중심으로 - (유사도: 0.0925, 스케일링: 0.4624, 종합점수: 0.1387, 관련 (유사도 0.0925))\n",
            "3. 한국 60세 이상 노인에서 사구체여과율과 악력의 연관성: 2016–2018년 국민건강영양조사 (유사도: 0.0903, 스케일링: 0.4516, 종합점수: 0.1355, 관련 (유사도 0.0903))\n",
            "4. 국내 온라인 학술지의 접근성 평가 및 개선에 관한 연구 (유사도: 0.0861, 스케일링: 0.4305, 종합점수: 0.1291, 관련 (유사도 0.0861))\n",
            "5. 진드기의 수분조절 생리와 진드기 방제전략 (유사도: 0.0831, 스케일링: 0.4156, 종합점수: 0.1247, 관련 (유사도 0.0831))\n",
            "\n",
            "채용공고: 컴퓨터개론\n",
            "상위 5개 추천 논문:\n",
            "1. 초기 국어문법론 탐구의 터전이 된 『진단학보』 (유사도: 0.1637, 스케일링: 0.8184, 종합점수: 0.2455, 관련 (유사도 0.1637))\n",
            "2. 피부색과 무게중심 프로필을 이용한 손동작 인식 알고리즘 (유사도: 0.0154, 스케일링: 0.0772, 종합점수: 0.2232, 비관련 (부분 일치 1개))\n",
            "3. 한국 60세 이상 노인에서 사구체여과율과 악력의 연관성: 2016–2018년 국민건강영양조사 (유사도: 0.1327, 스케일링: 0.6636, 종합점수: 0.1991, 관련 (유사도 0.1327))\n",
            "4. 물리 기반 유한 단층 미끌림 역산을 위한 CPInterface (COMSOL–PyLith Interface) 개발 (유사도: 0.1255, 스케일링: 0.6275, 종합점수: 0.1882, 관련 (유사도 0.1255))\n",
            "5. 일제의 부산·경남권 전쟁 시설 설치와 학생강제동원의 국제법적 불법성 (유사도: 0.1241, 스케일링: 0.6204, 종합점수: 0.1861, 관련 (유사도 0.1241))\n",
            "\n",
            "채용공고: 전산학 전분야\n",
            "상위 5개 추천 논문:\n",
            "1. 항공 시스템용 전자 하드웨어 개발을 위한 미국 및 유럽의 가이드라인 : RTCA DO-254와 ECSS-Q-ST-60-02C의 비교 분석 연구 (유사도: 0.1173, 스케일링: 0.5865, 종합점수: 0.3759, 관련 (유사도 0.1173, 부분 일치 1개))\n",
            "2. 한국 60세 이상 노인에서 사구체여과율과 악력의 연관성: 2016–2018년 국민건강영양조사 (유사도: 0.1235, 스케일링: 0.6176, 종합점수: 0.1853, 관련 (유사도 0.1235))\n",
            "3. 정약용의 ‘영명지체(靈明之體)’, 그 연원과 함의 (유사도: 0.1150, 스케일링: 0.5749, 종합점수: 0.1725, 관련 (유사도 0.1150))\n",
            "4. 인프라스트럭처 정립을 통한 콘텐츠디자인의 개발 방향성 제안 : 온 · 오프라인 콘텐츠를 중심으로 (유사도: 0.1147, 스케일링: 0.5737, 종합점수: 0.1721, 관련 (유사도 0.1147))\n",
            "5. 물리 기반 유한 단층 미끌림 역산을 위한 CPInterface (COMSOL–PyLith Interface) 개발 (유사도: 0.1135, 스케일링: 0.5674, 종합점수: 0.1702, 관련 (유사도 0.1135))\n",
            "\n",
            "채용공고: 광역최적화 및 인공지능(A.I.)\n",
            "상위 5개 추천 논문:\n",
            "1. 군사용 인공지능의 한미간 개발체계 비교 (유사도: 0.2094, 스케일링: 1.0000, 종합점수: 0.8000, 관련 (유사도 0.2094, 키워드 일치 1개 (인공지능), 부분 일치 1개))\n",
            "2. AI 시스템의 위험 완화를 위한 정책적 접근방안 연구: AI 영향평가를 중심으로 (유사도: 0.0925, 스케일링: 0.4624, 종합점수: 0.6387, 관련 (유사도 0.0925, 키워드 일치 2개 (artificial intelligence, 인공지능), 부분 일치 1개))\n",
            "3. 국방 분야에서의 LLM 활용 문제점과 해결 전략 (유사도: 0.1899, 스케일링: 0.9495, 종합점수: 0.5849, 관련 (유사도 0.1899, 키워드 일치 1개 (인공지능)))\n",
            "4. 대학 교양 교육을 위한 AI·소프트웨어 교육 (유사도: 0.1330, 스케일링: 0.6649, 종합점수: 0.4995, 관련 (유사도 0.1330, 키워드 일치 2개 (artificial intelligence, 인공지능)))\n",
            "5. 메타버스(Metaverse) 대중화 시대의 기독교윤리적 책임 (유사도: 0.0511, 스케일링: 0.2555, 종합점수: 0.3766, 관련 (유사도 0.0511, 키워드 일치 1개 (인공지능)))\n",
            "\n",
            "채용공고: 컴퓨터 소프트웨어\n",
            "상위 5개 추천 논문:\n",
            "1. 군사용 인공지능의 한미간 개발체계 비교 (유사도: 0.1828, 스케일링: 0.9138, 종합점수: 0.7741, 관련 (유사도 0.1828, 키워드 일치 1개 (인공지능), 부분 일치 1개))\n",
            "2. 대학 교양 교육을 위한 AI·소프트웨어 교육 (유사도: 0.1414, 스케일링: 0.7069, 종합점수: 0.7121, 관련 (유사도 0.1414, 키워드 일치 1개 (인공지능), 분야-제목 매칭))\n",
            "3. AI 시스템의 위험 완화를 위한 정책적 접근방안 연구: AI 영향평가를 중심으로 (유사도: 0.1181, 스케일링: 0.5907, 종합점수: 0.6772, 관련 (유사도 0.1181, 키워드 일치 1개 (인공지능), 부분 일치 1개))\n",
            "4. 국방 분야에서의 LLM 활용 문제점과 해결 전략 (유사도: 0.1819, 스케일링: 0.9097, 종합점수: 0.5729, 관련 (유사도 0.1819, 키워드 일치 1개 (인공지능)))\n",
            "5. 메타버스(Metaverse) 대중화 시대의 기독교윤리적 책임 (유사도: 0.0624, 스케일링: 0.3118, 종합점수: 0.3935, 관련 (유사도 0.0624, 키워드 일치 1개 (인공지능)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "채용공고 배치 평가: 100%|██████████| 5/5 [00:00<00:00, 21.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 평가 결과 ===\n",
            "모델 유형: Hi-BERT\n",
            "MRR: 0.9750\n",
            "NDCG@10: 0.9633\n",
            "Precision@5: 0.9000\n",
            "Precision@10: 0.9250\n",
            "=== 모델 검증 완료 ===\n",
            "\n",
            "=== 검증 세트에서 Hi-BERT 모델 검증 ===\n",
            "\n",
            "=== Hi-BERT 모델 검증 시작 (배치 처리 최적화) ===\n",
            "논문 임베딩 사전 계산 중...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "논문 임베딩 100개 사전 계산 완료\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "채용공고 배치 평가:  40%|████      | 2/5 [00:00<00:00, 16.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "채용공고: 정보통신 및 보안(정보통신,해킹보안,컴퓨터보안,네트워크보안,멀티미디어보안,사물인터넷,인공지능 등)\n",
            "상위 5개 추천 논문:\n",
            "1. AI 시스템의 위험 완화를 위한 정책적 접근방안 연구: AI 영향평가를 중심으로 (유사도: 0.1108, 스케일링: 0.5542, 종합점수: 0.6662, 관련 (유사도 0.1108, 키워드 일치 1개 (인공지능), 부분 일치 1개))\n",
            "2. 군사용 인공지능의 한미간 개발체계 비교 (유사도: 0.0795, 스케일링: 0.3974, 종합점수: 0.6192, 관련 (유사도 0.0795, 키워드 일치 1개 (인공지능), 부분 일치 1개))\n",
            "3. 국방 분야에서의 LLM 활용 문제점과 해결 전략 (유사도: 0.1313, 스케일링: 0.6564, 종합점수: 0.4969, 관련 (유사도 0.1313, 키워드 일치 1개 (인공지능)))\n",
            "4. 대학 교양 교육을 위한 AI·소프트웨어 교육 (유사도: 0.0661, 스케일링: 0.3303, 종합점수: 0.3991, 관련 (유사도 0.0661, 키워드 일치 1개 (인공지능)))\n",
            "5. 메타버스(Metaverse) 대중화 시대의 기독교윤리적 책임 (유사도: 0.0363, 스케일링: 0.1814, 종합점수: 0.3544, 관련 (키워드 일치 1개 (인공지능)))\n",
            "\n",
            "채용공고: IT 전 분야\n",
            "상위 5개 추천 논문:\n",
            "1. 한국 60세 이상 노인에서 사구체여과율과 악력의 연관성: 2016–2018년 국민건강영양조사 (유사도: 0.1357, 스케일링: 0.6787, 종합점수: 0.2036, 관련 (유사도 0.1357))\n",
            "2. 현장시험을 통한 항만 구역 내 블록 포장의 침하 특성 분석 (유사도: 0.1318, 스케일링: 0.6590, 종합점수: 0.1977, 관련 (유사도 0.1318))\n",
            "3. 이중막 구조를 적용한 우주용 전개형 메쉬 안테나의 열적 특성 분석 (유사도: 0.1315, 스케일링: 0.6573, 종합점수: 0.1972, 관련 (유사도 0.1315))\n",
            "4. 초기 국어문법론 탐구의 터전이 된 『진단학보』 (유사도: 0.1304, 스케일링: 0.6522, 종합점수: 0.1957, 관련 (유사도 0.1304))\n",
            "5. 물리 기반 유한 단층 미끌림 역산을 위한 CPInterface (COMSOL–PyLith Interface) 개발 (유사도: 0.1293, 스케일링: 0.6465, 종합점수: 0.1939, 관련 (유사도 0.1293))\n",
            "\n",
            "채용공고: 컴퓨터공학과 전 분야\n",
            "상위 5개 추천 논문:\n",
            "1. 군사용 인공지능의 한미간 개발체계 비교 (유사도: 0.1347, 스케일링: 0.6736, 종합점수: 0.7021, 관련 (유사도 0.1347, 키워드 일치 1개 (인공지능), 부분 일치 1개))\n",
            "2. AI 시스템의 위험 완화를 위한 정책적 접근방안 연구: AI 영향평가를 중심으로 (유사도: 0.0918, 스케일링: 0.4591, 종합점수: 0.6377, 관련 (유사도 0.0918, 키워드 일치 1개 (인공지능), 부분 일치 1개))\n",
            "3. 국방 분야에서의 LLM 활용 문제점과 해결 전략 (유사도: 0.1504, 스케일링: 0.7522, 종합점수: 0.5256, 관련 (유사도 0.1504, 키워드 일치 1개 (인공지능)))\n",
            "4. 4차 산업혁명 시대 스포츠의 사회･철학적 함의 (유사도: 0.1499, 스케일링: 0.7493, 종합점수: 0.5248, 관련 (유사도 0.1499, 키워드 일치 1개 (빅데이터)))\n",
            "5. 빅데이터 기반 홈 뷰티디바이스에 대한 소비자 인식 변화 분석 -2020년부터 2024년을 중심으로- (유사도: 0.0965, 스케일링: 0.4826, 종합점수: 0.4448, 관련 (유사도 0.0965, 키워드 일치 1개 (빅데이터)))\n",
            "\n",
            "채용공고: 연구직2\n",
            "상위 5개 추천 논문:\n",
            "1. 군사용 인공지능의 한미간 개발체계 비교 (유사도: 0.2146, 스케일링: 1.0000, 종합점수: 0.8000, 관련 (유사도 0.2146, 키워드 일치 1개 (인공지능), 부분 일치 1개))\n",
            "2. AI 시스템의 위험 완화를 위한 정책적 접근방안 연구: AI 영향평가를 중심으로 (유사도: 0.0956, 스케일링: 0.4781, 종합점수: 0.6434, 관련 (유사도 0.0956, 키워드 일치 1개 (인공지능), 부분 일치 1개))\n",
            "3. 국방 분야에서의 LLM 활용 문제점과 해결 전략 (유사도: 0.1890, 스케일링: 0.9450, 종합점수: 0.5835, 관련 (유사도 0.1890, 키워드 일치 1개 (인공지능)))\n",
            "4. 대학 교양 교육을 위한 AI·소프트웨어 교육 (유사도: 0.1292, 스케일링: 0.6462, 종합점수: 0.4939, 관련 (유사도 0.1292, 키워드 일치 1개 (인공지능)))\n",
            "5. 메타버스(Metaverse) 대중화 시대의 기독교윤리적 책임 (유사도: 0.0637, 스케일링: 0.3186, 종합점수: 0.3956, 관련 (유사도 0.0637, 키워드 일치 1개 (인공지능)))\n",
            "\n",
            "채용공고: 메타구조체 최적설계 알고리즘 개발\n",
            "상위 5개 추천 논문:\n",
            "1. 군사용 인공지능의 한미간 개발체계 비교 (유사도: 0.1664, 스케일링: 0.8319, 종합점수: 0.7496, 관련 (유사도 0.1664, 키워드 일치 1개 (인공지능), 부분 일치 1개))\n",
            "2. AI 시스템의 위험 완화를 위한 정책적 접근방안 연구: AI 영향평가를 중심으로 (유사도: 0.1099, 스케일링: 0.5496, 종합점수: 0.6649, 관련 (유사도 0.1099, 키워드 일치 1개 (인공지능), 부분 일치 1개))\n",
            "3. 국방 분야에서의 LLM 활용 문제점과 해결 전략 (유사도: 0.1930, 스케일링: 0.9651, 종합점수: 0.5895, 관련 (유사도 0.1930, 키워드 일치 1개 (인공지능)))\n",
            "4. 대학 교양 교육을 위한 AI·소프트웨어 교육 (유사도: 0.1072, 스케일링: 0.5360, 종합점수: 0.4608, 관련 (유사도 0.1072, 키워드 일치 1개 (인공지능)))\n",
            "5. 메타버스(Metaverse) 대중화 시대의 기독교윤리적 책임 (유사도: 0.0667, 스케일링: 0.3337, 종합점수: 0.4001, 관련 (유사도 0.0667, 키워드 일치 1개 (인공지능)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "채용공고 배치 평가: 100%|██████████| 5/5 [00:00<00:00, 19.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 평가 결과 ===\n",
            "모델 유형: Hi-BERT\n",
            "MRR: 0.9750\n",
            "NDCG@10: 0.9836\n",
            "Precision@5: 0.9800\n",
            "Precision@10: 0.9400\n",
            "=== 모델 검증 완료 ===\n",
            "\n",
            "=== Hi-BERT 학습 결과 ===\n",
            "테스트 세트 결과:\n",
            "MRR: 0.9750\n",
            "NDCG@10: 0.9633\n",
            "Precision@5: 0.9000\n",
            "Precision@10: 0.9250\n",
            "\n",
            "검증 세트 결과:\n",
            "MRR: 0.9750\n",
            "NDCG@10: 0.9836\n",
            "Precision@5: 0.9800\n",
            "Precision@10: 0.9400\n",
            "\n",
            "총 학습 소요 시간: 1080.45초 (18.01분)\n",
            "모든 모델이 /content/drive/MyDrive/Workspaces/Hibrainnet/Hi-BERT/models/hi_bert_model_kosimcse에 저장되었습니다.\n",
            "=== Hi-BERT 모델 학습 완료 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}