{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOnEdiRVoym/Zqxdt+qaXii",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sungkwangsong/EasyOCR/blob/master/notebooks/KorSciBERT_%ED%8C%A8%EC%B9%98%EC%99%80_%EC%82%AC%EC%9A%A9_%EC%98%88.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9I_f6vnQJnN",
        "outputId": "76c0c249-5a24-498d-a861-ef9ee50af7fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HBN_MODEL_DIR_PATH='/content/drive/MyDrive/Workspaces/Hibrainnet/models/KoSciBERT'\n",
        "HBN_DATASET_DIR_PATH='/content/drive/MyDrive/Workspaces/Hibrainnet/datasets/samples'"
      ],
      "metadata": {
        "id": "Vi9mA5JnrNV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/models/KoSciBERT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ic1W1s6r5Gl",
        "outputId": "e0d8b06d-3f2d-42ca-8ebb-2ed89d87dd2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/models/KoSciBERT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KoNLPy 및 의존성 설치\n",
        "!pip install konlpy\n",
        "\n",
        "# Java 설치\n",
        "!apt-get update -qq\n",
        "!apt-get install -y openjdk-8-jdk python-dev python3-dev\n",
        "\n",
        "# Mecab 설치\n",
        "!pip install mecab-python3\n",
        "!apt-get install -y curl git\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
        "\n",
        "# 설치 확인\n",
        "import konlpy\n",
        "from konlpy.tag import Mecab\n",
        "print(\"KoNLPy 버전:\", konlpy.__version__)\n",
        "print(\"Mecab 설치 완료\")"
      ],
      "metadata": {
        "id": "jtB7MogPx2rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 패키지 임포트\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "class KorSciBertSearch:\n",
        "    def __init__(self, model_dir, max_seq_length=128):\n",
        "        \"\"\"\n",
        "        KorSci BERT 토크나이저를 활용한 논문 검색 시스템 초기화\n",
        "\n",
        "        Args:\n",
        "            model_dir: KorSci BERT 모델이 저장된 디렉토리 경로\n",
        "            max_seq_length: 최대 시퀀스 길이\n",
        "        \"\"\"\n",
        "        self.model_dir = model_dir\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.papers = None\n",
        "        self.paper_embeddings = None\n",
        "\n",
        "        # 필요한 파일 경로 설정\n",
        "        self.vocab_file = os.path.join(model_dir, \"vocab_kisti.txt\")\n",
        "\n",
        "        # 모델 디렉토리를 시스템 경로에 추가 (파일 임포트를 위해)\n",
        "        if model_dir not in sys.path:\n",
        "            sys.path.append(model_dir)\n",
        "\n",
        "        # 토크나이저 초기화\n",
        "        self._init_tokenizer()\n",
        "\n",
        "    def _init_tokenizer(self):\n",
        "        \"\"\"KorSci 토크나이저 초기화\"\"\"\n",
        "        try:\n",
        "            # 토크나이저 모듈 직접 임포트\n",
        "            sys.path.append(self.model_dir)\n",
        "            import tokenization_kisti as tokenization\n",
        "\n",
        "            # 토크나이저 초기화 (반드시 Mecab 사용)\n",
        "            self.tokenizer = tokenization.FullTokenizer(\n",
        "                vocab_file=self.vocab_file,\n",
        "                do_lower_case=False,\n",
        "                tokenizer_type=\"Mecab\"\n",
        "            )\n",
        "            print(\"KISTI 토크나이저 초기화 완료 (Mecab)\")\n",
        "        except Exception as e:\n",
        "            # 첫 번째 오류 메시지 기록\n",
        "            first_error = f\"Mecab 토크나이저 초기화 오류: {e}\"\n",
        "            print(first_error)\n",
        "\n",
        "            # 기존 토크나이저 모듈을 재로드\n",
        "            if 'tokenization_kisti' in sys.modules:\n",
        "                del sys.modules['tokenization_kisti']\n",
        "\n",
        "            try:\n",
        "                # tokenization_kisti.py 파일을 명시적으로 찾아 임포트\n",
        "                tokenization_path = os.path.join(self.model_dir, 'tokenization_kisti.py')\n",
        "                if os.path.exists(tokenization_path):\n",
        "                    import importlib.util\n",
        "                    spec = importlib.util.spec_from_file_location(\"tokenization_kisti\", tokenization_path)\n",
        "                    tokenization = importlib.util.module_from_spec(spec)\n",
        "                    spec.loader.exec_module(tokenization)\n",
        "\n",
        "                    # WordPiece 토크나이저로 대체\n",
        "                    self.tokenizer = tokenization.FullTokenizer(\n",
        "                        vocab_file=self.vocab_file,\n",
        "                        do_lower_case=False,\n",
        "                        tokenizer_type=\"WordPiece\"\n",
        "                    )\n",
        "                    print(\"KISTI 토크나이저 초기화 완료 (WordPiece)\")\n",
        "                else:\n",
        "                    raise FileNotFoundError(f\"토크나이저 파일을 찾을 수 없습니다: {tokenization_path}\")\n",
        "            except Exception as e2:\n",
        "                # 두 번째 오류 메시지 기록 및 예외 발생\n",
        "                print(f\"WordPiece 토크나이저 초기화 오류: {e2}\")\n",
        "                print(f\"첫 번째 시도 오류: {first_error}\")\n",
        "                raise RuntimeError(\"KISTI 토크나이저 초기화에 실패했습니다.\")\n",
        "\n",
        "    def load_papers(self, papers_path):\n",
        "        \"\"\"\n",
        "        논문 데이터 로드\n",
        "\n",
        "        Args:\n",
        "            papers_path: 논문 데이터 파일 경로 (JSON 또는 CSV)\n",
        "        \"\"\"\n",
        "        file_ext = os.path.splitext(papers_path)[1].lower()\n",
        "\n",
        "        if file_ext == '.json':\n",
        "            with open(papers_path, 'r', encoding='utf-8') as f:\n",
        "                self.papers = json.load(f)\n",
        "        elif file_ext == '.csv':\n",
        "            self.papers = pd.read_csv(papers_path).to_dict('records')\n",
        "        else:\n",
        "            raise ValueError(f\"지원되지 않는 파일 형식입니다: {file_ext}\")\n",
        "\n",
        "        print(f\"총 {len(self.papers)}개의 논문 데이터를 로드했습니다.\")\n",
        "\n",
        "    def _prepare_tfidf_corpus(self):\n",
        "        \"\"\"토크나이저를 활용한 TF-IDF 코퍼스 준비\"\"\"\n",
        "        # 모든 논문 텍스트 토큰화\n",
        "        corpus = []\n",
        "        for paper in self.papers:\n",
        "            title = paper.get('title', '')\n",
        "            abstract = paper.get('abstract', '')\n",
        "            keywords = ' '.join(paper.get('keywords', []))\n",
        "            paper_text = title + \" \" + abstract + \" \" + keywords\n",
        "            paper_tokens = self.tokenizer.tokenize(paper_text)\n",
        "            corpus.append(\" \".join(paper_tokens))\n",
        "\n",
        "        # TF-IDF 벡터화\n",
        "        self.tfidf_vectorizer = TfidfVectorizer()\n",
        "        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "        return corpus\n",
        "\n",
        "    def create_paper_embeddings(self):\n",
        "        \"\"\"토크나이저와 TF-IDF를 활용한 논문 임베딩 생성\"\"\"\n",
        "        if self.papers is None:\n",
        "            raise ValueError(\"먼저 load_papers()를 통해 논문 데이터를 로드해야 합니다.\")\n",
        "\n",
        "        print(\"논문 임베딩 생성 중...\")\n",
        "\n",
        "        # TF-IDF 코퍼스 준비 및 벡터화\n",
        "        self._prepare_tfidf_corpus()\n",
        "\n",
        "        # TF-IDF 행렬을 numpy 배열로 변환\n",
        "        self.paper_embeddings = self.tfidf_matrix.toarray()\n",
        "\n",
        "        print(f\"임베딩 생성 완료. 형태: {self.paper_embeddings.shape}\")\n",
        "\n",
        "    def get_embedding(self, text):\n",
        "        \"\"\"\n",
        "        주어진 텍스트의 TF-IDF 임베딩을 추출\n",
        "\n",
        "        Args:\n",
        "            text: 임베딩을 추출할 텍스트\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: 임베딩 벡터\n",
        "        \"\"\"\n",
        "        # 토큰화\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        tokenized_text = \" \".join(tokens)\n",
        "\n",
        "        # TF-IDF 벡터화\n",
        "        vector = self.tfidf_vectorizer.transform([tokenized_text]).toarray()[0]\n",
        "\n",
        "        return vector\n",
        "\n",
        "    def search(self, query, top_k=10):\n",
        "        \"\"\"\n",
        "        쿼리와 가장 유사한 논문을 검색\n",
        "\n",
        "        Args:\n",
        "            query: 검색 쿼리 텍스트\n",
        "            top_k: 반환할 상위 결과 수\n",
        "\n",
        "        Returns:\n",
        "            list: 관련 논문 목록 (유사도 내림차순)\n",
        "        \"\"\"\n",
        "        if self.papers is None or self.paper_embeddings is None:\n",
        "            raise ValueError(\"검색하기 전에 먼저 논문 데이터를 로드하고 임베딩을 생성해야 합니다.\")\n",
        "\n",
        "        # 쿼리 임베딩 계산\n",
        "        query_embedding = self.get_embedding(query)\n",
        "\n",
        "        # 코사인 유사도 계산\n",
        "        similarities = cosine_similarity([query_embedding], self.paper_embeddings)[0]\n",
        "\n",
        "        # 유사도에 따라 정렬된 인덱스 얻기\n",
        "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "        # 결과 구성\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            results.append({\n",
        "                'paper': self.papers[idx],\n",
        "                'similarity': similarities[idx]\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"리소스 해제 (필요시)\"\"\"\n",
        "        pass"
      ],
      "metadata": {
        "id": "eSvP_9mK21Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def create_sample_papers():\n",
        "    \"\"\"샘플 논문 데이터 생성\"\"\"\n",
        "    return [\n",
        "        {\n",
        "            \"id\": \"paper1\",\n",
        "            \"title\": \"한국어 자연어처리를 위한 BERT 기반 임베딩 모델 연구\",\n",
        "            \"abstract\": \"본 연구에서는 한국어 자연어처리를 위한 BERT 기반 임베딩 모델을 제안한다. 제안된 모델은 대용량 한국어 코퍼스를 사용하여 사전학습되었으며, 다양한 자연어처리 태스크에서 우수한 성능을 보인다.\",\n",
        "            \"authors\": [\"김철수\", \"이영희\"],\n",
        "            \"year\": 2022,\n",
        "            \"keywords\": [\"자연어처리\", \"BERT\", \"임베딩\", \"한국어\"]\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"paper2\",\n",
        "            \"title\": \"과학기술 문헌 기반 지식 그래프 구축 및 활용 방안\",\n",
        "            \"abstract\": \"본 논문에서는 과학기술 문헌을 활용한 지식 그래프 구축 방법론을 제안한다. 구축된 지식 그래프는 연구 동향 분석, 연구자 네트워크 분석 등 다양한 분야에 활용될 수 있다.\",\n",
        "            \"authors\": [\"박지훈\", \"최민지\"],\n",
        "            \"year\": 2023,\n",
        "            \"keywords\": [\"지식 그래프\", \"과학기술 문헌\", \"정보 추출\", \"온톨로지\"]\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"paper3\",\n",
        "            \"title\": \"미세먼지 측정 센서 네트워크 구축 및 오염지도 개발\",\n",
        "            \"abstract\": \"본 연구에서는 도시 지역의 미세먼지 농도를 효과적으로 모니터링하기 위한 센서 네트워크 구축 방안과 실시간 오염지도 개발 방법을 제안한다. 제안된 시스템은 저비용 센서를 활용하여 고해상도 공간 데이터를 생성한다.\",\n",
        "            \"authors\": [\"임성호\", \"박성준\", \"김효진\"],\n",
        "            \"year\": 2023,\n",
        "            \"keywords\": [\"미세먼지\", \"센서네트워크\", \"오염지도\", \"사물인터넷\", \"환경모니터링\"]\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"paper4\",\n",
        "            \"title\": \"심층 신경망 기반 환자 건강 예측 모델\",\n",
        "            \"abstract\": \"본 연구는 심층 신경망을 활용하여 환자의 건강 상태를 예측하는 모델을 제안한다. 전자의무기록(EMR) 데이터를 활용하여 다양한 질병의 발생 위험을 예측하는 방법을 제시한다.\",\n",
        "            \"authors\": [\"정민수\", \"강다현\"],\n",
        "            \"year\": 2021,\n",
        "            \"keywords\": [\"심층 신경망\", \"헬스케어\", \"예측 모델\", \"전자의무기록\"]\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"paper5\",\n",
        "            \"title\": \"한국어 과학기술 문헌의 자동 분류 시스템 개발\",\n",
        "            \"abstract\": \"본 연구에서는 KorSci BERT를 활용한 한국어 과학기술 문헌 자동 분류 시스템을 개발하였다. 과학기술 분야의 특화된 어휘와 문맥을 반영하여 높은 분류 정확도를 달성하였으며, 분야별 논문 추천 시스템에 활용 가능하다.\",\n",
        "            \"authors\": [\"이지원\", \"박태민\"],\n",
        "            \"year\": 2022,\n",
        "            \"keywords\": [\"문서 분류\", \"과학기술 문헌\", \"BERT\", \"기계학습\", \"자연어처리\"]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "# 샘플 데이터 생성 및 저장\n",
        "sample_papers = create_sample_papers()\n",
        "sample_papers_path = f\"{HBN_DATASET_DIR_PATH}/sample_papers.json\"\n",
        "\n",
        "with open(sample_papers_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(sample_papers, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"샘플 논문 데이터가 {sample_papers_path}에 저장되었습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNycHSRW-eUU",
        "outputId": "921ac705-4481-40cc-f407-78b9d647d723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "샘플 논문 데이터가 /content/drive/MyDrive/Workspaces/Hibrainnet/datasets/samples/sample_papers.json에 저장되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_search_demo(papers_path=f\"{HBN_DATASET_DIR_PATH}/papers.json\"):\n",
        "    \"\"\"\n",
        "    검색 데모 실행\n",
        "\n",
        "    Args:\n",
        "        papers_path: 논문 데이터 파일 경로\n",
        "    \"\"\"\n",
        "    # KorSci BERT 모델 경로 설정\n",
        "    model_dir = HBN_MODEL_DIR_PATH\n",
        "\n",
        "    # 검색 엔진 초기화\n",
        "    search_engine = KorSciBertSearch(model_dir)\n",
        "\n",
        "    try:\n",
        "        # 논문 데이터 로드 및 임베딩 생성\n",
        "        search_engine.load_papers(papers_path)\n",
        "        search_engine.create_paper_embeddings()\n",
        "\n",
        "        # 검색 수행\n",
        "        test_queries = [\n",
        "            \"자연어처리 기술을 활용한 과학기술 문헌 분석\",\n",
        "            \"미세먼지 측정 및 모니터링 방법\",\n",
        "            \"딥러닝 기반 의료 데이터 분석\"\n",
        "        ]\n",
        "\n",
        "        for i, query in enumerate(test_queries, 1):\n",
        "            print(f\"\\n\\n===== 검색 쿼리 {i}: {query} =====\")\n",
        "            results = search_engine.search(query, top_k=5)\n",
        "\n",
        "            # 결과를 유사도 내림차순으로 정렬 (이미 정렬되어 있지만 명시적으로 재정렬)\n",
        "            results = sorted(results, key=lambda x: x['similarity'], reverse=True)\n",
        "\n",
        "            # 결과 출력\n",
        "            for j, result in enumerate(results, 1):\n",
        "                paper = result['paper']\n",
        "                print(f\"\\n[{j}] {paper['title']} (유사도: {result['similarity']:.4f})\")\n",
        "                print(f\"저자: {', '.join(paper['authors'])}\")\n",
        "                print(f\"연도: {paper['year']}\")\n",
        "                print(f\"키워드: {', '.join(paper['keywords'])}\")\n",
        "                abstract = paper['abstract']\n",
        "                print(f\"초록: {abstract[:150]}...\" if len(abstract) > 150 else f\"초록: {abstract}\")\n",
        "\n",
        "    finally:\n",
        "        search_engine.close()\n",
        "\n",
        "# 데모 실행 (저장된 샘플 데이터 파일 사용)\n",
        "run_search_demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHI_jFyd3cdO",
        "outputId": "23c54797-3577-4291-c113-5c00a44058ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KISTI 토크나이저 초기화 완료 (Mecab)\n",
            "총 5개의 논문 데이터를 로드했습니다.\n",
            "논문 임베딩 생성 중...\n",
            "임베딩 생성 완료. 형태: (5, 86)\n",
            "\n",
            "\n",
            "===== 검색 쿼리 1: 자연어처리 기술을 활용한 과학기술 문헌 분석 =====\n",
            "\n",
            "[1] 한국어 과학기술 문헌의 자동 분류 시스템 개발 (유사도: 0.5207)\n",
            "저자: 이지원, 박태민\n",
            "연도: 2022\n",
            "키워드: 문서 분류, 과학기술 문헌, BERT, 기계학습, 자연어처리\n",
            "초록: 본 연구에서는 KorSci BERT를 활용한 한국어 과학기술 문헌 자동 분류 시스템을 개발하였다. 과학기술 분야의 특화된 어휘와 문맥을 반영하여 높은 분류 정확도를 달성하였으며, 분야별 논문 추천 시스템에 활용 가능하다.\n",
            "\n",
            "[2] 과학기술 문헌 기반 지식 그래프 구축 및 활용 방안 (유사도: 0.5009)\n",
            "저자: 박지훈, 최민지\n",
            "연도: 2023\n",
            "키워드: 지식 그래프, 과학기술 문헌, 정보 추출, 온톨로지\n",
            "초록: 본 논문에서는 과학기술 문헌을 활용한 지식 그래프 구축 방법론을 제안한다. 구축된 지식 그래프는 연구 동향 분석, 연구자 네트워크 분석 등 다양한 분야에 활용될 수 있다.\n",
            "\n",
            "[3] 한국어 자연어처리를 위한 BERT 기반 임베딩 모델 연구 (유사도: 0.2468)\n",
            "저자: 김철수, 이영희\n",
            "연도: 2022\n",
            "키워드: 자연어처리, BERT, 임베딩, 한국어\n",
            "초록: 본 연구에서는 한국어 자연어처리를 위한 BERT 기반 임베딩 모델을 제안한다. 제안된 모델은 대용량 한국어 코퍼스를 사용하여 사전학습되었으며, 다양한 자연어처리 태스크에서 우수한 성능을 보인다.\n",
            "\n",
            "[4] 심층 신경망 기반 환자 건강 예측 모델 (유사도: 0.0304)\n",
            "저자: 정민수, 강다현\n",
            "연도: 2021\n",
            "키워드: 심층 신경망, 헬스케어, 예측 모델, 전자의무기록\n",
            "초록: 본 연구는 심층 신경망을 활용하여 환자의 건강 상태를 예측하는 모델을 제안한다. 전자의무기록(EMR) 데이터를 활용하여 다양한 질병의 발생 위험을 예측하는 방법을 제시한다.\n",
            "\n",
            "[5] 미세먼지 측정 센서 네트워크 구축 및 오염지도 개발 (유사도: 0.0141)\n",
            "저자: 임성호, 박성준, 김효진\n",
            "연도: 2023\n",
            "키워드: 미세먼지, 센서네트워크, 오염지도, 사물인터넷, 환경모니터링\n",
            "초록: 본 연구에서는 도시 지역의 미세먼지 농도를 효과적으로 모니터링하기 위한 센서 네트워크 구축 방안과 실시간 오염지도 개발 방법을 제안한다. 제안된 시스템은 저비용 센서를 활용하여 고해상도 공간 데이터를 생성한다.\n",
            "\n",
            "\n",
            "===== 검색 쿼리 2: 미세먼지 측정 및 모니터링 방법 =====\n",
            "\n",
            "[1] 미세먼지 측정 센서 네트워크 구축 및 오염지도 개발 (유사도: 0.5077)\n",
            "저자: 임성호, 박성준, 김효진\n",
            "연도: 2023\n",
            "키워드: 미세먼지, 센서네트워크, 오염지도, 사물인터넷, 환경모니터링\n",
            "초록: 본 연구에서는 도시 지역의 미세먼지 농도를 효과적으로 모니터링하기 위한 센서 네트워크 구축 방안과 실시간 오염지도 개발 방법을 제안한다. 제안된 시스템은 저비용 센서를 활용하여 고해상도 공간 데이터를 생성한다.\n",
            "\n",
            "[2] 심층 신경망 기반 환자 건강 예측 모델 (유사도: 0.0260)\n",
            "저자: 정민수, 강다현\n",
            "연도: 2021\n",
            "키워드: 심층 신경망, 헬스케어, 예측 모델, 전자의무기록\n",
            "초록: 본 연구는 심층 신경망을 활용하여 환자의 건강 상태를 예측하는 모델을 제안한다. 전자의무기록(EMR) 데이터를 활용하여 다양한 질병의 발생 위험을 예측하는 방법을 제시한다.\n",
            "\n",
            "[3] 과학기술 문헌 기반 지식 그래프 구축 및 활용 방안 (유사도: 0.0253)\n",
            "저자: 박지훈, 최민지\n",
            "연도: 2023\n",
            "키워드: 지식 그래프, 과학기술 문헌, 정보 추출, 온톨로지\n",
            "초록: 본 논문에서는 과학기술 문헌을 활용한 지식 그래프 구축 방법론을 제안한다. 구축된 지식 그래프는 연구 동향 분석, 연구자 네트워크 분석 등 다양한 분야에 활용될 수 있다.\n",
            "\n",
            "[4] 한국어 과학기술 문헌의 자동 분류 시스템 개발 (유사도: 0.0000)\n",
            "저자: 이지원, 박태민\n",
            "연도: 2022\n",
            "키워드: 문서 분류, 과학기술 문헌, BERT, 기계학습, 자연어처리\n",
            "초록: 본 연구에서는 KorSci BERT를 활용한 한국어 과학기술 문헌 자동 분류 시스템을 개발하였다. 과학기술 분야의 특화된 어휘와 문맥을 반영하여 높은 분류 정확도를 달성하였으며, 분야별 논문 추천 시스템에 활용 가능하다.\n",
            "\n",
            "[5] 한국어 자연어처리를 위한 BERT 기반 임베딩 모델 연구 (유사도: 0.0000)\n",
            "저자: 김철수, 이영희\n",
            "연도: 2022\n",
            "키워드: 자연어처리, BERT, 임베딩, 한국어\n",
            "초록: 본 연구에서는 한국어 자연어처리를 위한 BERT 기반 임베딩 모델을 제안한다. 제안된 모델은 대용량 한국어 코퍼스를 사용하여 사전학습되었으며, 다양한 자연어처리 태스크에서 우수한 성능을 보인다.\n",
            "\n",
            "\n",
            "===== 검색 쿼리 3: 딥러닝 기반 의료 데이터 분석 =====\n",
            "\n",
            "[1] 과학기술 문헌 기반 지식 그래프 구축 및 활용 방안 (유사도: 0.2009)\n",
            "저자: 박지훈, 최민지\n",
            "연도: 2023\n",
            "키워드: 지식 그래프, 과학기술 문헌, 정보 추출, 온톨로지\n",
            "초록: 본 논문에서는 과학기술 문헌을 활용한 지식 그래프 구축 방법론을 제안한다. 구축된 지식 그래프는 연구 동향 분석, 연구자 네트워크 분석 등 다양한 분야에 활용될 수 있다.\n",
            "\n",
            "[2] 심층 신경망 기반 환자 건강 예측 모델 (유사도: 0.0927)\n",
            "저자: 정민수, 강다현\n",
            "연도: 2021\n",
            "키워드: 심층 신경망, 헬스케어, 예측 모델, 전자의무기록\n",
            "초록: 본 연구는 심층 신경망을 활용하여 환자의 건강 상태를 예측하는 모델을 제안한다. 전자의무기록(EMR) 데이터를 활용하여 다양한 질병의 발생 위험을 예측하는 방법을 제시한다.\n",
            "\n",
            "[3] 한국어 자연어처리를 위한 BERT 기반 임베딩 모델 연구 (유사도: 0.0750)\n",
            "저자: 김철수, 이영희\n",
            "연도: 2022\n",
            "키워드: 자연어처리, BERT, 임베딩, 한국어\n",
            "초록: 본 연구에서는 한국어 자연어처리를 위한 BERT 기반 임베딩 모델을 제안한다. 제안된 모델은 대용량 한국어 코퍼스를 사용하여 사전학습되었으며, 다양한 자연어처리 태스크에서 우수한 성능을 보인다.\n",
            "\n",
            "[4] 미세먼지 측정 센서 네트워크 구축 및 오염지도 개발 (유사도: 0.0509)\n",
            "저자: 임성호, 박성준, 김효진\n",
            "연도: 2023\n",
            "키워드: 미세먼지, 센서네트워크, 오염지도, 사물인터넷, 환경모니터링\n",
            "초록: 본 연구에서는 도시 지역의 미세먼지 농도를 효과적으로 모니터링하기 위한 센서 네트워크 구축 방안과 실시간 오염지도 개발 방법을 제안한다. 제안된 시스템은 저비용 센서를 활용하여 고해상도 공간 데이터를 생성한다.\n",
            "\n",
            "[5] 한국어 과학기술 문헌의 자동 분류 시스템 개발 (유사도: 0.0000)\n",
            "저자: 이지원, 박태민\n",
            "연도: 2022\n",
            "키워드: 문서 분류, 과학기술 문헌, BERT, 기계학습, 자연어처리\n",
            "초록: 본 연구에서는 KorSci BERT를 활용한 한국어 과학기술 문헌 자동 분류 시스템을 개발하였다. 과학기술 분야의 특화된 어휘와 문맥을 반영하여 높은 분류 정확도를 달성하였으며, 분야별 논문 추천 시스템에 활용 가능하다.\n"
          ]
        }
      ]
    }
  ]
}