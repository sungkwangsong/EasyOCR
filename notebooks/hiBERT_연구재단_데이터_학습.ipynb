{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOzYCcY1mLlUt4tmpwxBgXF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sungkwangsong/EasyOCR/blob/master/notebooks/hiBERT_%EC%97%B0%EA%B5%AC%EC%9E%AC%EB%8B%A8_%EB%8D%B0%EC%9D%B4%ED%84%B0_%ED%95%99%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UjJMjU8kRsiL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b278001e-5784-48de-fa31-399c82081686"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HBN_DATASETS_DIR_PATH = '/content/drive/MyDrive/Workspaces/Hibrainnet/datasets/hibrainnet'\n",
        "HBN_MODELS_DIR_PATH = '/content/drive/MyDrive/Workspaces/Hibrainnet/models/hibrainnet'\n",
        "HBN_EMBEDDINGS_DIR_PATH = '/content/drive/MyDrive/Workspaces/Hibrainnet/embeddings/hibrainnet'\n",
        "HBN_ARTICLES_DIR_PATH = '/content/drive/MyDrive/Workspaces/Hibrainnet/hbn-data-lake/papers/kci/data'"
      ],
      "metadata": {
        "id": "K7cIgJXz7-wk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel, AdamW\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 시드 설정\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# GPU 사용 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# KoSimCSE-BERT 모델 및 토크나이저 로드\n",
        "model_name = \"BM-K/KoSimCSE-bert\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "model.to(device)\n",
        "\n",
        "# 한국연구재단 분류 데이터 로드\n",
        "def load_nrf_data(file_path, encoding='utf-8'):\n",
        "    df = pd.read_csv(file_path, encoding=encoding)\n",
        "    print(f\"데이터 로드 완료. 형태: {df.shape}\")\n",
        "    print(f\"컬럼: {df.columns.tolist()}\")\n",
        "    return df\n",
        "\n",
        "# 데이터셋 클래스 정의 (SimCSE 방식)\n",
        "class NRFDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 각 텍스트를 두 번 토큰화 (dropout을 통한 데이터 증강)\n",
        "        text = self.texts[idx]\n",
        "\n",
        "        # 첫 번째 인코딩\n",
        "        inputs1 = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # 두 번째 인코딩 (동일 텍스트, 다른 dropout 마스크)\n",
        "        inputs2 = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids1': inputs1['input_ids'].squeeze(),\n",
        "            'attention_mask1': inputs1['attention_mask'].squeeze(),\n",
        "            'input_ids2': inputs2['input_ids'].squeeze(),\n",
        "            'attention_mask2': inputs2['attention_mask'].squeeze(),\n",
        "        }\n",
        "\n",
        "# SimCSE 손실 함수 정의\n",
        "class SimCSELoss(torch.nn.Module):\n",
        "    def __init__(self, temperature=0.05):\n",
        "        super(SimCSELoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.cos_sim = torch.nn.CosineSimilarity(dim=-1)\n",
        "        self.loss_fct = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, embeddings_a, embeddings_b):\n",
        "        batch_size = embeddings_a.size(0)\n",
        "\n",
        "        # 코사인 유사도 계산\n",
        "        cos_sim_matrix = self.cos_sim(embeddings_a.unsqueeze(1), embeddings_b.unsqueeze(0)) / self.temperature\n",
        "\n",
        "        # 레이블: 대각선 요소(자기 자신)가 양성 쌍\n",
        "        labels = torch.arange(batch_size, device=cos_sim_matrix.device)\n",
        "\n",
        "        return self.loss_fct(cos_sim_matrix, labels)\n",
        "\n",
        "# 학습 함수\n",
        "def train(model, data_loader, optimizer, loss_fn, device, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    valid_batches = 0\n",
        "\n",
        "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}\")\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        try:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 배치 데이터를 GPU로 이동\n",
        "            input_ids1 = batch['input_ids1'].to(device)\n",
        "            attention_mask1 = batch['attention_mask1'].to(device)\n",
        "            input_ids2 = batch['input_ids2'].to(device)\n",
        "            attention_mask2 = batch['attention_mask2'].to(device)\n",
        "\n",
        "            # 배치가 비어있는지 확인\n",
        "            if input_ids1.size(0) == 0 or input_ids2.size(0) == 0:\n",
        "                continue\n",
        "\n",
        "            # 첫 번째 문장 임베딩\n",
        "            outputs1 = model(input_ids=input_ids1, attention_mask=attention_mask1)\n",
        "            embeddings1 = outputs1.last_hidden_state[:, 0]  # [CLS] 토큰 임베딩\n",
        "\n",
        "            # 두 번째 문장 임베딩\n",
        "            outputs2 = model(input_ids=input_ids2, attention_mask=attention_mask2)\n",
        "            embeddings2 = outputs2.last_hidden_state[:, 0]  # [CLS] 토큰 임베딩\n",
        "\n",
        "            # 손실 계산\n",
        "            loss = loss_fn(embeddings1, embeddings2)\n",
        "\n",
        "            # 역전파\n",
        "            loss.backward()\n",
        "\n",
        "            # 그래디언트 클리핑 (폭발 방지)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            valid_batches += 1\n",
        "            progress_bar.set_postfix({'loss': total_loss / (valid_batches if valid_batches > 0 else 1)})\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"배치 처리 중 오류 발생: {e}\")\n",
        "            continue\n",
        "\n",
        "    return total_loss / (valid_batches if valid_batches > 0 else 1)\n",
        "\n",
        "# 평가 함수\n",
        "def evaluate(model, data_loader, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            # 배치 데이터를 GPU로 이동\n",
        "            input_ids1 = batch['input_ids1'].to(device)\n",
        "            attention_mask1 = batch['attention_mask1'].to(device)\n",
        "            input_ids2 = batch['input_ids2'].to(device)\n",
        "            attention_mask2 = batch['attention_mask2'].to(device)\n",
        "\n",
        "            # 첫 번째 문장 임베딩\n",
        "            outputs1 = model(input_ids=input_ids1, attention_mask=attention_mask1)\n",
        "            embeddings1 = outputs1.last_hidden_state[:, 0]\n",
        "\n",
        "            # 두 번째 문장 임베딩\n",
        "            outputs2 = model(input_ids=input_ids2, attention_mask=attention_mask2)\n",
        "            embeddings2 = outputs2.last_hidden_state[:, 0]\n",
        "\n",
        "            # 손실 계산\n",
        "            loss = loss_fn(embeddings1, embeddings2)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# 메인 함수\n",
        "def main():\n",
        "    # 파일 경로 설정\n",
        "    file_path = f\"{HBN_DATASETS_DIR_PATH}/한국연구재단_전공분류 - 학습용(중분류포함).csv\"\n",
        "\n",
        "    # 데이터 로드\n",
        "    df = load_nrf_data(file_path, encoding='utf-8')\n",
        "\n",
        "    # 컬럼명 확인 및 실제 컬럼명으로 접근\n",
        "    print(\"정확한 컬럼명:\")\n",
        "    for col in df.columns:\n",
        "        print(f\"- {col}\")\n",
        "\n",
        "    # 학습 데이터 준비\n",
        "    # 키워드와 설명을 결합하여 텍스트 생성\n",
        "    texts = []\n",
        "\n",
        "    # 컬럼명 추출 - 실제 컬럼명과 일치하는지 확인\n",
        "    keyword_col = [col for col in df.columns if 'KEYWORDS' in col][0]\n",
        "    description_col = [col for col in df.columns if 'DESCRIPTION' in col][0]\n",
        "\n",
        "    print(f\"사용할 키워드 컬럼: {keyword_col}\")\n",
        "    print(f\"사용할 설명 컬럼: {description_col}\")\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        keywords = str(row[keyword_col])\n",
        "        description = str(row[description_col])\n",
        "\n",
        "        # 처음 5개 행만 출력하여 데이터 확인\n",
        "        if _ < 5:\n",
        "            print(f\"행 {_+1} 샘플:\")\n",
        "            print(f\"  키워드: {keywords[:100]}...\")\n",
        "            print(f\"  설명: {description[:100]}...\")\n",
        "\n",
        "        # 키워드와 설명이 모두 있는 경우만 학습 데이터에 추가\n",
        "        if keywords != 'nan' and description != 'nan' and keywords.strip() and description.strip():\n",
        "            # 텍스트 길이가 너무 짧지 않은 경우만 추가 (최소 10자)\n",
        "            if len(keywords) > 10:\n",
        "                texts.append(keywords)\n",
        "            if len(description) > 10:\n",
        "                texts.append(description)\n",
        "            # 키워드와 설명을 결합한 텍스트도 추가\n",
        "            combined = f\"{keywords} {description}\"\n",
        "            if len(combined) > 20:  # 결합 텍스트는 더 길어야 함\n",
        "                texts.append(combined)\n",
        "\n",
        "    print(f\"전처리된 텍스트 수: {len(texts)}\")\n",
        "\n",
        "    # 텍스트 데이터가 너무 적은 경우 경고\n",
        "    if len(texts) < 100:\n",
        "        print(f\"경고: 학습 데이터가 매우 적습니다 ({len(texts)}개). 결과가 제한적일 수 있습니다.\")\n",
        "\n",
        "        # 데이터 증강: 텍스트 조각화를 통한 추가 데이터 생성\n",
        "        augmented_texts = []\n",
        "        for text in texts:\n",
        "            if len(text) > 50:\n",
        "                # 긴 텍스트는 여러 조각으로 나누어 추가 데이터 생성\n",
        "                chunks = [text[i:i+50] for i in range(0, len(text), 25)]  # 50자씩, 25자 겹침\n",
        "                augmented_texts.extend(chunks)\n",
        "\n",
        "        texts.extend(augmented_texts)\n",
        "        print(f\"데이터 증강 후 텍스트 수: {len(texts)}\")\n",
        "\n",
        "    # 학습/검증 데이터 분할\n",
        "    train_texts, val_texts = train_test_split(texts, test_size=0.1, random_state=42)\n",
        "\n",
        "    print(f\"학습 데이터 크기: {len(train_texts)}\")\n",
        "    print(f\"검증 데이터 크기: {len(val_texts)}\")\n",
        "\n",
        "    # 데이터셋 및 데이터로더 생성\n",
        "    train_dataset = NRFDataset(train_texts, tokenizer)\n",
        "    val_dataset = NRFDataset(val_texts, tokenizer)\n",
        "\n",
        "    # 배치 크기 조정 (데이터셋 크기에 따라)\n",
        "    batch_size = min(16, max(4, len(train_texts) // 10))  # 데이터셋의 1/10, 최소 4, 최대 16\n",
        "    print(f\"배치 크기: {batch_size}\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # 손실 함수 및 옵티마이저 설정\n",
        "    loss_fn = SimCSELoss(temperature=0.05)\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "    # 학습 실행\n",
        "    num_epochs = 5\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # 학습\n",
        "        train_loss = train(model, train_loader, optimizer, loss_fn, device, epoch)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "        # 평가\n",
        "        val_loss = evaluate(model, val_loader, loss_fn, device)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # 최고 성능 모델 저장\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            print(f\"Saving the best model with validation loss: {val_loss:.4f}\")\n",
        "            model_save_path = f\"{HBN_MODELS_DIR_PATH}/hiBERT\"\n",
        "            model.save_pretrained(model_save_path)\n",
        "            tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "    print(\"학습 완료!\")\n",
        "\n",
        "    # 간단한 테스트\n",
        "    test_sentence = \"인공지능 딥러닝 연구\"\n",
        "    print(f\"테스트 문장: '{test_sentence}'\")\n",
        "\n",
        "    # 파인튜닝된 모델로 임베딩 생성\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(test_sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        embedding = outputs.last_hidden_state[:, 0].cpu().numpy()\n",
        "\n",
        "        print(f\"임베딩 형태: {embedding.shape}\")\n",
        "        print(\"임베딩 벡터 (처음 5개 값):\", embedding[0][:5])\n",
        "\n",
        "# 스크립트 실행\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "NlwK-BnO7tXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "\n",
        "class PaperSearchSystem:\n",
        "    def __init__(self, model_path, device=None):\n",
        "        \"\"\"\n",
        "        논문 검색 시스템 초기화\n",
        "\n",
        "        Args:\n",
        "            model_path: 파인튜닝된 hiBERT 모델 경로\n",
        "            device: 연산 장치 (None이면 자동 감지)\n",
        "        \"\"\"\n",
        "        if device is None:\n",
        "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        else:\n",
        "            self.device = device\n",
        "\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # 모델 및 토크나이저 로드\n",
        "        try:\n",
        "            # 파인튜닝된 모델 로드\n",
        "            self.tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "            self.model = BertModel.from_pretrained(model_path)\n",
        "            print(f\"파인튜닝된 hiBERT 모델을 성공적으로 로드했습니다: {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"모델 로드 중 오류 발생: {e}\")\n",
        "            print(f\"기본 KoSimCSE-BERT 모델을 대신 로드합니다.\")\n",
        "            self.tokenizer = BertTokenizer.from_pretrained(\"BM-K/KoSimCSE-bert\")\n",
        "            self.model = BertModel.from_pretrained(\"BM-K/KoSimCSE-bert\")\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        # 논문 데이터 및 임베딩 저장소\n",
        "        self.papers = []\n",
        "        self.paper_embeddings = None\n",
        "\n",
        "    def load_papers_from_dir(self, dir_path):\n",
        "        \"\"\"\n",
        "        디렉토리에서 JSON 논문 파일들을 로드\n",
        "\n",
        "        Args:\n",
        "            dir_path: 논문 JSON 파일이 있는 디렉토리 경로\n",
        "        \"\"\"\n",
        "        json_files = glob.glob(os.path.join(dir_path, \"*.json\"))\n",
        "        print(f\"Found {len(json_files)} JSON files in directory: {dir_path}\")\n",
        "\n",
        "        for file_path in tqdm(json_files, desc=\"Loading papers\"):\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    paper_data = json.load(f)\n",
        "\n",
        "                # 기본 값 설정 (파일 구조에 따라 다를 수 있음)\n",
        "                paper_info = {\n",
        "                    'title': '',\n",
        "                    'abstract': '',\n",
        "                    'keywords': [],\n",
        "                    'authors': [],\n",
        "                    'category': '',\n",
        "                    'file_path': file_path\n",
        "                }\n",
        "\n",
        "                # 제목 추출\n",
        "                article_info = paper_data.get('articleInfo', {})\n",
        "                title_group = article_info.get('title-group', {})\n",
        "\n",
        "                if isinstance(title_group.get('article-title'), list):\n",
        "                    for title_item in title_group.get('article-title', []):\n",
        "                        if title_item.get('@lang') == 'original':\n",
        "                            paper_info['title'] = title_item.get('#text', '')\n",
        "                            break\n",
        "                elif isinstance(title_group.get('article-title'), dict):\n",
        "                    paper_info['title'] = title_group.get('article-title', {}).get('#text', '')\n",
        "\n",
        "                # 초록 추출\n",
        "                abstract_group = article_info.get('abstract-group', {})\n",
        "                if isinstance(abstract_group.get('abstract'), list):\n",
        "                    for abstract_item in abstract_group.get('abstract', []):\n",
        "                        if abstract_item.get('@lang') == 'original':\n",
        "                            paper_info['abstract'] = abstract_item.get('#text', '')\n",
        "                            break\n",
        "                elif isinstance(abstract_group.get('abstract'), dict):\n",
        "                    paper_info['abstract'] = abstract_group.get('abstract', {}).get('#text', '')\n",
        "\n",
        "                # 키워드 추출\n",
        "                detail_info = paper_data.get('detailInfo', {})\n",
        "                detail_article_info = detail_info.get('articleInfo', {})\n",
        "                keyword_group = detail_article_info.get('keyword-group', {})\n",
        "\n",
        "                if isinstance(keyword_group.get('keyword'), list):\n",
        "                    paper_info['keywords'] = keyword_group.get('keyword', [])\n",
        "                elif isinstance(keyword_group.get('keyword'), str):\n",
        "                    paper_info['keywords'] = [keyword_group.get('keyword', '')]\n",
        "\n",
        "                # 저자 추출\n",
        "                author_group = article_info.get('author-group', {})\n",
        "                if isinstance(author_group.get('author'), list):\n",
        "                    for author in author_group.get('author', []):\n",
        "                        if isinstance(author, str):\n",
        "                            paper_info['authors'].append(author)\n",
        "                        elif isinstance(author, dict):\n",
        "                            author_name = author.get('#text', '')\n",
        "                            if author_name:\n",
        "                                paper_info['authors'].append(author_name)\n",
        "                elif isinstance(author_group.get('author'), dict):\n",
        "                    author_name = author_group.get('author', {}).get('#text', '')\n",
        "                    if author_name:\n",
        "                        paper_info['authors'].append(author_name)\n",
        "\n",
        "                # 카테고리 추출\n",
        "                paper_info['category'] = article_info.get('article-categories', '')\n",
        "\n",
        "                # 논문 정보가 제대로 존재하는 경우만 추가\n",
        "                if paper_info['title'] or paper_info['abstract']:\n",
        "                    self.papers.append(paper_info)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading paper from {file_path}: {e}\")\n",
        "\n",
        "        print(f\"Successfully loaded {len(self.papers)} papers.\")\n",
        "\n",
        "    def create_paper_embeddings(self, batch_size=4):\n",
        "        \"\"\"\n",
        "        논문의 임베딩 벡터 생성\n",
        "\n",
        "        Args:\n",
        "            batch_size: 배치 크기\n",
        "        \"\"\"\n",
        "        if not self.papers:\n",
        "            raise ValueError(\"Please load papers first using load_papers_from_dir()\")\n",
        "\n",
        "        # 제목, 초록, 키워드 결합\n",
        "        texts = []\n",
        "        for paper in self.papers:\n",
        "            title = paper.get('title', '')\n",
        "            abstract = paper.get('abstract', '')\n",
        "            keywords = ' '.join(paper.get('keywords', []))\n",
        "\n",
        "            # 텍스트 결합 (제목이 중요하므로 2번 포함)\n",
        "            text = f\"{title} {title} {abstract} {keywords}\"\n",
        "            texts.append(text)\n",
        "\n",
        "        # 배치 처리로 임베딩 생성\n",
        "        embeddings = []\n",
        "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Creating paper embeddings\"):\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "\n",
        "            # 토큰화 및 텐서 생성\n",
        "            inputs = self.tokenizer(\n",
        "                batch_texts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            )\n",
        "\n",
        "            # GPU로 이동\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "            # 임베딩 생성\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                batch_embeddings = outputs.last_hidden_state[:, 0].cpu().numpy()\n",
        "                embeddings.extend(batch_embeddings)\n",
        "\n",
        "        self.paper_embeddings = np.array(embeddings)\n",
        "        print(f\"Created embeddings with shape: {self.paper_embeddings.shape}\")\n",
        "\n",
        "    def save_embeddings(self, output_file):\n",
        "        \"\"\"\n",
        "        생성된 임베딩 저장\n",
        "\n",
        "        Args:\n",
        "            output_file: 저장할 파일 경로\n",
        "        \"\"\"\n",
        "        if self.paper_embeddings is None:\n",
        "            raise ValueError(\"No embeddings to save. Create embeddings first.\")\n",
        "\n",
        "        # 논문 정보와 임베딩을 함께 저장\n",
        "        data_to_save = {\n",
        "            'papers': self.papers,\n",
        "            'embeddings': self.paper_embeddings.tolist()\n",
        "        }\n",
        "\n",
        "        # 디렉토리 생성\n",
        "        os.makedirs(os.path.dirname(os.path.abspath(output_file)), exist_ok=True)\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data_to_save, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"Saved papers and embeddings to {output_file}\")\n",
        "\n",
        "    def load_embeddings(self, embeddings_file):\n",
        "        \"\"\"\n",
        "        저장된 임베딩과 논문 정보 로드\n",
        "\n",
        "        Args:\n",
        "            embeddings_file: 임베딩 파일 경로\n",
        "        \"\"\"\n",
        "        with open(embeddings_file, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        self.papers = data.get('papers', [])\n",
        "        self.paper_embeddings = np.array(data.get('embeddings', []))\n",
        "\n",
        "        print(f\"Loaded {len(self.papers)} papers and embeddings with shape: {self.paper_embeddings.shape}\")\n",
        "\n",
        "    def search(self, query, top_k=5, min_similarity=0.3):\n",
        "        \"\"\"\n",
        "        쿼리와 의미적으로 유사한 논문 검색\n",
        "\n",
        "        Args:\n",
        "            query: 검색 쿼리\n",
        "            top_k: 반환할 상위 결과 수\n",
        "            min_similarity: 최소 유사도 점수\n",
        "\n",
        "        Returns:\n",
        "            검색 결과 (논문 정보와 유사도 점수)\n",
        "        \"\"\"\n",
        "        if not self.papers or self.paper_embeddings is None:\n",
        "            raise ValueError(\"Papers and embeddings must be loaded before searching\")\n",
        "\n",
        "        # 쿼리 임베딩 생성\n",
        "        inputs = self.tokenizer(\n",
        "            query,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            query_embedding = outputs.last_hidden_state[:, 0].cpu().numpy()\n",
        "\n",
        "        # 코사인 유사도 계산\n",
        "        similarities = cosine_similarity(query_embedding, self.paper_embeddings)[0]\n",
        "\n",
        "        # 유사도 점수에 따라 정렬된 인덱스 가져오기\n",
        "        sorted_indices = similarities.argsort()[::-1]\n",
        "\n",
        "        # 결과 생성\n",
        "        results = []\n",
        "        for idx in sorted_indices:\n",
        "            score = similarities[idx]\n",
        "            if score < min_similarity or len(results) >= top_k:\n",
        "                break\n",
        "\n",
        "            paper_info = self.papers[idx].copy()\n",
        "            paper_info['similarity_score'] = float(score)\n",
        "            results.append(paper_info)\n",
        "\n",
        "        return results\n",
        "\n",
        "def display_search_results(results):\n",
        "    \"\"\"\n",
        "    검색 결과를 화면에 출력\n",
        "\n",
        "    Args:\n",
        "        results: 검색 결과 리스트\n",
        "    \"\"\"\n",
        "    if not results:\n",
        "        print(\"검색 결과가 없습니다.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n검색 결과 ({len(results)}개):\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    for i, paper in enumerate(results):\n",
        "        print(f\"{i+1}. [{paper['similarity_score']:.4f}] {paper['title']}\")\n",
        "        if paper.get('authors'):\n",
        "            print(f\"   저자: {', '.join(paper['authors'])}\")\n",
        "        if paper.get('category'):\n",
        "            print(f\"   분야: {paper['category']}\")\n",
        "        if paper.get('keywords'):\n",
        "            print(f\"   키워드: {', '.join(paper['keywords'])}\")\n",
        "        if paper.get('abstract'):\n",
        "            # 초록 앞부분만 표시\n",
        "            abstract = paper['abstract']\n",
        "            if len(abstract) > 200:\n",
        "                abstract = abstract[:200] + \"...\"\n",
        "            print(f\"   초록: {abstract}\")\n",
        "        print(f\"   파일: {os.path.basename(paper['file_path'])}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "def main():\n",
        "    # 설정 변수들\n",
        "    model_path = f\"{HBN_MODELS_DIR_PATH}/hiBERT\"  # 모델 경로\n",
        "    papers_dir = HBN_ARTICLES_DIR_PATH       # 논문 디렉토리 경로\n",
        "    embeddings_file = f\"{HBN_EMBEDDINGS_DIR_PATH}/articles_embeddings.json\"  # 임베딩 파일 경로\n",
        "    create_embeddings = False                  # 임베딩 새로 생성 여부\n",
        "    top_k = 5                                  # 검색 결과 개수\n",
        "\n",
        "    # 테스트용 검색 쿼리들\n",
        "    test_queries = [\n",
        "        \"온톨로지 기반 검색\",\n",
        "        \"인공지능 알고리즘\",\n",
        "        \"데이터 마이닝\",\n",
        "        \"추론 시스템\",\n",
        "        \"지식 그래프\"\n",
        "    ]\n",
        "\n",
        "    # 검색 시스템 초기화\n",
        "    search_system = PaperSearchSystem(model_path)\n",
        "\n",
        "    # 임베딩 생성 또는 로드\n",
        "    if create_embeddings:\n",
        "        # 논문 데이터 로드\n",
        "        search_system.load_papers_from_dir(papers_dir)\n",
        "        # 임베딩 생성\n",
        "        search_system.create_paper_embeddings()\n",
        "        # 임베딩 저장\n",
        "        search_system.save_embeddings(embeddings_file)\n",
        "    elif os.path.exists(embeddings_file):\n",
        "        # 저장된 임베딩 로드\n",
        "        search_system.load_embeddings(embeddings_file)\n",
        "    else:\n",
        "        # 논문 데이터 로드\n",
        "        search_system.load_papers_from_dir(papers_dir)\n",
        "        # 임베딩 생성\n",
        "        search_system.create_paper_embeddings()\n",
        "        # 임베딩 저장\n",
        "        search_system.save_embeddings(embeddings_file)\n",
        "\n",
        "    # 각 테스트 쿼리로 검색 수행\n",
        "    for query in test_queries:\n",
        "        print(f\"\\n[검색어: '{query}']\")\n",
        "        try:\n",
        "            results = search_system.search(query, top_k=top_k)\n",
        "            display_search_results(results)\n",
        "        except Exception as e:\n",
        "            print(f\"검색 중 오류 발생: {e}\")\n",
        "\n",
        "    # 대화형 모드\n",
        "    print(\"\\n===== hiBERT 한국 논문 검색 시스템 =====\")\n",
        "    print(\"'quit' 또는 'exit'를 입력하면 종료합니다.\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"\\n검색어를 입력하세요: \")\n",
        "        if query.lower() in ['quit', 'exit', '종료']:\n",
        "            print(\"검색 시스템을 종료합니다.\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            results = search_system.search(query, top_k=top_k)\n",
        "            display_search_results(results)\n",
        "        except Exception as e:\n",
        "            print(f\"검색 중 오류가 발생했습니다: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRBhxLQ7MpI0",
        "outputId": "4c9f3a66-69c6-44a8-f8b9-4544869c1d8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "파인튜닝된 hiBERT 모델을 성공적으로 로드했습니다: /content/drive/MyDrive/Workspaces/Hibrainnet/models/hibrainnet/hiBERT\n",
            "Loaded 698 papers and embeddings with shape: (698, 768)\n",
            "\n",
            "[검색어: '온톨로지 기반 검색']\n",
            "\n",
            "검색 결과 (1개):\n",
            "================================================================================\n",
            "1. [0.4094] 온톨로지 기반에서 연관 마이닝 방법을 이용한 지식 추론 알고리즘 연구\n",
            "   저자: 황현숙(부경대학교), 이준연(동명대학교)\n",
            "   분야: 전자/정보통신공학\n",
            "   키워드: Ontology(온톨로지), Association Mining(연관마이닝), Inference Algorithm(추론알고리즘), User Profile Modeling(사용자 프로파일 모델링), Databases(데이터베이스)\n",
            "   초록: 정보 검색에 대한 연구는 방대한 데이터에서 원하는 검색 정보를 제공할 뿐 만 아니라 개인의 취향에 따른 맞춤 검색 및 추론된 지식을 제공하는 데 초점을 두고 있다. 본 논문의 목적은 데이터를 개념화하여 분류 및 정의할 수 있는 온톨로지 구조를 기반으로 숨어있는 지식을 발견하여 개인 맞춤 검색을 제공하는 추론 알고리즘에 대해 연구하는 것이다. 현재의 검색에서...\n",
            "   파일: KCI-ART001293627-온톨로지 기반에서 연관 마이닝 방법을 이용한 지식 추론 알고리즘 연구.json\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[검색어: '인공지능 알고리즘']\n",
            "\n",
            "검색 결과 (2개):\n",
            "================================================================================\n",
            "1. [0.3407] 인공지능을 활용하는 행정작용에 대한 법적 고찰\n",
            "   저자: 강현호(성균관대학교)\n",
            "   분야: 법학\n",
            "   키워드: 인공지능(Künstliche Intelligenz), 설명요구권(Recht auf Erklärung), 알고리즘(Algorithmus), 자동적 처분(automatische Disposition), 딥러닝(Deep Learning)\n",
            "   초록: 국문초록\n",
            "\n",
            "인공지능이 점차적으로 발전되고 있다. 인공지능이란 사람의 지능과 유사하게 능력을 발휘하는 기계적 프로그램 내지 그러한 프로그램을 탑재한 장치라고 할 수 있을 것이다. 인공지능을 발달시켜서 인간 사회에 유익을 가져오려는 시도는 앞으로도 계속될 것이고, 그러한 발전에 발맞추어 행정을 수행하는 과정이나 행정결정을 내리는데 있어서도 인공지능이 활용될 것...\n",
            "   파일: KCI-ART003109018-인공지능을 활용하는 행정작용에 대한 법적 고찰.json\n",
            "--------------------------------------------------------------------------------\n",
            "2. [0.3407] 인공지능을 활용하는 행정작용에 대한 법적 고찰\n",
            "   저자: 강현호(성균관대학교)\n",
            "   분야: 법학\n",
            "   키워드: 인공지능(Künstliche Intelligenz), 설명요구권(Recht auf Erklärung), 알고리즘(Algorithmus), 자동적 처분(automatische Disposition), 딥러닝(Deep Learning)\n",
            "   초록: 국문초록\n",
            "\n",
            "인공지능이 점차적으로 발전되고 있다. 인공지능이란 사람의 지능과 유사하게 능력을 발휘하는 기계적 프로그램 내지 그러한 프로그램을 탑재한 장치라고 할 수 있을 것이다. 인공지능을 발달시켜서 인간 사회에 유익을 가져오려는 시도는 앞으로도 계속될 것이고, 그러한 발전에 발맞추어 행정을 수행하는 과정이나 행정결정을 내리는데 있어서도 인공지능이 활용될 것...\n",
            "   파일: KCI-ART003109018-인공지능을 활용하는 행정작용에 대한 법적 고찰 (1).json\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[검색어: '데이터 마이닝']\n",
            "\n",
            "검색 결과 (2개):\n",
            "================================================================================\n",
            "1. [0.3045] Gustave Guillaume 대용량 데이터베이스 검색 시스템 개발\n",
            "   분야: 프랑스문화학\n",
            "   키워드: 기욤 데이터베이스(Base de donn&eacute;es Gustave Guillaume), 인덱스검색(Recherche dans l'index), 원문검색(Recherche dans les textes), 기욤(Gustave Guillaume), 정보검색시스템(la recherche d'information)\n",
            "   파일: KCI-ART001047515-Gustave Guillaume 대용량 데이터베이스 검색 시스템 개발.json\n",
            "--------------------------------------------------------------------------------\n",
            "2. [0.3000] 데이터 베이스 특성에 따른 효율적인 데이터 마이닝 알고리즘\n",
            "   저자: 박지현((주)더모스트), 고찬(서울산업대학교)\n",
            "   분야: 수학\n",
            "   키워드: 데이터 마이닝, 데이터베이스 특성, 연관 규칙 탐사ⓒ THE KOREAN SOCIETY FOR INDUSTRIAL AND APPLIED MATHEMATICS, 2006, 데이터 마이닝, 데이터베이스 특성, 연관 규칙 탐사ⓒ THE KOREAN SOCIETY FOR INDUSTRIAL AND APPLIED MATHEMATICS, 2006\n",
            "   초록: 인터넷과 웹 기술 발전에 따라 데이터베이스에 축적되는 자료의 양이 급속히 늘어나고 있다. 데이터베이스의 응용 범위가 확대되고 대용량 데이터베이스로부터 유용한 지식을 발견하고자 하는 데이터 마이닝(Data Mining) 기술에 대한 연구가 활발하게 진행되고 있다. 기존의 알고리즘들은 대부분 후보 항목 집합들을 줄임과 동시에 데이터베이스의 크기를 줄이는 방법으...\n",
            "   파일: KCI-ART001021651-데이터 베이스 특성에 따른 효율적인 데이터 마이닝 알고리즘.json\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[검색어: '추론 시스템']\n",
            "검색 결과가 없습니다.\n",
            "\n",
            "[검색어: '지식 그래프']\n",
            "검색 결과가 없습니다.\n",
            "\n",
            "===== hiBERT 한국 논문 검색 시스템 =====\n",
            "'quit' 또는 'exit'를 입력하면 종료합니다.\n",
            "\n",
            "\n",
            "검색어를 입력하세요: exit\n",
            "검색 시스템을 종료합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "\n",
        "def main():\n",
        "    # 1. 설정 변수 (여기에서 실제 경로로 변경하세요)\n",
        "    MODEL_PATH = f\"{HBN_MODELS_DIR_PATH}/hiBERT\"  # hiBERT 모델 경로\n",
        "    PAPERS_DIR = HBN_ARTICLES_DIR_PATH       # 논문 파일이 있는 디렉토리\n",
        "\n",
        "    # 사용할 검색어 설정\n",
        "    QUERIES = [\n",
        "        \"온톨로지 기반 검색 시스템\",\n",
        "        # \"데이터 마이닝 기법\",\n",
        "        # \"지식 추론 알고리즘\",\n",
        "        # \"시맨틱 웹\",\n",
        "        # \"연관 마이닝 방법\",\n",
        "        # \"SWRL 추론 언어\",\n",
        "        # \"Jess 엔진\",\n",
        "        # \"온톨로지 제약조건\",\n",
        "        # \"개인 맞춤 검색\"\n",
        "    ]\n",
        "\n",
        "    print(f\"[설정 정보]\")\n",
        "    print(f\"- 모델 경로: {MODEL_PATH}\")\n",
        "    print(f\"- 논문 디렉토리: {PAPERS_DIR}\")\n",
        "    print(f\"- 검색어 개수: {len(QUERIES)}개\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # 2. 장치 설정\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"사용 장치: {device}\")\n",
        "\n",
        "    # 3. 모델 로드\n",
        "    try:\n",
        "        print(f\"모델 로드 중...\")\n",
        "        tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
        "        model = BertModel.from_pretrained(MODEL_PATH)\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        print(f\"모델 로드 완료: {MODEL_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"모델 로드 실패: {e}\")\n",
        "        print(f\"기본 KoSimCSE-BERT 모델로 대체합니다.\")\n",
        "        tokenizer = BertTokenizer.from_pretrained(\"BM-K/KoSimCSE-bert\")\n",
        "        model = BertModel.from_pretrained(\"BM-K/KoSimCSE-bert\")\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "    # 4. 논문 파일 로드\n",
        "    json_files = glob.glob(os.path.join(PAPERS_DIR, \"*.json\"))\n",
        "    print(f\"논문 파일 {len(json_files)}개 발견\")\n",
        "\n",
        "    papers = []\n",
        "    for file_path in tqdm(json_files, desc=\"논문 로드 중\"):\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                paper_data = json.load(f)\n",
        "\n",
        "            # 논문 정보 추출\n",
        "            article_info = paper_data.get('articleInfo', {})\n",
        "\n",
        "            # 제목 추출\n",
        "            title = \"\"\n",
        "            title_group = article_info.get('title-group', {})\n",
        "            if isinstance(title_group.get('article-title'), list):\n",
        "                for title_item in title_group.get('article-title', []):\n",
        "                    if title_item.get('@lang') == 'original':\n",
        "                        title = title_item.get('#text', '')\n",
        "                        break\n",
        "            elif isinstance(title_group.get('article-title'), dict):\n",
        "                title = title_group.get('article-title', {}).get('#text', '')\n",
        "\n",
        "            # 초록 추출\n",
        "            abstract = \"\"\n",
        "            abstract_group = article_info.get('abstract-group', {})\n",
        "            if isinstance(abstract_group.get('abstract'), list):\n",
        "                for abstract_item in abstract_group.get('abstract', []):\n",
        "                    if abstract_item.get('@lang') == 'original':\n",
        "                        abstract = abstract_item.get('#text', '')\n",
        "                        break\n",
        "            elif isinstance(abstract_group.get('abstract'), dict):\n",
        "                abstract = abstract_group.get('abstract', {}).get('#text', '')\n",
        "\n",
        "            # 논문 정보 저장\n",
        "            if title or abstract:\n",
        "                papers.append({\n",
        "                    'title': title,\n",
        "                    'abstract': abstract,\n",
        "                    'file_path': file_path\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"파일 처리 오류 ({file_path}): {e}\")\n",
        "\n",
        "    print(f\"총 {len(papers)}개 논문 로드 완료\")\n",
        "\n",
        "    # 5. 임베딩 생성\n",
        "    print(\"논문 임베딩 생성 중...\")\n",
        "    paper_embeddings = []\n",
        "\n",
        "    for paper in tqdm(papers, desc=\"임베딩 생성\"):\n",
        "        # 제목과 초록 결합 (제목은 더 중요하므로 두 번 포함)\n",
        "        text = f\"{paper['title']} {paper['title']} {paper['abstract']}\"\n",
        "\n",
        "        # 토큰화\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "        # GPU로 이동\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # 임베딩 계산\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            embedding = outputs.last_hidden_state[:, 0].cpu().numpy()\n",
        "            paper_embeddings.append(embedding[0])\n",
        "\n",
        "    paper_embeddings = np.array(paper_embeddings)\n",
        "    print(f\"임베딩 생성 완료: {paper_embeddings.shape}\")\n",
        "\n",
        "    # 6. 각 검색어에 대한 검색 수행\n",
        "    for i, query in enumerate(QUERIES, 1):\n",
        "        print(f\"\\n[{i}/{len(QUERIES)}] 검색어: '{query}'\")\n",
        "\n",
        "        # 쿼리 임베딩 생성\n",
        "        inputs = tokenizer(\n",
        "            query,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            query_embedding = outputs.last_hidden_state[:, 0].cpu().numpy()\n",
        "\n",
        "        # 유사도 계산\n",
        "        similarities = cosine_similarity(query_embedding, paper_embeddings)[0]\n",
        "        top_indices = similarities.argsort()[::-1][:5]  # 상위 5개 결과\n",
        "\n",
        "        # 결과 출력\n",
        "        print(\"-\" * 80)\n",
        "        for rank, idx in enumerate(top_indices, 1):\n",
        "            paper = papers[idx]\n",
        "            score = similarities[idx]\n",
        "            print(f\"{rank}. [{score:.4f}] {paper['title']}\")\n",
        "\n",
        "            # 초록 앞부분 표시\n",
        "            abstract = paper['abstract']\n",
        "            if len(abstract) > 200:\n",
        "                abstract = abstract[:200] + \"...\"\n",
        "            print(f\"   초록: {abstract}\")\n",
        "            print(f\"   파일: {os.path.basename(paper['file_path'])}\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "    print(\"\\n모든 검색 테스트가 완료되었습니다.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "o2SZNa2GO7eG",
        "outputId": "37b2e6e8-e445-4227-a319-dbf258cd74a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[설정 정보]\n",
            "- 모델 경로: /content/drive/MyDrive/Workspaces/Hibrainnet/models/hibrainnet/hiBERT\n",
            "- 논문 디렉토리: /content/drive/MyDrive/Workspaces/Hibrainnet/hbn-data-lake/papers/kci/data\n",
            "- 검색어 개수: 1개\n",
            "--------------------------------------------------\n",
            "사용 장치: cuda\n",
            "모델 로드 중...\n",
            "모델 로드 완료: /content/drive/MyDrive/Workspaces/Hibrainnet/models/hibrainnet/hiBERT\n",
            "논문 파일 717개 발견\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "논문 로드 중: 100%|██████████| 717/717 [00:13<00:00, 53.02it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 717개 논문 로드 완료\n",
            "논문 임베딩 생성 중...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "임베딩 생성: 100%|██████████| 717/717 [00:09<00:00, 79.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임베딩 생성 완료: (717, 768)\n",
            "\n",
            "[1/1] 검색어: '온톨로지 기반 검색 시스템'\n",
            "--------------------------------------------------------------------------------\n",
            "1. [0.4261] 온톨로지 기반에서 연관 마이닝 방법을 이용한 지식 추론 알고리즘 연구\n",
            "   초록: 정보 검색에 대한 연구는 방대한 데이터에서 원하는 검색 정보를 제공할 뿐 만 아니라 개인의 취향에 따른 맞춤 검색 및 추론된 지식을 제공하는 데 초점을 두고 있다. 본 논문의 목적은 데이터를 개념화하여 분류 및 정의할 수 있는 온톨로지 구조를 기반으로 숨어있는 지식을 발견하여 개인 맞춤 검색을 제공하는 추론 알고리즘에 대해 연구하는 것이다. 현재의 검색에서...\n",
            "   파일: KCI-ART001293627-온톨로지 기반에서 연관 마이닝 방법을 이용한 지식 추론 알고리즘 연구.json\n",
            "--------------------------------------------------------------------------------\n",
            "2. [0.2350] 웹 기반의 삼차원 스테레오 카메라 원격제어 시스템 설계 및 구현\n",
            "   초록: \n",
            "   파일: KCI-ART000910284-웹 기반의 삼차원 스테레오 카메라 원격제어 시스템 설계 및 구현.json\n",
            "--------------------------------------------------------------------------------\n",
            "3. [0.2316] 나노간극 구동기를 이용한 나노기계적 단백질 검출기\n",
            "   초록: \n",
            "   파일: KCI-ART001094584-나노간극 구동기를 이용한 나노기계적 단백질 검출기.json\n",
            "--------------------------------------------------------------------------------\n",
            "4. [0.2035] 정보검색 활용을 위한 다국어 데이터베이스의 ILI(Inter-Lingual-Index) 방법론 연구- 유로워드넷의 ILI 확장과 관련하여 -\n",
            "   초록: \n",
            "   파일: KCI-ART001147966-정보검색 활용을 위한 다국어 데이터베이스의 ILI(Inter-Lingual-Index) 방법론 연구- 유로워드넷의 ILI 확장과 관련하여 -.json\n",
            "--------------------------------------------------------------------------------\n",
            "5. [0.1965] 고품질 과학기술 학술정보 생산을 위한 종합 포털 서비스 체계의 설계\n",
            "   초록: KISTI-ACOMS ver. 2.0은 2005년부터 4년간 340여 학회에 보급되었고 130여 학회가 사용 중이며 여전히 사용을 희망하는 학회가 많다. 그러나 여러 학회들의 희망과 업그레이드 요청에 비해 지난 3 년간 시스템의 실질적인 개선은 이루어지지 못하였고, 그 동안 국내외의 유사한 유료 온라인 투고/심사 관리 시스템을 도입하는 학회가 생겨나기 시작...\n",
            "   파일: KCI-ART001393318-고품질 과학기술 학술정보 생산을 위한 종합 포털 서비스 체계의 설계.json\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "모든 검색 테스트가 완료되었습니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "논문 검색 모듈: hiBERT 모델을 사용한 의미 기반 논문 검색 기능 제공\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "\n",
        "class PaperSearchEngine:\n",
        "    \"\"\"\n",
        "    hiBERT 기반 논문 검색 엔진 클래스\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path):\n",
        "        \"\"\"\n",
        "        검색 엔진 초기화\n",
        "\n",
        "        Args:\n",
        "            model_path (str): hiBERT 모델 경로\n",
        "        \"\"\"\n",
        "        self.model_path = model_path\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.papers = []\n",
        "        self.paper_embeddings = None\n",
        "        self.load_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"모델 및 토크나이저 로드\"\"\"\n",
        "        try:\n",
        "            self.tokenizer = BertTokenizer.from_pretrained(self.model_path)\n",
        "            self.model = BertModel.from_pretrained(self.model_path)\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval()\n",
        "            print(f\"hiBERT 모델 로드 성공: {self.model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"모델 로드 실패: {e}\")\n",
        "            print(\"기본 KoSimCSE-BERT 모델로 대체합니다.\")\n",
        "            self.tokenizer = BertTokenizer.from_pretrained(\"BM-K/KoSimCSE-bert\")\n",
        "            self.model = BertModel.from_pretrained(\"BM-K/KoSimCSE-bert\")\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval()\n",
        "\n",
        "    def load_papers_from_dir(self, papers_dir):\n",
        "        \"\"\"\n",
        "        디렉토리에서 논문 파일 로드\n",
        "\n",
        "        Args:\n",
        "            papers_dir (str): 논문 JSON 파일 디렉토리 경로\n",
        "\n",
        "        Returns:\n",
        "            int: 로드된 논문 수\n",
        "        \"\"\"\n",
        "        json_files = glob.glob(os.path.join(papers_dir, \"*.json\"))\n",
        "        print(f\"논문 파일 {len(json_files)}개 발견\")\n",
        "\n",
        "        self.papers = []\n",
        "        for file_path in tqdm(json_files, desc=\"논문 로드 중\"):\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    paper_data = json.load(f)\n",
        "\n",
        "                paper_info = self._extract_paper_info(paper_data, file_path)\n",
        "\n",
        "                # 논문 정보가 제대로 존재하는 경우만 추가\n",
        "                if paper_info['title'] or paper_info['abstract']:\n",
        "                    self.papers.append(paper_info)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"파일 처리 오류 ({file_path}): {e}\")\n",
        "\n",
        "        print(f\"총 {len(self.papers)}개 논문 로드 완료\")\n",
        "        return len(self.papers)\n",
        "\n",
        "    def _extract_paper_info(self, paper_data, file_path):\n",
        "        \"\"\"\n",
        "        논문 데이터에서 필요한 정보 추출\n",
        "\n",
        "        Args:\n",
        "            paper_data (dict): 논문 JSON 데이터\n",
        "            file_path (str): 파일 경로\n",
        "\n",
        "        Returns:\n",
        "            dict: 추출된 논문 정보\n",
        "        \"\"\"\n",
        "        # 기본 정보 구조 생성\n",
        "        paper_info = {\n",
        "            'title': '',\n",
        "            'abstract': '',\n",
        "            'keywords': [],\n",
        "            'authors': [],\n",
        "            'category': '',\n",
        "            'file_path': file_path\n",
        "        }\n",
        "\n",
        "        # 논문 정보 추출\n",
        "        article_info = paper_data.get('articleInfo', {})\n",
        "        detail_info = paper_data.get('detailInfo', {})\n",
        "        detail_article_info = detail_info.get('articleInfo', {})\n",
        "\n",
        "        # 제목 추출\n",
        "        title_group = article_info.get('title-group', {})\n",
        "        if isinstance(title_group.get('article-title'), list):\n",
        "            for title_item in title_group.get('article-title', []):\n",
        "                if title_item.get('@lang') == 'original':\n",
        "                    paper_info['title'] = title_item.get('#text', '')\n",
        "                    break\n",
        "        elif isinstance(title_group.get('article-title'), dict):\n",
        "            paper_info['title'] = title_group.get('article-title', {}).get('#text', '')\n",
        "\n",
        "        # 초록 추출\n",
        "        abstract_group = article_info.get('abstract-group', {})\n",
        "        if isinstance(abstract_group.get('abstract'), list):\n",
        "            for abstract_item in abstract_group.get('abstract', []):\n",
        "                if abstract_item.get('@lang') == 'original':\n",
        "                    paper_info['abstract'] = abstract_item.get('#text', '')\n",
        "                    break\n",
        "        elif isinstance(abstract_group.get('abstract'), dict):\n",
        "            paper_info['abstract'] = abstract_group.get('abstract', {}).get('#text', '')\n",
        "\n",
        "        # 키워드 추출\n",
        "        keyword_group = detail_article_info.get('keyword-group', {})\n",
        "        if isinstance(keyword_group.get('keyword'), list):\n",
        "            paper_info['keywords'] = keyword_group.get('keyword', [])\n",
        "        elif isinstance(keyword_group.get('keyword'), str):\n",
        "            paper_info['keywords'] = [keyword_group.get('keyword', '')]\n",
        "\n",
        "        # 저자 추출\n",
        "        author_group = article_info.get('author-group', {})\n",
        "        if isinstance(author_group.get('author'), list):\n",
        "            for author in author_group.get('author', []):\n",
        "                if isinstance(author, str):\n",
        "                    paper_info['authors'].append(author)\n",
        "                elif isinstance(author, dict):\n",
        "                    author_name = author.get('#text', '')\n",
        "                    if author_name:\n",
        "                        paper_info['authors'].append(author_name)\n",
        "        elif isinstance(author_group.get('author'), dict):\n",
        "            author_name = author_group.get('author', {}).get('#text', '')\n",
        "            if author_name:\n",
        "                paper_info['authors'].append(author_name)\n",
        "\n",
        "        # 카테고리 추출\n",
        "        paper_info['category'] = article_info.get('article-categories', '')\n",
        "\n",
        "        return paper_info\n",
        "\n",
        "    def create_embeddings(self, batch_size=4):\n",
        "        \"\"\"\n",
        "        논문 임베딩 생성\n",
        "\n",
        "        Args:\n",
        "            batch_size (int): 배치 크기\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: 생성된 임베딩 배열\n",
        "        \"\"\"\n",
        "        if not self.papers:\n",
        "            raise ValueError(\"논문을 먼저 로드해야 합니다.\")\n",
        "\n",
        "        paper_embeddings = []\n",
        "\n",
        "        for paper in tqdm(self.papers, desc=\"임베딩 생성\"):\n",
        "            # 제목, 키워드, 초록 결합\n",
        "            title = paper.get('title', '')\n",
        "            abstract = paper.get('abstract', '')\n",
        "            keywords_str = ' '.join(paper.get('keywords', []))\n",
        "\n",
        "            # 결합 텍스트 생성 (제목과 키워드에 가중치 부여)\n",
        "            combined_text = f\"{title} {title} {keywords_str} {keywords_str} {abstract}\"\n",
        "\n",
        "            # 임베딩 생성\n",
        "            embedding = self._create_embedding(combined_text)\n",
        "            paper_embeddings.append(embedding)\n",
        "\n",
        "        self.paper_embeddings = np.array(paper_embeddings)\n",
        "        print(f\"임베딩 생성 완료: {self.paper_embeddings.shape}\")\n",
        "        return self.paper_embeddings\n",
        "\n",
        "    def _create_embedding(self, text):\n",
        "        \"\"\"\n",
        "        텍스트에 대한 임베딩 생성\n",
        "\n",
        "        Args:\n",
        "            text (str): 임베딩할 텍스트\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: 생성된 임베딩 벡터\n",
        "        \"\"\"\n",
        "        # 토큰화\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "        # GPU로 이동\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        # 임베딩 계산\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            embedding = outputs.last_hidden_state[:, 0].cpu().numpy()[0]\n",
        "\n",
        "        return embedding\n",
        "\n",
        "    def search(self, query, top_k=5):\n",
        "        \"\"\"\n",
        "        쿼리로 논문 검색\n",
        "\n",
        "        Args:\n",
        "            query (str): 검색 쿼리\n",
        "            top_k (int): 반환할 상위 결과 수\n",
        "\n",
        "        Returns:\n",
        "            list: 검색 결과 (논문 정보와 유사도 점수)\n",
        "        \"\"\"\n",
        "        if not self.papers or self.paper_embeddings is None:\n",
        "            raise ValueError(\"논문과 임베딩을 먼저 생성해야 합니다.\")\n",
        "\n",
        "        # 쿼리 임베딩 생성\n",
        "        query_embedding = self._create_embedding(query)\n",
        "\n",
        "        # 유사도 계산\n",
        "        similarities = cosine_similarity([query_embedding], self.paper_embeddings)[0]\n",
        "\n",
        "        # 상위 결과 인덱스\n",
        "        top_indices = similarities.argsort()[::-1][:top_k]\n",
        "\n",
        "        # 결과 생성\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            paper_info = self.papers[idx].copy()\n",
        "            paper_info['similarity_score'] = float(similarities[idx])\n",
        "            results.append(paper_info)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def get_paper_similarity_matrix(self, max_papers=None):\n",
        "        \"\"\"\n",
        "        논문 간 유사도 매트릭스 계산\n",
        "\n",
        "        Args:\n",
        "            max_papers (int, optional): 계산할 최대 논문 수\n",
        "\n",
        "        Returns:\n",
        "            tuple: (유사도 매트릭스, 대상 논문 목록)\n",
        "        \"\"\"\n",
        "        if self.paper_embeddings is None:\n",
        "            raise ValueError(\"임베딩을 먼저 생성해야 합니다.\")\n",
        "\n",
        "        # 계산할 논문 수 제한 (메모리 문제 방지)\n",
        "        if max_papers is not None and max_papers < len(self.papers):\n",
        "            embeddings = self.paper_embeddings[:max_papers]\n",
        "            papers = self.papers[:max_papers]\n",
        "        else:\n",
        "            embeddings = self.paper_embeddings\n",
        "            papers = self.papers\n",
        "\n",
        "        # 유사도 매트릭스 계산\n",
        "        similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "        return similarity_matrix, papers\n",
        "\n",
        "    def save_embeddings(self, output_file):\n",
        "        \"\"\"\n",
        "        임베딩 저장\n",
        "\n",
        "        Args:\n",
        "            output_file (str): 저장할 파일 경로\n",
        "        \"\"\"\n",
        "        if self.paper_embeddings is None:\n",
        "            raise ValueError(\"임베딩을 먼저 생성해야 합니다.\")\n",
        "\n",
        "        # 저장할 데이터 준비\n",
        "        data = {\n",
        "            'papers': self.papers,\n",
        "            'embeddings': self.paper_embeddings.tolist()\n",
        "        }\n",
        "\n",
        "        # 디렉토리 생성\n",
        "        os.makedirs(os.path.dirname(os.path.abspath(output_file)), exist_ok=True)\n",
        "\n",
        "        # 파일에 저장\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"임베딩 저장 완료: {output_file}\")\n",
        "\n",
        "    def load_embeddings(self, input_file):\n",
        "        \"\"\"\n",
        "        저장된 임베딩 로드\n",
        "\n",
        "        Args:\n",
        "            input_file (str): 임베딩 파일 경로\n",
        "\n",
        "        Returns:\n",
        "            bool: 성공 여부\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(input_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            self.papers = data.get('papers', [])\n",
        "            self.paper_embeddings = np.array(data.get('embeddings', []))\n",
        "\n",
        "            print(f\"임베딩 로드 완료: {input_file}\")\n",
        "            print(f\"논문 {len(self.papers)}개, 임베딩 형태 {self.paper_embeddings.shape}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"임베딩 로드 실패: {e}\")\n",
        "            return False"
      ],
      "metadata": {
        "id": "y7qeo24GPPaD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "논문 검색 모듈 테스트 스크립트\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "def print_search_result(result, rank):\n",
        "    \"\"\"\n",
        "    검색 결과 출력 헬퍼 함수\n",
        "\n",
        "    Args:\n",
        "        result (dict): 검색 결과 항목\n",
        "        rank (int): 순위\n",
        "    \"\"\"\n",
        "    print(f\"{rank}. [{result['similarity_score']:.4f}] {result['title']}\")\n",
        "\n",
        "    # 저자 표시\n",
        "    if result.get('authors'):\n",
        "        authors_str = ', '.join(result['authors'])\n",
        "        print(f\"   저자: {authors_str}\")\n",
        "\n",
        "    # 키워드 표시\n",
        "    if result.get('keywords'):\n",
        "        keywords_str = ', '.join(result['keywords'])\n",
        "        print(f\"   키워드: {keywords_str}\")\n",
        "\n",
        "    # 카테고리 표시\n",
        "    if result.get('category'):\n",
        "        print(f\"   분야: {result['category']}\")\n",
        "\n",
        "    # 초록 앞부분 표시\n",
        "    abstract = result.get('abstract', '')\n",
        "    if len(abstract) > 200:\n",
        "        abstract = abstract[:200] + \"...\"\n",
        "    print(f\"   초록: {abstract}\")\n",
        "    print(f\"   파일: {os.path.basename(result['file_path'])}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "def print_similarity_matrix(similarity_matrix, papers, max_display=10):\n",
        "    \"\"\"\n",
        "    유사도 매트릭스 출력\n",
        "\n",
        "    Args:\n",
        "        similarity_matrix: 유사도 매트릭스\n",
        "        papers: 논문 목록\n",
        "        max_display: 최대 표시할 행/열 수\n",
        "    \"\"\"\n",
        "    n = min(similarity_matrix.shape[0], max_display)\n",
        "\n",
        "    print(\"\\n논문 간 유사도 매트릭스:\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # 헤더 출력\n",
        "    header = \"ID |\"\n",
        "    for i in range(n):\n",
        "        paper_id = f\"P{i+1}\"\n",
        "        header += f\" {paper_id:^6} |\"\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "\n",
        "    # 유사도 매트릭스 출력\n",
        "    for i in range(n):\n",
        "        row = f\"P{i+1} |\"\n",
        "        for j in range(n):\n",
        "            if i == j:\n",
        "                row += f\" {'---':^6} |\"  # 자기 자신과의 유사도는 표시 생략\n",
        "            else:\n",
        "                row += f\" {similarity_matrix[i, j]:.4f} |\"\n",
        "        print(row)\n",
        "\n",
        "    print(\"-\" * len(header))\n",
        "\n",
        "    # 각 논문 제목 출력\n",
        "    for i in range(n):\n",
        "        print(f\"P{i+1}: {papers[i]['title'][:50]}{'...' if len(papers[i]['title']) > 50 else ''}\")\n",
        "\n",
        "def test_search_engine():\n",
        "    \"\"\"\n",
        "    논문 검색 엔진 테스트 함수\n",
        "    \"\"\"\n",
        "    # 설정 변수\n",
        "    MODEL_PATH = f\"{HBN_MODELS_DIR_PATH}/hiBERT\"  # hiBERT 모델 경로\n",
        "    PAPERS_DIR = HBN_ARTICLES_DIR_PATH       # 논문 디렉토리 경로\n",
        "    EMBEDDINGS_FILE = f\"{HBN_EMBEDDINGS_DIR_PATH}/paper_embeddings.json\"  # 임베딩 저장/로드 파일\n",
        "    CREATE_NEW_EMBEDDINGS = False              # 새 임베딩 생성 여부\n",
        "\n",
        "    # 테스트 검색어\n",
        "    TEST_QUERIES = [\n",
        "        \"온톨로지 기반 검색 시스템\",\n",
        "        \"데이터 마이닝 기법\",\n",
        "        \"지식 추론 알고리즘\",\n",
        "        # \"시맨틱 웹\",\n",
        "        # \"연관 마이닝 방법\",\n",
        "        # \"SWRL 추론 언어\",\n",
        "        # \"Jess 엔진\",\n",
        "        # \"온톨로지 제약조건\",\n",
        "        # \"개인 맞춤 검색\"\n",
        "    ]\n",
        "\n",
        "    # 시작 시간 기록\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"[테스트 설정]\")\n",
        "    print(f\"- 모델 경로: {MODEL_PATH}\")\n",
        "    print(f\"- 논문 디렉토리: {PAPERS_DIR}\")\n",
        "    print(f\"- 임베딩 파일: {EMBEDDINGS_FILE}\")\n",
        "    print(f\"- 새 임베딩 생성: {CREATE_NEW_EMBEDDINGS}\")\n",
        "    print(f\"- 테스트 검색어: {len(TEST_QUERIES)}개\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # 1. 검색 엔진 초기화\n",
        "    search_engine = PaperSearchEngine(MODEL_PATH)\n",
        "\n",
        "    # 2. 임베딩 로드 또는 생성\n",
        "    if not CREATE_NEW_EMBEDDINGS and os.path.exists(EMBEDDINGS_FILE):\n",
        "        # 기존 임베딩 로드\n",
        "        success = search_engine.load_embeddings(EMBEDDINGS_FILE)\n",
        "        if not success:\n",
        "            print(\"임베딩 로드 실패, 새로 생성합니다.\")\n",
        "            CREATE_NEW_EMBEDDINGS = True\n",
        "\n",
        "    if CREATE_NEW_EMBEDDINGS or not os.path.exists(EMBEDDINGS_FILE):\n",
        "        # 새 임베딩 생성\n",
        "        search_engine.load_papers_from_dir(PAPERS_DIR)\n",
        "        search_engine.create_embeddings()\n",
        "        search_engine.save_embeddings(EMBEDDINGS_FILE)\n",
        "\n",
        "    # 3. 각 테스트 검색어로 검색 실행\n",
        "    print(\"\\n===== 검색 테스트 시작 =====\")\n",
        "\n",
        "    for i, query in enumerate(TEST_QUERIES, 1):\n",
        "        print(f\"\\n[{i}/{len(TEST_QUERIES)}] 검색어: '{query}'\")\n",
        "\n",
        "        try:\n",
        "            # 검색 수행\n",
        "            results = search_engine.search(query, top_k=5)\n",
        "\n",
        "            # 결과 출력\n",
        "            print(\"-\" * 80)\n",
        "            for rank, result in enumerate(results, 1):\n",
        "                print_search_result(result, rank)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"검색 중 오류 발생: {e}\")\n",
        "\n",
        "    # 4. 논문 간 유사도 분석\n",
        "    try:\n",
        "        similarity_matrix, papers = search_engine.get_paper_similarity_matrix(max_papers=10)\n",
        "        print_similarity_matrix(similarity_matrix, papers)\n",
        "    except Exception as e:\n",
        "        print(f\"유사도 분석 중 오류 발생: {e}\")\n",
        "\n",
        "    # 5. 실행 시간 출력\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"\\n테스트 완료! 실행 시간: {elapsed_time:.2f}초\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_search_engine()"
      ],
      "metadata": {
        "id": "GhMFfgRYPWwl",
        "outputId": "3a79de14-eefe-4634-b843-2f701fcaed4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[테스트 설정]\n",
            "- 모델 경로: /content/drive/MyDrive/Workspaces/Hibrainnet/models/hibrainnet/hiBERT\n",
            "- 논문 디렉토리: /content/drive/MyDrive/Workspaces/Hibrainnet/hbn-data-lake/papers/kci/data\n",
            "- 임베딩 파일: /content/drive/MyDrive/Workspaces/Hibrainnet/embeddings/hibrainnet/paper_embeddings.json\n",
            "- 새 임베딩 생성: False\n",
            "- 테스트 검색어: 3개\n",
            "--------------------------------------------------------------------------------\n",
            "hiBERT 모델 로드 성공: /content/drive/MyDrive/Workspaces/Hibrainnet/models/hibrainnet/hiBERT\n",
            "임베딩 로드 완료: /content/drive/MyDrive/Workspaces/Hibrainnet/embeddings/hibrainnet/paper_embeddings.json\n",
            "논문 698개, 임베딩 형태 (698, 768)\n",
            "\n",
            "===== 검색 테스트 시작 =====\n",
            "\n",
            "[1/3] 검색어: '온톨로지 기반 검색 시스템'\n",
            "--------------------------------------------------------------------------------\n",
            "1. [0.3659] 온톨로지 기반에서 연관 마이닝 방법을 이용한 지식 추론 알고리즘 연구\n",
            "   저자: 황현숙(부경대학교), 이준연(동명대학교)\n",
            "   키워드: Ontology(온톨로지), Association Mining(연관마이닝), Inference Algorithm(추론알고리즘), User Profile Modeling(사용자 프로파일 모델링), Databases(데이터베이스)\n",
            "   분야: 전자/정보통신공학\n",
            "   초록: 정보 검색에 대한 연구는 방대한 데이터에서 원하는 검색 정보를 제공할 뿐 만 아니라 개인의 취향에 따른 맞춤 검색 및 추론된 지식을 제공하는 데 초점을 두고 있다. 본 논문의 목적은 데이터를 개념화하여 분류 및 정의할 수 있는 온톨로지 구조를 기반으로 숨어있는 지식을 발견하여 개인 맞춤 검색을 제공하는 추론 알고리즘에 대해 연구하는 것이다. 현재의 검색에서...\n",
            "   파일: KCI-ART001293627-온톨로지 기반에서 연관 마이닝 방법을 이용한 지식 추론 알고리즘 연구.json\n",
            "--------------------------------------------------------------------------------\n",
            "2. [0.2132] 가상 환경에서의 딥러닝 기반 폐색영역 검출을 위한 데이터베이스 구축\n",
            "   저자: 김경수, 황성호, 이재인, 곽석우, 강원율, 신대영\n",
            "   키워드: Virtual Environment(가상환경), Database(데이터베이스), Deep Learning(딥러닝), Object Detection (객체 인식), Image Segmentation(이미지 세그멘테이션), Occlusion Area(폐색 영역), RDBMS(관계형 데이터베이스 관리 시스템)\n",
            "   분야: 유압공학\n",
            "   초록: This paper proposes a method for constructing and verifying datasets used in deep learning technology, to prevent safety accidents in automated construction machinery or autonomous vehicles. Although ...\n",
            "   파일: KCI-ART002871979-가상 환경에서의 딥러닝 기반 폐색영역 검출을 위한 데이터베이스 구축.json\n",
            "--------------------------------------------------------------------------------\n",
            "3. [0.2036] Semantics of Uncertain Databases based on Linear Logic\n",
            "   키워드: 불확실성데이터베이스, 의미론, 선형논리, Uncertain database, semantics, linear logic\n",
            "   분야: 컴퓨터학\n",
            "   초록: In the study of the formal semantics of uncertain databases, we typically take an algebraic approach by mapping an uncertain database to possible worlds which are a set of relational databases. In thi...\n",
            "   파일: KCI-ART001419663-Semantics of Uncertain Databases based on Linear Logic.json\n",
            "--------------------------------------------------------------------------------\n",
            "4. [0.1963] 유압 솔레노이드 밸브 고장 진단 및 원격 데이터 수집 시스템 개발\n",
            "   저자: 유승진, 이재경\n",
            "   키워드: Hydraulic Solenoid Valve (유압 솔레노이드 밸브), Fault Diagnosis (고장 진단), Data Acquisition (데이터 수집), Artificial Intelligence (인공지능), IoT (사물인터넷)\n",
            "   분야: 유압공학\n",
            "   초록: This paper presents the development of a fault diagnosis and remote data acquisition system for hydraulic solenoid valves designed to enhance machine reliability and serve as a backbone for prognostic...\n",
            "   파일: KCI-ART003142820-유압 솔레노이드 밸브 고장 진단 및 원격 데이터 수집 시스템 개발.json\n",
            "--------------------------------------------------------------------------------\n",
            "5. [0.1963] 유압 솔레노이드 밸브 고장 진단 및 원격 데이터 수집 시스템 개발\n",
            "   저자: 유승진, 이재경\n",
            "   키워드: Hydraulic Solenoid Valve (유압 솔레노이드 밸브), Fault Diagnosis (고장 진단), Data Acquisition (데이터 수집), Artificial Intelligence (인공지능), IoT (사물인터넷)\n",
            "   분야: 유압공학\n",
            "   초록: This paper presents the development of a fault diagnosis and remote data acquisition system for hydraulic solenoid valves designed to enhance machine reliability and serve as a backbone for prognostic...\n",
            "   파일: KCI-ART003142820-유압 솔레노이드 밸브 고장 진단 및 원격 데이터 수집 시스템 개발 (1).json\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[2/3] 검색어: '데이터 마이닝 기법'\n",
            "--------------------------------------------------------------------------------\n",
            "1. [0.3612] 효율적인 최근접 질의 처리를 위한 Voronoi 다이어그램 기반 그리드 검색 구조\n",
            "   키워드: Voronoi 다이어그램(Voronoi diagram), 데이터베이스(Database), 최근접 검색질의(Nearest-neighbor search queries)\n",
            "   분야: 컴퓨터학\n",
            "   초록: 최근접 질의(nearest-neighbor query)는 멀티미디어 시스템이나 GIS 시스템과 같은 여러 가지 응용 분야에서 사용되는 중요한 질의 처리 기법 중 하나이다. 최근접 검색 기법들을 위한 다양한 연구가 제안되었으나 이러한 기법들은 질의 수행 시 데이터를 검색하여 최근접 질의를 처리하므로 성능의 한계가 있었다. 본 논문에서는 정적인 데이터에 대하여...\n",
            "   파일: KCI-ART001225610-효율적인 최근접 질의 처리를 위한 Voronoi 다이어그램 기반 그리드 검색 구조.json\n",
            "--------------------------------------------------------------------------------\n",
            "2. [0.3573] 데이터 베이스 특성에 따른 효율적인 데이터 마이닝 알고리즘\n",
            "   저자: 박지현((주)더모스트), 고찬(서울산업대학교)\n",
            "   키워드: 데이터 마이닝, 데이터베이스 특성, 연관 규칙 탐사ⓒ THE KOREAN SOCIETY FOR INDUSTRIAL AND APPLIED MATHEMATICS, 2006, 데이터 마이닝, 데이터베이스 특성, 연관 규칙 탐사ⓒ THE KOREAN SOCIETY FOR INDUSTRIAL AND APPLIED MATHEMATICS, 2006\n",
            "   분야: 수학\n",
            "   초록: 인터넷과 웹 기술 발전에 따라 데이터베이스에 축적되는 자료의 양이 급속히 늘어나고 있다. 데이터베이스의 응용 범위가 확대되고 대용량 데이터베이스로부터 유용한 지식을 발견하고자 하는 데이터 마이닝(Data Mining) 기술에 대한 연구가 활발하게 진행되고 있다. 기존의 알고리즘들은 대부분 후보 항목 집합들을 줄임과 동시에 데이터베이스의 크기를 줄이는 방법으...\n",
            "   파일: KCI-ART001021651-데이터 베이스 특성에 따른 효율적인 데이터 마이닝 알고리즘.json\n",
            "--------------------------------------------------------------------------------\n",
            "3. [0.3319] MySQL과 Redis의 데이터 처리 성능 비교 평가\n",
            "   저자: 방혁(Department of Information Security, The University of Suwon,  Republic of Korea), 김서현(Department of Information Communication, The University of  Suwon, Republic of Korea), 전상훈(수원대학교)\n",
            "   키워드: Data, Relational Database Management System, Not only SQL, 데이터, 관계형 데이터베이스, 비 관계형 데이터베이스\n",
            "   분야: 컴퓨터학\n",
            "   초록: 최근 디지털 변화와 코로나19의 영향으로 온라인 활동이 급증함에 따라 대규모 데이터 처리와 유지보수의 중요성이 점점 커지고있다. 이 연구는 데이터 저장 및 관리에 널리 사용되는 두 주요 데이터베이스 유형인 관계형 데이터베이스(RDBMS)와 비관계형 데이터베이스(NoSQL)의 성능을 비교 분석한다. 구체적으로, RDBMS의 대표 예인 MySQL과 NoSQL의...\n",
            "   파일: KCI-ART003098301-MySQL과 Redis의 데이터 처리 성능 비교 평가.json\n",
            "--------------------------------------------------------------------------------\n",
            "4. [0.3145] 데이터준비를 위한 XML 기반의 분산 MDR 검색 시스템 설계\n",
            "   저자: 고석범(육군사관학교), 윤성대(부경대학교)\n",
            "   키워드: Data Mining(데이터마이닝), Data Preparation(데이터준비), Metadata Registry(메타데이터 레지스트리), XML, Distributed Database System(분산 데이터베이스 시스템), Data Mining(데이터마이닝), Data Preparation(데이터준비), Metadata Registry(메타데이터 레지스트리), XML, Distributed Database System(분산 데이터베이스 시스템)\n",
            "   분야: 전자/정보통신공학\n",
            "   초록: 데이터마이닝은 방대한 데이터로부터 다차원적인 정보를 추출하는 것이다. 방대하게 구축되어 있는 데이터베이스에서 임의의 테이블의 컬럼에 대해 참조 할 수 있는 정보는 단순하게 컬럼명과 자료형 혹은 간단한 주석 정도이다. 그러한 비구조적이고 빈약한 내용만으로는 데이터마이닝을 위한 자료수집 및 자료탐색 단계에서 컬럼의 용도와 특성 및 스키마를 파악하여 데이터를 정...\n",
            "   파일: KCI-ART000956956-데이터준비를 위한 XML 기반의 분산 MDR 검색 시스템 설계.json\n",
            "--------------------------------------------------------------------------------\n",
            "5. [0.2915] 해양전문정보센터의 멀티미디어 메타데이터베이스 및 디지털도서관 통합정보시스템 구현에 관한 연구\n",
            "   저자: 한종엽(한국해양연구원), 최영준(㈜킨스 e사업본부)\n",
            "   키워드: 디지털도서관, 해양전문정보센터, 멀티미디어 정보처리, 메타 데이터베이스 시스템, 메타데이터 분류체계Digital Library, Oceanographic Information Center, Multimedia Information Processing, Meta Database System, MODS(Metadata Object Description Schema), Metadata Classification\n",
            "   분야: 문헌정보학\n",
            "   초록: 본 연구는 국내 해양전문정보센터에서 효율적인 정보서비스를 위해 필요한 멀티미디어 메타데이터베이스와 디지털도서관 통합정보시스템을 구현할 목적으로 선행연구를 조사하고 분석하였다. 연구대상자원은 해양분야의 인쇄매체, 네트워크자원, 원문화일, 동영상 등을 범위로 하였다. 본 연구에서는 인쇄매체를 포함한 각종 멀티미디어 컨텐츠 자원의 기술과 조직을 위해 LC표준으로...\n",
            "   파일: KCI-ART000933083-해양전문정보센터의 멀티미디어 메타데이터베이스 및 디지털도서관 통합정보시스템 구현에 관한 연구.json\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[3/3] 검색어: '지식 추론 알고리즘'\n",
            "--------------------------------------------------------------------------------\n",
            "1. [0.2953] 크로스 링크된 단백질 서브시퀀스를 찾는 알고리즘\n",
            "   저자: 김성권(중앙대학교)\n",
            "   키워드: cross links, protein sequences, cross links, protein sequences, 단백질 시퀀스, 크로스 링크\n",
            "   분야: 컴퓨터학\n",
            "   초록: 단백질의 구조를 예측하는 과정에 사용될 수 있는 다음 문제를 고려한다. 길이가n이고 원소가 모두 양수인 두 배열 A, B 와 양수 M 이 주어질 때, A[i]+...+ A[j] + B[k] +...+ B[l] = M 이 되는 부배열 쌍 A[i] ,..., A[j], 1≤i≤j≤n 과 B[k] ,..., B[l], 1≤k≤l≤n 을 모두 찾으시오. 본 논문에...\n",
            "   파일: KCI-ART000906254-크로스 링크된 단백질 서브시퀀스를 찾는 알고리즘.json\n",
            "--------------------------------------------------------------------------------\n",
            "2. [0.1981] 도메인 조합 기반 단백질 상호작용 가능성 순위 부여 기법\n",
            "   저자: 한동수(한국과학기술원), 장우혁(영남대학교), 이성독, 김홍숙(한국정보통신대)\n",
            "   키워드: Proteinprotein interaction, Domain combination, Domain combination pair, Prediction model, AP matrix, Protein-protein interaction possibility ranking method., 단백질-단백질 상호작용, 도메인 조합, 도메인 조합 쌍, 예측 모델, 출현 확률 행렬, 단백질-단백질 상호작용 가능성 순위 부여 기법\n",
            "   분야: 컴퓨터학\n",
            "   초록: 상에 단백질 및 관련 데이터의 축적에 따라, 도메인에 기반하여 단백질의 상호작용을 계산적으로 예측하는 많은 기법들이 제안되었다. 그러나, 대부분의 기법들이 예측에서 낮은 정확도와 복수개의 단백질 쌍에 대한 상호작용 가능성들 간에 순위 정보를 제공하지 못하는 등의 한계로 인하여 실무 적용에 한계를 가지고 있다. 본 논문에서는 도메인 조합 기반 단백질 상호작용...\n",
            "   파일: KCI-ART001187202-도메인 조합 기반 단백질 상호작용 가능성 순위 부여 기법.json\n",
            "--------------------------------------------------------------------------------\n",
            "3. [0.1894] 온톨로지 기반에서 연관 마이닝 방법을 이용한 지식 추론 알고리즘 연구\n",
            "   저자: 황현숙(부경대학교), 이준연(동명대학교)\n",
            "   키워드: Ontology(온톨로지), Association Mining(연관마이닝), Inference Algorithm(추론알고리즘), User Profile Modeling(사용자 프로파일 모델링), Databases(데이터베이스)\n",
            "   분야: 전자/정보통신공학\n",
            "   초록: 정보 검색에 대한 연구는 방대한 데이터에서 원하는 검색 정보를 제공할 뿐 만 아니라 개인의 취향에 따른 맞춤 검색 및 추론된 지식을 제공하는 데 초점을 두고 있다. 본 논문의 목적은 데이터를 개념화하여 분류 및 정의할 수 있는 온톨로지 구조를 기반으로 숨어있는 지식을 발견하여 개인 맞춤 검색을 제공하는 추론 알고리즘에 대해 연구하는 것이다. 현재의 검색에서...\n",
            "   파일: KCI-ART001293627-온톨로지 기반에서 연관 마이닝 방법을 이용한 지식 추론 알고리즘 연구.json\n",
            "--------------------------------------------------------------------------------\n",
            "4. [0.1878] 빅데이터 분석을 이용한 “단백질 보충제” 소비자 인식 연구\n",
            "   저자: 한춘미(상명대학교), 정계연(상명대학교 일반대학원), 홍완수(상명대학교)\n",
            "   키워드: 단백질, 단백질 보충제, 빅데이터, CONCOR, 텍스톰, Protein, Protein Supplement, Big Data, CONCOR, TEXTOM\n",
            "   분야: 관광학\n",
            "   초록: 본 연구는 최근 새롭게 주목받고 있는 단백질 보충제와 관련하여 빅데이터 분석을 통해 소비자 인식을 연구하였다. 이를 위해 다양한 포털사이트에서 단백질 보충제와 관련한 자료를 수집하였으며, 수집된 자료는 형태소 분석을 통해 총 50개의 핵심단어를 도출하였다. 도출된 핵심단어는 집단분석에 해당하는 CONOCR 분석을 통해 최종 4개의 구성 노드로 분류하였다. ...\n",
            "   파일: KCI-ART002895627-빅데이터 분석을 이용한 “단백질 보충제” 소비자 인식 연구.json\n",
            "--------------------------------------------------------------------------------\n",
            "5. [0.1664] 「性惡」편의 논리와 구조\n",
            "   저자: 이경무(춘천교육대학교)\n",
            "   키워드: 「性惡」편, 논리, 구조, 성악, 위선, 정당화, ‘xìng ě(性惡)’piān, logic, frame, xìng ě(性惡), wěi shàn(僞善), justification\n",
            "   분야: 철학\n",
            "   초록: 이 글에서는 「性惡」편의 논리와 구조를 탐색해 보고자 한다. 이때의 논리란 「性惡」편의 논증 체계 또는 추론 체계를 의미한다. 그리고 구조란 이러한 논증이나 추론에서 다루어지고 있는 기초 개념들의 관계를 가리킨다.\n",
            "맹자의 성선설이 그러하듯 순자의 성악설 또한 일련의 논변을 이루고 있다. 그런데 순자의 성악설은 맹자의 성선설을 논박하기 위한 것이다. 순자의 ...\n",
            "   파일: KCI-ART002314857-「性惡」편의 논리와 구조.json\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "논문 간 유사도 매트릭스:\n",
            "--------------------------------------------------------------------------------\n",
            "ID |   P1   |   P2   |   P3   |   P4   |   P5   |   P6   |   P7   |   P8   |   P9   |  P10   |\n",
            "----------------------------------------------------------------------------------------------\n",
            "P1 |  ---   | 0.2921 | 0.2956 | 0.3734 | 0.2513 | 0.2457 | 0.3328 | 0.2877 | 0.3972 | 0.1921 |\n",
            "P2 | 0.2921 |  ---   | 0.3920 | 0.2000 | 0.2838 | 0.3558 | 0.3087 | 0.4049 | 0.2767 | 0.2957 |\n",
            "P3 | 0.2956 | 0.3920 |  ---   | 0.3597 | 0.3047 | 0.3006 | 0.3364 | 0.4910 | 0.4348 | 0.4241 |\n",
            "P4 | 0.3734 | 0.2000 | 0.3597 |  ---   | 0.1067 | 0.2067 | 0.1985 | 0.3517 | 0.2447 | 0.3506 |\n",
            "P5 | 0.2513 | 0.2838 | 0.3047 | 0.1067 |  ---   | 0.2913 | 0.3429 | 0.3247 | 0.3889 | 0.1840 |\n",
            "P6 | 0.2457 | 0.3558 | 0.3006 | 0.2067 | 0.2913 |  ---   | 0.5502 | 0.3182 | 0.3306 | 0.3018 |\n",
            "P7 | 0.3328 | 0.3087 | 0.3364 | 0.1985 | 0.3429 | 0.5502 |  ---   | 0.2998 | 0.3074 | 0.2122 |\n",
            "P8 | 0.2877 | 0.4049 | 0.4910 | 0.3517 | 0.3247 | 0.3182 | 0.2998 |  ---   | 0.4027 | 0.5295 |\n",
            "P9 | 0.3972 | 0.2767 | 0.4348 | 0.2447 | 0.3889 | 0.3306 | 0.3074 | 0.4027 |  ---   | 0.4061 |\n",
            "P10 | 0.1921 | 0.2957 | 0.4241 | 0.3506 | 0.1840 | 0.3018 | 0.2122 | 0.5295 | 0.4061 |  ---   |\n",
            "----------------------------------------------------------------------------------------------\n",
            "P1: 온톨로지 기반에서 연관 마이닝 방법을 이용한 지식 추론 알고리즘 연구\n",
            "P2: 인공지능의 한국어 능력 종합 평가를 위한 벤치마크\n",
            "P3: 생성형 인공지능 시대의 문식성 교육 방향 탐색─행위자 네트워크 이론을 중심으로─\n",
            "P4: 인공지능의 발달과 문제점에 대한 고찰 －싱가포르･중국･일본을 중심으로－\n",
            "P5: 예비유아교사가 지각하는 유아과학교육 필요성과 인공지능 지식, 과학교수태도 및 과학교수효능감...\n",
            "P6: IoT 센서기반의 인공지능 기술이 융합된 김치 냉장고의 미래\n",
            "P7: 잔류가스 분석기(RGA)와 인공지능 모델링을 이용한  모니터링 시스템 개발\n",
            "P8: 중국에서 인공지능+모델로의 국가 발전전략의 변화 ―+인공지능에서 인공지능+로\n",
            "P9: 생성형 인공지능에 대한 인공지능 자기효능감과 인공지능 불안, 인공지능 신뢰가 태도와 수용의...\n",
            "P10: 인공지능의 무기화 경쟁과 인공지능 군사혁신\n",
            "\n",
            "테스트 완료! 실행 시간: 3.17초\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "논문 검색 모듈 간단 사용 예제\n",
        "특정 검색어로 논문 검색 및 결과 출력\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "# 설정 변수 (실제 환경에 맞게 수정)\n",
        "MODEL_PATH = f\"{HBN_MODELS_DIR_PATH}/hiBERT\"     # hiBERT 모델 경로\n",
        "PAPERS_DIR = HBN_ARTICLES_DIR_PATH          # 논문 디렉토리 경로\n",
        "EMBEDDINGS_FILE = f\"{HBN_EMBEDDINGS_DIR_PATH}/paper_embeddings.json\"     # 임베딩 파일 경로\n",
        "\n",
        "# 사용할 검색어 정의\n",
        "# QUERY = \"온톨로지 기반 연관 마이닝 방법을 이용한 지식 추론\"\n",
        "# QUERY = \"다문화 가정\"\n",
        "QUERY = \"도시 재생\"\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    메인 함수: 검색 모듈 사용 예제\n",
        "    \"\"\"\n",
        "    print(f\"검색어: '{QUERY}'\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # 1. 검색 엔진 초기화\n",
        "    engine = PaperSearchEngine(MODEL_PATH)\n",
        "\n",
        "    # 2. 임베딩 로드 또는 생성\n",
        "    if os.path.exists(EMBEDDINGS_FILE):\n",
        "        # 기존 임베딩 로드\n",
        "        engine.load_embeddings(EMBEDDINGS_FILE)\n",
        "    else:\n",
        "        # 새 임베딩 생성\n",
        "        engine.load_papers_from_dir(PAPERS_DIR)\n",
        "        engine.create_embeddings()\n",
        "        engine.save_embeddings(EMBEDDINGS_FILE)\n",
        "\n",
        "    # 3. 검색 실행\n",
        "    results = engine.search(QUERY, top_k=3)\n",
        "\n",
        "    # 4. 결과 출력\n",
        "    print(f\"\\n검색 결과 ({len(results)}개):\")\n",
        "    for i, paper in enumerate(results, 1):\n",
        "        print(f\"\\n결과 {i}:\")\n",
        "        print(f\"제목: {paper['title']}\")\n",
        "        print(f\"유사도: {paper['similarity_score']:.4f}\")\n",
        "\n",
        "        if paper.get('keywords'):\n",
        "            print(f\"키워드: {', '.join(paper['keywords'])}\")\n",
        "\n",
        "        if paper.get('abstract'):\n",
        "            # 초록 앞부분 표시\n",
        "            abstract = paper['abstract']\n",
        "            if len(abstract) > 150:\n",
        "                abstract = abstract[:150] + \"...\"\n",
        "            print(f\"초록: {abstract}\")\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "IrMhX0HQPdm0",
        "outputId": "36134620-d819-4746-914f-1a61b9714e4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "검색어: '도시 재생'\n",
            "------------------------------------------------------------\n",
            "hiBERT 모델 로드 성공: /content/drive/MyDrive/Workspaces/Hibrainnet/models/hibrainnet/hiBERT\n",
            "임베딩 로드 완료: /content/drive/MyDrive/Workspaces/Hibrainnet/embeddings/hibrainnet/paper_embeddings.json\n",
            "논문 698개, 임베딩 형태 (698, 768)\n",
            "\n",
            "검색 결과 (3개):\n",
            "\n",
            "결과 1:\n",
            "제목: 광주광역시의 도시 재생과 지속가능한 도시 성장 방안\n",
            "유사도: 0.3618\n",
            "키워드: 도시 재생, 지속가능한 도시 성장, 어반 폴리, 주택재개발, 주택재건축., urban regeneration, sustainable urban growth, urban folly, housing redevelopment, housing reconstruction.\n",
            "초록: 전 세계적으로 최근의 도시정책은 구도심 쇠퇴와 도시 재생이 쟁점이다. 본 연구에서는 광주광역시의 도시 재생과정을 고찰하고, 이를 기반으로 도시 정체성과 잠재자원에 기초한 지속가능한 도시성장 방안을 모색하고자 하였다. 분석 결과 광주시 도시 재생은 1960년대 이후 재개...\n",
            "------------------------------------------------------------\n",
            "\n",
            "결과 2:\n",
            "제목: 미국 텍사스주 러벅시의 도시 구조와 지속가능한 도시 재생\n",
            "유사도: 0.3491\n",
            "키워드: urban structure, civic center declining, sustainable regeneration, culture, 도시 구조, 도심 쇠퇴, 지속가능한 도시 재생, 문화\n",
            "초록: 최근에는 도시의 환경적, 경제적, 사회적 조화를 주제로 하는 문화·역사·정체성·생태 중심의 지속가능한 재생이 다양하게 제시되고 있다. 하지만, 도시 재생의 기반이 되는 도시 구조는 배제되어 있다. 이에 본 연구에서는 농·목축 지역을 배경으로 성장한 미국 텍사스주 러벅시...\n",
            "------------------------------------------------------------\n",
            "\n",
            "결과 3:\n",
            "제목: 시스템 다이내믹스를 이용한 감성도시 재생 모형 개발\n",
            "유사도: 0.3255\n",
            "키워드: emotion, urban regeneration, emotional urban regeneration, system dynamics, 감성, 도시재생, 감성 도시 재생, 시스템 다이나믹스\n",
            "초록: 이 연구는 감성도시 재생의 이론을 체계화하여 감성도시 재생에 대한 실태를 실증 분석하였다. 도시재생은 지역, 주로도시 지역에서 발생한 사회, 문화, 경제적 문제를 해결하고 지역의 혁신 나아가 사회 혁신을 도모하기 위해 장기적인 관점에서 개선하는 것을 목적으로 실시하는 ...\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}